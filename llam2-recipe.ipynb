{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af2ee7fb-e888-4e38-a349-c7c40dfd2963",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "# Fine-tune LLaMA 2 models on SageMaker JumpStart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85addd9d-ec89-44a7-9fb5-9bc24fe9993b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker in /opt/conda/lib/python3.10/site-packages (2.173.0)\n",
      "Collecting sagemaker\n",
      "  Downloading sagemaker-2.187.0.tar.gz (886 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m886.2/886.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting datasets\n",
      "  Obtaining dependency information for datasets from https://files.pythonhosted.org/packages/09/7e/fd4d6441a541dba61d0acb3c1fd5df53214c2e9033854e837a99dd9e0793/datasets-2.14.5-py3-none-any.whl.metadata\n",
      "  Downloading datasets-2.14.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (23.1.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.26.131 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.28.14)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (2.2.1)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.10/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.25.1)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (4.23.4)\n",
      "Requirement already satisfied: smdebug_rulesconfig==1.0.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (4.11.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (21.3)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.4.4)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.10/site-packages (from sagemaker) (0.3.1)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.10/site-packages (from sagemaker) (0.7.5)\n",
      "Requirement already satisfied: PyYAML~=6.0 in /opt/conda/lib/python3.10/site-packages/PyYAML-6.0-py3.10-linux-x86_64.egg (from sagemaker) (6.0)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.10/site-packages (from sagemaker) (4.16.0)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from sagemaker) (2.5.2)\n",
      "Requirement already satisfied: tblib==1.7.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.7.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (12.0.1)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.64.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Obtaining dependency information for xxhash from https://files.pythonhosted.org/packages/13/c3/e942893f4864a424514c81640f114980cfd5aff7e7414d1e0255f4571111/xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.15)\n",
      "Collecting fsspec[http]<2023.9.0,>=2023.1.0 (from datasets)\n",
      "  Obtaining dependency information for fsspec[http]<2023.9.0,>=2023.1.0 from https://files.pythonhosted.org/packages/e3/bd/4c0a4619494188a9db5d77e2100ab7d544a42e76b2447869d8e124e981d8/fsspec-2023.6.0-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2023.6.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Obtaining dependency information for aiohttp from https://files.pythonhosted.org/packages/3e/f6/fcda07dd1e72260989f0b22dde999ecfe80daa744f23ca167083683399bc/aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting huggingface-hub<1.0.0,>=0.14.0 (from datasets)\n",
      "  Obtaining dependency information for huggingface-hub<1.0.0,>=0.14.0 from https://files.pythonhosted.org/packages/72/21/51cddb8850ed3f4dbc21e57c3dabc49e64d5577857ddda7b2eb0ffc2ec0e/huggingface_hub-0.17.2-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.17.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: botocore<1.32.0,>=1.31.14 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.26.131->sagemaker) (1.31.14)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.26.131->sagemaker) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.26.131->sagemaker) (0.6.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets)\n",
      "  Obtaining dependency information for async-timeout<5.0,>=4.0.0a3 from https://files.pythonhosted.org/packages/a7/fa/e01228c2938de91d47b307831c62ab9e4001e747789d0b05baf779a6488c/async_timeout-4.0.3-py3-none-any.whl.metadata\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Obtaining dependency information for frozenlist>=1.1.1 from https://files.pythonhosted.org/packages/1e/28/74b8b6451c89c070d34e753d8b65a1e4ce508a6808b18529f36e8c0e2184/frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker) (3.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->sagemaker) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from google-pasta->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.18.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->sagemaker) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->sagemaker) (2022.1)\n",
      "Requirement already satisfied: ppft>=1.7.6.7 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker) (1.7.6.7)\n",
      "Requirement already satisfied: pox>=0.3.3 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.3)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.10/site-packages (from schema->sagemaker) (21.6.0)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.19.0->datasets)\n",
      "  Obtaining dependency information for urllib3<3,>=1.21.1 from https://files.pythonhosted.org/packages/c5/05/c214b32d21c0b465506f95c4f28ccbcba15022e000b043b72b3df7728471/urllib3-1.26.16-py2.py3-none-any.whl.metadata\n",
      "  Downloading urllib3-1.26.16-py2.py3-none-any.whl.metadata (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.4/48.4 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading datasets-2.14.5-py3-none-any.whl (519 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.17.2-py3-none-any.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.7/225.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.1/143.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: sagemaker\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-2.187.0-py2.py3-none-any.whl size=1186947 sha256=036b7afbdfc58560d4280c345dfda3c63e6738cc6633c232d02a5d6952c7da15\n",
      "  Stored in directory: /root/.cache/pip/wheels/21/d2/2b/ed240a9d3084c5b4e3e7fd2ca8f9c11659bb98f0b87b6b1ca3\n",
      "Successfully built sagemaker\n",
      "Installing collected packages: xxhash, urllib3, multidict, fsspec, frozenlist, async-timeout, yarl, aiosignal, huggingface-hub, aiohttp, sagemaker, datasets\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.0.4\n",
      "    Uninstalling urllib3-2.0.4:\n",
      "      Successfully uninstalled urllib3-2.0.4\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2022.7.1\n",
      "    Uninstalling fsspec-2022.7.1:\n",
      "      Successfully uninstalled fsspec-2022.7.1\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.173.0\n",
      "    Uninstalling sagemaker-2.173.0:\n",
      "      Successfully uninstalled sagemaker-2.173.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "distributed 2022.7.0 requires tornado<6.2,>=6.0.3, but you have tornado 6.3.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed aiohttp-3.8.5 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.14.5 frozenlist-1.4.0 fsspec-2023.6.0 huggingface-hub-0.17.2 multidict-6.0.4 sagemaker-2.187.0 urllib3-1.26.16 xxhash-3.3.0 yarl-1.9.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade sagemaker datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1a85fe2-0da7-4611-9c91-d411da2a6d31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12303ebb1320446b87b793c9cf37e7a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/3.46k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eda7c3a827c6412dae4239dbff4fe626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/1.88k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89f48a668d48415e91c3dc7100ab8c62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/5.39k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.25.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a778da5d059494fa6f7969d79b3894a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/6.71M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e887457ff568484b8376765045b554f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/6118 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa3665041dd84bbb86e4e056bbc32d21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1080 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f792271d6396459d8dd64ac035686635",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6118 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62d618eb069a4affaf80c35368f7975d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1080 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55a7fbd5119d478883e117170822c01c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6118 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54665fb59526454c8fb6d894459603d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1080 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "997171b5e9444192973731737530a761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6118 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14d2020dd7de4961a435220fb0b13d83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1080 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"m3hrdadfi/recipe_nlg_lite\")\n",
    "newset = dataset.map(lambda x: {'instruction': 'Generate a recipe for the dish '+x['name']})\n",
    "newset = newset.map(lambda x: {'context': x['description']})\n",
    "newset = newset.map(lambda x: {'response': 'Ingredients :\\n '+ x['ingredients']+'\\nSteps:\\n'+x['steps']})\n",
    "newset = newset.select_columns(['instruction','context','response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dfd324cb-92df-4451-961a-27d7b8baba66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "smallset = newset['train'].shard(num_shards=2, index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1cc91030-0da1-49fd-b8d7-fd1a655bd44f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3059"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(smallset['instruction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "880ad23c-571d-4e08-bc50-0eefb5a97f46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81b71d9800ce42de87680547f646638e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "4218922"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smallset.to_json('train.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e9fbf002-3ee3-4cc8-8fce-871939f1bd19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ingredients :\\n 2 oz dried mushrooms, 4 cups water, 2 cups green or brown lentils, , 6 cups low sodium vegetable broth, 2 tbsp olive oil, 1 whole large onion, diced, 2 tbsp tomato paste, 2 tbsp tamari sauce, 1 1/2 tsp crushed garlic, 28 oz crushed fire roasted tomatoes, 1/3 cup fresh chopped parsley, , 1 tsp smoked paprika, 3/4 tsp salt, or more to taste, 1 tsp basil, 1 tsp oregano, 1/2 tsp turmeric, 1/2 tsp thyme, 1/2 tsp black pepper, 1/4 tsp cayenne pepper, cooked pasta for serving\\nSteps:\\nboil mushrooms in 4 cups water for 5 minutes . let them soak in the hot water for a few minutes while you cook the lentils, until the liquid is dark brown and has cooled off a bit.meanwhile, bring lentils to a boil in vegetable broth . reduce to a simmer, then cook for about 15 minutes or until just tender . drain . remove the soaked mushrooms with a slotted spoon and rinse them under cool running water to remove any excess residue . chop up the soaked mushrooms, then rinse them once again there can be stubborn debris that clings, multiple rinses are the best way to remove any residual dirt . reserve the chopped mushrooms . line a handheld mesh strainer with a coffee filter . strain the mushroom cooking liquid through a coffee filter into a bowl or measuring cup . this may take a few minutes and a bit of maneuvering . i find i have to swirl the liquid around a bit for it to go through the filter, and often i need to replace the filter a couple of times in order to get all of the liquid to go through . while this takes some effort, the liquid often contains a bit of dirt residue from the harvested mushrooms . straining it out allows us to use this very flavorful liquid in the finished product . reserve the strained liquid . in a large 5 quart saute pan or sauce pot, heat 2 tbsp extra virgin olive oil over medium . saute diced onion for 10 15 minutes until soft, translucent and starting to turn golden . add the tomato paste, tamari and minced garlic to the onions . saute for 1 2 minutes longer until fragrant . pour in the reserved mushroom liquid to deglaze the pan, scraping up the brown bits from the bottom and simmering them in the liquid for a few minutes . add the fire roasted tomatoes to the pot along with the cooked lentils, soaked diced mushrooms, 1/4 cup freshly chopped parsley and remaining spices . stir . cook the mixture 10 15 minutes longer until the lentils are fully tender, the excess liquid has reduced, and the sauce is thick . serve sauce over pasta as a vegan alternative to bolognese, or you can enjoy this on its own as a flavorful lentil stew . garnish with remaining 1 tbsp parsley . if you're not vegan, a sprinkle of parmesan on top is also lovely.\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smallset[\"response\"][200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90451114-7cf5-445c-88e3-02ccaa5d3a4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "template = {\n",
    "    \"prompt\": \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "    \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "    \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{context}\\n\\n\",\n",
    "    \"completion\": \" {response}\",\n",
    "}\n",
    "with open(\"template.json\", \"w\") as f:\n",
    "    json.dump(template, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2e5489-33dc-4623-92da-f6fc97bd25ab",
   "metadata": {},
   "source": [
    "---\n",
    "Next, we create a prompt template for using the data in an instruction / input format for the training job (since we are instruction fine-tuning the model in this example), and also for inferencing the deployed endpoint.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22171b1-1cec-4cec-9ce4-db62761633d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Upload dataset to S3\n",
    "---\n",
    "\n",
    "We will upload the prepared dataset to S3 which will be used for fine-tuning.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5e1ee29a-8439-4788-8088-35a433fe2110",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "Training data: s3://sagemaker-us-east-1-879412751908/recipe_dataset\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "import sagemaker\n",
    "import random\n",
    "\n",
    "output_bucket = sagemaker.Session().default_bucket()\n",
    "local_data_file = \"train.jsonl\"\n",
    "train_data_location = f\"s3://{output_bucket}/recipe_dataset\"\n",
    "S3Uploader.upload(local_data_file, train_data_location)\n",
    "S3Uploader.upload(\"template.json\", train_data_location)\n",
    "print(f\"Training data: {train_data_location}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a3586ee0-392b-468f-af01-d655d2503e17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id, model_version = \"meta-textgeneration-llama-2-7b\", \"*\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e61340-bc81-477d-aaf1-f37e8c554863",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train the model\n",
    "---\n",
    "Next, we fine-tune the LLaMA v2 7B model on the summarization dataset from Dolly. Finetuning scripts are based on scripts provided by [this repo](https://github.com/facebookresearch/llama-recipes/tree/main). To learn more about the fine-tuning scripts, please checkout section [5. Few notes about the fine-tuning method](#5.-Few-notes-about-the-fine-tuning-method). For a list of supported hyper-parameters and their default values, please see section [3. Supported Hyper-parameters for fine-tuning](#3.-Supported-Hyper-parameters-for-fine-tuning).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9a71087e-9c9e-42d7-999e-5f3fac07bc4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: meta-textgeneration-llama-2-7b-2023-09-24-03-45-50-812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-24 03:45:50 Starting - Starting the training job...\n",
      "2023-09-24 03:46:06 Starting - Preparing the instances for training.........\n",
      "2023-09-24 03:47:25 Downloading - Downloading input data............\n",
      "2023-09-24 03:49:26 Training - Downloading the training image..................\n",
      "2023-09-24 03:52:47 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-09-24 03:52:48,801 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-09-24 03:52:48,815 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-24 03:52:48,823 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-09-24 03:52:48,825 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-09-24 03:52:56,099 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing ./lib/accelerate/accelerate-0.21.0-py3-none-any.whl (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/bitsandbytes/bitsandbytes-0.39.1-py3-none-any.whl (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/black/black-23.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/brotli/Brotli-1.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/datasets/datasets-2.14.1-py3-none-any.whl (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/fire/fire-0.5.0.tar.gz\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mProcessing ./lib/inflate64/inflate64-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/loralib/loralib-0.1.1-py3-none-any.whl (from -r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/multivolumefile/multivolumefile-0.2.3-py3-none-any.whl (from -r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/mypy-extensions/mypy_extensions-1.0.0-py3-none-any.whl (from -r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pathspec/pathspec-0.11.1-py3-none-any.whl (from -r requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/peft/peft-0.4.0-py3-none-any.whl (from -r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/py7zr/py7zr-0.20.5-py3-none-any.whl (from -r requirements.txt (line 13))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pybcj/pybcj-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 14))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pycryptodomex/pycryptodomex-3.18.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 15))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pyppmd/pyppmd-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 16))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pytorch-triton/pytorch_triton-2.1.0+9e3e10c5ed-cp310-cp310-linux_x86_64.whl (from -r requirements.txt (line 17))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pyzstd/pyzstd-0.15.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 18))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/safetensors/safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 19))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/scipy/scipy-1.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 20))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/termcolor/termcolor-2.3.0-py3-none-any.whl (from -r requirements.txt (line 21))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/texttable/texttable-1.6.7-py2.py3-none-any.whl (from -r requirements.txt (line 22))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tokenize-rt/tokenize_rt-5.1.0-py2.py3-none-any.whl (from -r requirements.txt (line 23))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tokenizers/tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 24))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/torch/torch-2.1.0.dev20230728+cu118-cp310-cp310-linux_x86_64.whl (from -r requirements.txt (line 25))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/transformers/transformers-4.31.0-py3-none-any.whl (from -r requirements.txt (line 26))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_script_utilities/sagemaker_jumpstart_script_utilities-1.1.8-py2.py3-none-any.whl (from -r requirements.txt (line 27))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_huggingface_script_utilities/sagemaker_jumpstart_huggingface_script_utilities-1.0.7-py2.py3-none-any.whl (from -r requirements.txt (line 28))\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (8.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: platformdirs>=2 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (3.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tomli>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (12.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (2.28.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (2023.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (3.8.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (0.14.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire==0.5.0->-r requirements.txt (line 6)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from pytorch-triton==2.1.0+9e3e10c5ed->-r requirements.txt (line 17)) (3.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0.dev20230728+cu118->-r requirements.txt (line 25)) (4.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0.dev20230728+cu118->-r requirements.txt (line 25)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0.dev20230728+cu118->-r requirements.txt (line 25)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0.dev20230728+cu118->-r requirements.txt (line 25)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 26)) (2023.5.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (22.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (6.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (4.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.9.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (2023.5.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.1.0.dev20230728+cu118->-r requirements.txt (line 25)) (2.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.1.0.dev20230728+cu118->-r requirements.txt (line 25)) (1.3.0)\u001b[0m\n",
      "\u001b[34mtokenizers is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: fire\u001b[0m\n",
      "\u001b[34mBuilding wheel for fire (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for fire (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116932 sha256=40a16aca5628d550d0d18c14d56ea10ac4837132485f54736c1f61cc854fe0a5\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/db/3d/41/7e69dca5f61e37d109a4457082ffc5c6edb55ab633bafded38\u001b[0m\n",
      "\u001b[34mSuccessfully built fire\u001b[0m\n",
      "\u001b[34mInstalling collected packages: texttable, safetensors, Brotli, bitsandbytes, tokenize-rt, termcolor, scipy, sagemaker-jumpstart-script-utilities, sagemaker-jumpstart-huggingface-script-utilities, pyzstd, pytorch-triton, pyppmd, pycryptodomex, pybcj, pathspec, mypy-extensions, multivolumefile, loralib, inflate64, torch, py7zr, fire, black, transformers, accelerate, peft, datasets\u001b[0m\n",
      "\u001b[34mAttempting uninstall: scipy\u001b[0m\n",
      "\u001b[34mFound existing installation: scipy 1.10.1\u001b[0m\n",
      "\u001b[34mUninstalling scipy-1.10.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled scipy-1.10.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: torch\u001b[0m\n",
      "\u001b[34mFound existing installation: torch 2.0.0\u001b[0m\n",
      "\u001b[34mUninstalling torch-2.0.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled torch-2.0.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.28.1\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.28.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.28.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.19.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.19.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.19.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: datasets\u001b[0m\n",
      "\u001b[34mFound existing installation: datasets 2.12.0\u001b[0m\n",
      "\u001b[34mUninstalling datasets-2.12.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled datasets-2.12.0\u001b[0m\n",
      "\u001b[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[34mfastai 2.7.12 requires torch<2.1,>=1.7, but you have torch 2.1.0.dev20230728+cu118 which is incompatible.\u001b[0m\n",
      "\u001b[34mSuccessfully installed Brotli-1.0.9 accelerate-0.21.0 bitsandbytes-0.39.1 black-23.7.0 datasets-2.14.1 fire-0.5.0 inflate64-0.3.1 loralib-0.1.1 multivolumefile-0.2.3 mypy-extensions-1.0.0 pathspec-0.11.1 peft-0.4.0 py7zr-0.20.5 pybcj-1.0.1 pycryptodomex-3.18.0 pyppmd-1.0.0 pytorch-triton-2.1.0+9e3e10c5ed pyzstd-0.15.9 safetensors-0.3.1 sagemaker-jumpstart-huggingface-script-utilities-1.0.7 sagemaker-jumpstart-script-utilities-1.1.8 scipy-1.11.1 termcolor-2.3.0 texttable-1.6.7 tokenize-rt-5.1.0 torch-2.1.0.dev20230728+cu118 transformers-4.31.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2023-09-24 03:54:14,708 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-09-24 03:54:14,708 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-09-24 03:54:14,773 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-24 03:54:14,797 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-24 03:54:14,820 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-24 03:54:14,829 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"code\": \"/opt/ml/input/data/code\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"enable_fsdp\": \"True\",\n",
      "        \"epoch\": \"3\",\n",
      "        \"instruction_tuned\": \"True\",\n",
      "        \"int8_quantization\": \"False\",\n",
      "        \"learning_rate\": \"0.0001\",\n",
      "        \"lora_alpha\": \"32\",\n",
      "        \"lora_dropout\": \"0.05\",\n",
      "        \"lora_r\": \"8\",\n",
      "        \"max_input_length\": \"1024\",\n",
      "        \"max_train_samples\": \"-1\",\n",
      "        \"max_val_samples\": \"-1\",\n",
      "        \"per_device_eval_batch_size\": \"1\",\n",
      "        \"per_device_train_batch_size\": \"4\",\n",
      "        \"preprocessing_num_workers\": \"None\",\n",
      "        \"seed\": \"10\",\n",
      "        \"train_data_split_seed\": \"0\",\n",
      "        \"validation_split_ratio\": \"0.2\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"code\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"meta-textgeneration-llama-2-7b-2023-09-24-03-45-50-812\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"/opt/ml/input/data/code/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"transfer_learning\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"transfer_learning.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"enable_fsdp\":\"True\",\"epoch\":\"3\",\"instruction_tuned\":\"True\",\"int8_quantization\":\"False\",\"learning_rate\":\"0.0001\",\"lora_alpha\":\"32\",\"lora_dropout\":\"0.05\",\"lora_r\":\"8\",\"max_input_length\":\"1024\",\"max_train_samples\":\"-1\",\"max_val_samples\":\"-1\",\"per_device_eval_batch_size\":\"1\",\"per_device_train_batch_size\":\"4\",\"preprocessing_num_workers\":\"None\",\"seed\":\"10\",\"train_data_split_seed\":\"0\",\"validation_split_ratio\":\"0.2\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=transfer_learning.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"code\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.2xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=transfer_learning\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=/opt/ml/input/data/code/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"code\":\"/opt/ml/input/data/code\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"enable_fsdp\":\"True\",\"epoch\":\"3\",\"instruction_tuned\":\"True\",\"int8_quantization\":\"False\",\"learning_rate\":\"0.0001\",\"lora_alpha\":\"32\",\"lora_dropout\":\"0.05\",\"lora_r\":\"8\",\"max_input_length\":\"1024\",\"max_train_samples\":\"-1\",\"max_val_samples\":\"-1\",\"per_device_eval_batch_size\":\"1\",\"per_device_train_batch_size\":\"4\",\"preprocessing_num_workers\":\"None\",\"seed\":\"10\",\"train_data_split_seed\":\"0\",\"validation_split_ratio\":\"0.2\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"meta-textgeneration-llama-2-7b-2023-09-24-03-45-50-812\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/input/data/code/sourcedir.tar.gz\",\"module_name\":\"transfer_learning\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"transfer_learning.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--enable_fsdp\",\"True\",\"--epoch\",\"3\",\"--instruction_tuned\",\"True\",\"--int8_quantization\",\"False\",\"--learning_rate\",\"0.0001\",\"--lora_alpha\",\"32\",\"--lora_dropout\",\"0.05\",\"--lora_r\",\"8\",\"--max_input_length\",\"1024\",\"--max_train_samples\",\"-1\",\"--max_val_samples\",\"-1\",\"--per_device_eval_batch_size\",\"1\",\"--per_device_train_batch_size\",\"4\",\"--preprocessing_num_workers\",\"None\",\"--seed\",\"10\",\"--train_data_split_seed\",\"0\",\"--validation_split_ratio\",\"0.2\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_CODE=/opt/ml/input/data/code\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_ENABLE_FSDP=True\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCH=3\u001b[0m\n",
      "\u001b[34mSM_HP_INSTRUCTION_TUNED=True\u001b[0m\n",
      "\u001b[34mSM_HP_INT8_QUANTIZATION=False\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.0001\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_ALPHA=32\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_DROPOUT=0.05\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_R=8\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_INPUT_LENGTH=1024\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_TRAIN_SAMPLES=-1\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_VAL_SAMPLES=-1\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=1\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=4\u001b[0m\n",
      "\u001b[34mSM_HP_PREPROCESSING_NUM_WORKERS=None\u001b[0m\n",
      "\u001b[34mSM_HP_SEED=10\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_DATA_SPLIT_SEED=0\u001b[0m\n",
      "\u001b[34mSM_HP_VALIDATION_SPLIT_RATIO=0.2\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 transfer_learning.py --enable_fsdp True --epoch 3 --instruction_tuned True --int8_quantization False --learning_rate 0.0001 --lora_alpha 32 --lora_dropout 0.05 --lora_r 8 --max_input_length 1024 --max_train_samples -1 --max_val_samples -1 --per_device_eval_batch_size 1 --per_device_train_batch_size 4 --preprocessing_num_workers None --seed 10 --train_data_split_seed 0 --validation_split_ratio 0.2\u001b[0m\n",
      "\u001b[34m2023-09-24 03:54:15,047 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so.11.0\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34mINFO:root:Using pre-trained artifacts in SAGEMAKER_ADDITIONAL_S3_DATA_PATH=/opt/ml/additonals3data\u001b[0m\n",
      "\u001b[34mINFO:root:Identify file serving.properties in the un-tar directory /opt/ml/additonals3data. Copying it over to /opt/ml/model for model deployment after training is finished.\u001b[0m\n",
      "\u001b[34mINFO:root:Invoking the training command ['torchrun', '--nnodes', '1', '--nproc_per_node', '1', 'llama_finetuning.py', '--model_name', '/opt/ml/additonals3data', '--num_gpus', '1', '--pure_bf16', '--dist_checkpoint_root_folder', 'model_checkpoints', '--dist_checkpoint_folder', 'fine-tuned', '--batch_size_training', '4', '--micro_batch_size', '4', '--train_file', '/opt/ml/input/data/training', '--lr', '0.0001', '--do_train', '--output_dir', 'saved_peft_model', '--num_epochs', '3', '--use_peft', '--peft_method', 'lora', '--max_train_samples', '-1', '--max_val_samples', '-1', '--seed', '10', '--per_device_eval_batch_size', '1', '--max_input_length', '1024', '--preprocessing_num_workers', '--validation_split_ratio', '0.2', '--train_data_split_seed', '0', '--num_workers_dataloader', '0', '--weight_decay', '0.1', '--lora_r', '8', '--lora_alpha', '32', '--lora_dropout', '0.05', '--enable_fsdp', '--instruction_tuned'].\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so.11.0\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 0. Rank is 0\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 0\u001b[0m\n",
      "\u001b[34m--> Running with torch dist debug set to detail\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mDownloading data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading data files: 100%|██████████| 1/1 [00:00<00:00, 7371.36it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files: 100%|██████████| 1/1 [00:00<00:00, 1003.66it/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 3059 examples [00:00, 216945.54 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/3059 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  63%|██████▎   | 1924/3059 [00:00<00:00, 19114.06 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 3059/3059 [00:00<00:00, 18887.89 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/3059 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  33%|███▎      | 1000/3059 [00:02<00:04, 497.30 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  65%|██████▌   | 2000/3059 [00:03<00:02, 505.93 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  98%|█████████▊| 3000/3059 [00:05<00:00, 507.23 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 3059/3059 [00:06<00:00, 505.22 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 3059/3059 [00:06<00:00, 504.86 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/3059 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  33%|███▎      | 1000/3059 [00:00<00:01, 2048.65 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  65%|██████▌   | 2000/3059 [00:00<00:00, 2099.84 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  98%|█████████▊| 3000/3059 [00:01<00:00, 2110.57 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 3059/3059 [00:01<00:00, 2091.40 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Split the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:29<00:29, 29.82s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:40<00:00, 18.62s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:40<00:00, 20.30s/it]\u001b[0m\n",
      "\u001b[34m--> Model /opt/ml/additonals3data\u001b[0m\n",
      "\u001b[34m--> /opt/ml/additonals3data has 6738.415616 Million params\u001b[0m\n",
      "\u001b[34mtrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\u001b[0m\n",
      "\u001b[34mbFloat16 enabled for mixed precision - using bfSixteen policy\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34mINFO:root:--> Training Set Length = 1125\u001b[0m\n",
      "\u001b[34mINFO:root:--> Validation Set Length = 282\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:306: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/281 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mNCCL version 2.17.1+cuda11.8\u001b[0m\n",
      "\u001b[34malgo-1:63:100 [0] nccl_net_ofi_init:1444 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34malgo-1:63:100 [0] nccl_net_ofi_init:1483 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34mstep 0 is completed and loss is 1.6308079957962036\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 1/281 [00:05<27:10,  5.82s/it]\u001b[0m\n",
      "\u001b[34mstep 1 is completed and loss is 1.544947862625122\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 2/281 [00:10<23:26,  5.04s/it]\u001b[0m\n",
      "\u001b[34mstep 2 is completed and loss is 1.5914475917816162\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 3/281 [00:14<22:11,  4.79s/it]\u001b[0m\n",
      "\u001b[34mstep 3 is completed and loss is 1.5233057737350464\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m▏         #033[0m| 4/281 [00:19<21:33,  4.67s/it]\u001b[0m\n",
      "\u001b[34mstep 4 is completed and loss is 1.4603772163391113\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 5/281 [00:23<21:10,  4.60s/it]\u001b[0m\n",
      "\u001b[34mstep 5 is completed and loss is 1.5622303485870361\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 6/281 [00:28<20:55,  4.57s/it]\u001b[0m\n",
      "\u001b[34mstep 6 is completed and loss is 1.480265498161316\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 7/281 [00:32<20:44,  4.54s/it]\u001b[0m\n",
      "\u001b[34mstep 7 is completed and loss is 1.4110292196273804\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 8/281 [00:37<20:35,  4.52s/it]\u001b[0m\n",
      "\u001b[34mstep 8 is completed and loss is 1.4649142026901245\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 9/281 [00:41<20:27,  4.51s/it]\u001b[0m\n",
      "\u001b[34mstep 9 is completed and loss is 1.3828179836273193\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▎         #033[0m| 10/281 [00:46<20:20,  4.51s/it]\u001b[0m\n",
      "\u001b[34mstep 10 is completed and loss is 1.6626672744750977\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 11/281 [00:50<20:14,  4.50s/it]\u001b[0m\n",
      "\u001b[34mstep 11 is completed and loss is 1.50972318649292\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 12/281 [00:55<20:09,  4.50s/it]\u001b[0m\n",
      "\u001b[34mstep 12 is completed and loss is 1.386502981185913\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▍         #033[0m| 13/281 [00:59<20:04,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 13 is completed and loss is 1.4156739711761475\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▍         #033[0m| 14/281 [01:04<19:58,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 14 is completed and loss is 1.2685332298278809\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▌         #033[0m| 15/281 [01:08<19:54,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 15 is completed and loss is 1.2096282243728638\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 16/281 [01:13<19:49,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 16 is completed and loss is 1.1390200853347778\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 17/281 [01:17<19:44,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 17 is completed and loss is 1.3474289178848267\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▋         #033[0m| 18/281 [01:22<19:40,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 18 is completed and loss is 1.2579431533813477\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 19/281 [01:26<19:35,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 19 is completed and loss is 1.360927700996399\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 20/281 [01:31<19:30,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 20 is completed and loss is 1.3886581659317017\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 21/281 [01:35<19:26,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 21 is completed and loss is 1.2549431324005127\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 22/281 [01:40<19:21,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 22 is completed and loss is 1.2627819776535034\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 23/281 [01:44<19:16,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 23 is completed and loss is 1.2579621076583862\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▊         #033[0m| 24/281 [01:49<19:12,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 24 is completed and loss is 1.2101176977157593\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 25/281 [01:53<19:07,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 25 is completed and loss is 1.1450004577636719\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 26/281 [01:57<19:03,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 26 is completed and loss is 1.2938058376312256\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m▉         #033[0m| 27/281 [02:02<18:58,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 27 is completed and loss is 1.0781861543655396\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m▉         #033[0m| 28/281 [02:06<18:54,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 28 is completed and loss is 1.2329543828964233\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m█         #033[0m| 29/281 [02:11<18:50,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 29 is completed and loss is 1.197715163230896\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 30/281 [02:15<18:45,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 30 is completed and loss is 1.1885526180267334\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 31/281 [02:20<18:40,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 31 is completed and loss is 1.0796030759811401\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█▏        #033[0m| 32/281 [02:24<18:36,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 32 is completed and loss is 1.2313857078552246\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 33/281 [02:29<18:31,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 33 is completed and loss is 1.1889358758926392\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 34/281 [02:33<18:27,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 34 is completed and loss is 1.053437352180481\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 35/281 [02:38<18:22,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 35 is completed and loss is 1.2376123666763306\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 36/281 [02:42<18:18,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 36 is completed and loss is 1.1672669649124146\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 37/281 [02:47<18:13,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 37 is completed and loss is 1.2047752141952515\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▎        #033[0m| 38/281 [02:51<18:09,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 38 is completed and loss is 1.0193458795547485\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 39/281 [02:56<18:04,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 39 is completed and loss is 1.1686030626296997\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 40/281 [03:00<18:00,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 40 is completed and loss is 1.0897455215454102\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 41/281 [03:05<17:55,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 41 is completed and loss is 1.3957583904266357\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 42/281 [03:09<17:51,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 42 is completed and loss is 1.1765645742416382\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▌        #033[0m| 43/281 [03:14<17:46,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 43 is completed and loss is 1.2309291362762451\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 44/281 [03:18<17:42,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 44 is completed and loss is 1.0745480060577393\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 45/281 [03:23<17:38,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 45 is completed and loss is 1.3246818780899048\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▋        #033[0m| 46/281 [03:27<17:33,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 46 is completed and loss is 1.0110993385314941\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 47/281 [03:32<17:28,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 47 is completed and loss is 1.0469646453857422\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 48/281 [03:36<17:24,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 48 is completed and loss is 0.9992446303367615\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 49/281 [03:41<17:19,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 49 is completed and loss is 1.0876944065093994\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 50/281 [03:45<17:15,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 50 is completed and loss is 1.0063363313674927\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 51/281 [03:50<17:10,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 51 is completed and loss is 0.9028053283691406\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▊        #033[0m| 52/281 [03:54<17:06,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 52 is completed and loss is 1.1418720483779907\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 53/281 [03:59<17:01,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 53 is completed and loss is 1.3214260339736938\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 54/281 [04:03<16:57,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 54 is completed and loss is 1.1285650730133057\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m█▉        #033[0m| 55/281 [04:07<16:52,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 55 is completed and loss is 1.0485193729400635\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m█▉        #033[0m| 56/281 [04:12<16:48,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 56 is completed and loss is 1.2415233850479126\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m██        #033[0m| 57/281 [04:16<16:43,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 57 is completed and loss is 1.2689889669418335\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 58/281 [04:21<16:39,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 58 is completed and loss is 1.006340503692627\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 59/281 [04:25<16:34,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 59 is completed and loss is 1.0306237936019897\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██▏       #033[0m| 60/281 [04:30<16:30,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 60 is completed and loss is 1.0671145915985107\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 61/281 [04:34<16:26,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 61 is completed and loss is 1.0337533950805664\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 62/281 [04:39<16:21,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 62 is completed and loss is 0.9734873175621033\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 63/281 [04:43<16:17,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 63 is completed and loss is 1.0081942081451416\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 64/281 [04:48<16:12,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 64 is completed and loss is 1.0199427604675293\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 65/281 [04:52<16:08,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 65 is completed and loss is 1.1697208881378174\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 66/281 [04:57<16:03,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 66 is completed and loss is 1.0549473762512207\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 67/281 [05:01<15:59,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 67 is completed and loss is 1.1030243635177612\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 68/281 [05:06<15:54,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 68 is completed and loss is 1.0044538974761963\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▍       #033[0m| 69/281 [05:10<15:50,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 69 is completed and loss is 0.9576418399810791\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▍       #033[0m| 70/281 [05:15<15:45,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 70 is completed and loss is 1.0775636434555054\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 71/281 [05:19<15:41,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 71 is completed and loss is 1.023313283920288\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 72/281 [05:24<15:36,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 72 is completed and loss is 1.0508188009262085\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 73/281 [05:28<15:32,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 73 is completed and loss is 1.021956205368042\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▋       #033[0m| 74/281 [05:33<15:27,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 74 is completed and loss is 1.186104416847229\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 75/281 [05:37<15:23,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 75 is completed and loss is 0.9113925695419312\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 76/281 [05:42<15:18,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 76 is completed and loss is 1.050410509109497\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 77/281 [05:46<15:14,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 77 is completed and loss is 1.3827555179595947\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 78/281 [05:51<15:09,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 78 is completed and loss is 1.1071816682815552\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 79/281 [05:55<15:05,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 79 is completed and loss is 1.0486382246017456\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 80/281 [06:00<15:00,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 80 is completed and loss is 1.1494816541671753\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 81/281 [06:04<14:56,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 81 is completed and loss is 1.0899765491485596\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 82/281 [06:09<14:51,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 82 is completed and loss is 0.9825391173362732\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 83/281 [06:13<14:47,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 83 is completed and loss is 1.0335713624954224\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 84/281 [06:17<14:42,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 84 is completed and loss is 1.1051138639450073\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m███       #033[0m| 85/281 [06:22<14:38,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 85 is completed and loss is 1.1102991104125977\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 86/281 [06:26<14:33,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 86 is completed and loss is 1.0405335426330566\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 87/281 [06:31<14:29,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 87 is completed and loss is 1.0427825450897217\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███▏      #033[0m| 88/281 [06:35<14:25,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 88 is completed and loss is 1.0681350231170654\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 89/281 [06:40<14:20,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 89 is completed and loss is 1.0841941833496094\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 90/281 [06:44<14:16,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 90 is completed and loss is 1.1263844966888428\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 91/281 [06:49<14:11,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 91 is completed and loss is 1.2608681917190552\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 92/281 [06:53<14:07,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 92 is completed and loss is 1.0292545557022095\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 93/281 [06:58<14:02,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 93 is completed and loss is 0.9638540744781494\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 94/281 [07:02<13:58,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 94 is completed and loss is 0.9588510394096375\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 95/281 [07:07<13:53,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 95 is completed and loss is 1.1113322973251343\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 96/281 [07:11<13:49,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 96 is completed and loss is 0.9433037042617798\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▍      #033[0m| 97/281 [07:16<13:44,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 97 is completed and loss is 1.2058581113815308\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▍      #033[0m| 98/281 [07:20<13:40,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 98 is completed and loss is 1.095820665359497\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▌      #033[0m| 99/281 [07:25<13:35,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 99 is completed and loss is 1.028791069984436\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 100/281 [07:29<13:31,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 100 is completed and loss is 1.2327097654342651\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 101/281 [07:34<13:26,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 101 is completed and loss is 1.240518569946289\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▋      #033[0m| 102/281 [07:38<13:22,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 102 is completed and loss is 1.179229497909546\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 103/281 [07:43<13:17,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 103 is completed and loss is 1.0958088636398315\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 104/281 [07:47<13:13,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 104 is completed and loss is 1.186731219291687\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 105/281 [07:52<13:08,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 105 is completed and loss is 1.1166203022003174\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 106/281 [07:56<13:04,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 106 is completed and loss is 1.2246090173721313\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 107/281 [08:01<12:59,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 107 is completed and loss is 1.0746877193450928\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 108/281 [08:05<12:55,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 108 is completed and loss is 0.9466702938079834\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 109/281 [08:10<12:50,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 109 is completed and loss is 1.0747036933898926\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 110/281 [08:14<12:46,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 110 is completed and loss is 0.9290971755981445\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m███▉      #033[0m| 111/281 [08:18<12:41,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 111 is completed and loss is 1.1404210329055786\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m███▉      #033[0m| 112/281 [08:23<12:37,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 112 is completed and loss is 1.1147255897521973\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m████      #033[0m| 113/281 [08:27<12:32,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 113 is completed and loss is 1.0660020112991333\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 114/281 [08:32<12:28,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 114 is completed and loss is 1.1040446758270264\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 115/281 [08:36<12:24,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 115 is completed and loss is 0.970947265625\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████▏     #033[0m| 116/281 [08:41<12:19,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 116 is completed and loss is 1.224220871925354\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 117/281 [08:45<12:15,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 117 is completed and loss is 1.0950486660003662\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 118/281 [08:50<12:10,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 118 is completed and loss is 1.1821540594100952\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 119/281 [08:54<12:06,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 119 is completed and loss is 1.0345869064331055\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 120/281 [08:59<12:01,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 120 is completed and loss is 1.0424338579177856\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 121/281 [09:03<11:57,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 121 is completed and loss is 1.1827470064163208\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 122/281 [09:08<11:52,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 122 is completed and loss is 1.3394583463668823\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 123/281 [09:12<11:48,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 123 is completed and loss is 1.0786101818084717\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 124/281 [09:17<11:43,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 124 is completed and loss is 1.2292261123657227\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 125/281 [09:21<11:39,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 125 is completed and loss is 0.9284576177597046\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▍     #033[0m| 126/281 [09:26<11:34,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 126 is completed and loss is 1.1098543405532837\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▌     #033[0m| 127/281 [09:30<11:30,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 127 is completed and loss is 1.0024527311325073\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 128/281 [09:35<11:25,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 128 is completed and loss is 0.9249864220619202\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 129/281 [09:39<11:21,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 129 is completed and loss is 1.122828483581543\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▋     #033[0m| 130/281 [09:44<11:16,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 130 is completed and loss is 1.2924607992172241\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 131/281 [09:48<11:12,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 131 is completed and loss is 0.9718734622001648\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 132/281 [09:53<11:07,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 132 is completed and loss is 0.9540174603462219\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 133/281 [09:57<11:03,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 133 is completed and loss is 0.994335949420929\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  48%|#033[34m████▊     #033[0m| 134/281 [10:02<10:58,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 134 is completed and loss is 1.015572190284729\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  48%|#033[34m████▊     #033[0m| 135/281 [10:06<10:54,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 135 is completed and loss is 1.0505967140197754\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  48%|#033[34m████▊     #033[0m| 136/281 [10:11<10:49,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 136 is completed and loss is 1.119061827659607\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  49%|#033[34m████▉     #033[0m| 137/281 [10:15<10:45,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 137 is completed and loss is 1.0072585344314575\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  49%|#033[34m████▉     #033[0m| 138/281 [10:19<10:40,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 138 is completed and loss is 1.0578577518463135\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  49%|#033[34m████▉     #033[0m| 139/281 [10:24<10:36,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 139 is completed and loss is 1.268079161643982\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  50%|#033[34m████▉     #033[0m| 140/281 [10:28<10:31,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 140 is completed and loss is 1.0317237377166748\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  50%|#033[34m█████     #033[0m| 141/281 [10:33<10:27,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 141 is completed and loss is 1.1448063850402832\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  51%|#033[34m█████     #033[0m| 142/281 [10:37<10:23,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 142 is completed and loss is 0.8924770951271057\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  51%|#033[34m█████     #033[0m| 143/281 [10:42<10:21,  4.50s/it]\u001b[0m\n",
      "\u001b[34mstep 143 is completed and loss is 0.8762581944465637\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  51%|#033[34m█████     #033[0m| 144/281 [10:46<10:15,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 144 is completed and loss is 1.1977287530899048\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  52%|#033[34m█████▏    #033[0m| 145/281 [10:51<10:10,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 145 is completed and loss is 1.0960150957107544\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  52%|#033[34m█████▏    #033[0m| 146/281 [10:55<10:05,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 146 is completed and loss is 1.1088814735412598\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  52%|#033[34m█████▏    #033[0m| 147/281 [11:00<10:01,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 147 is completed and loss is 1.0694259405136108\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  53%|#033[34m█████▎    #033[0m| 148/281 [11:04<09:56,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 148 is completed and loss is 0.9700782895088196\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  53%|#033[34m█████▎    #033[0m| 149/281 [11:09<09:51,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 149 is completed and loss is 1.0567634105682373\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  53%|#033[34m█████▎    #033[0m| 150/281 [11:13<09:47,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 150 is completed and loss is 0.9848166108131409\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  54%|#033[34m█████▎    #033[0m| 151/281 [11:18<09:42,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 151 is completed and loss is 1.1822539567947388\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  54%|#033[34m█████▍    #033[0m| 152/281 [11:22<09:38,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 152 is completed and loss is 1.1129462718963623\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  54%|#033[34m█████▍    #033[0m| 153/281 [11:27<09:33,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 153 is completed and loss is 1.0023541450500488\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  55%|#033[34m█████▍    #033[0m| 154/281 [11:31<09:29,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 154 is completed and loss is 1.018919587135315\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  55%|#033[34m█████▌    #033[0m| 155/281 [11:36<09:24,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 155 is completed and loss is 1.0683581829071045\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  56%|#033[34m█████▌    #033[0m| 156/281 [11:40<09:20,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 156 is completed and loss is 1.07933509349823\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  56%|#033[34m█████▌    #033[0m| 157/281 [11:45<09:15,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 157 is completed and loss is 1.2260453701019287\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  56%|#033[34m█████▌    #033[0m| 158/281 [11:49<09:11,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 158 is completed and loss is 0.9445688724517822\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  57%|#033[34m█████▋    #033[0m| 159/281 [11:54<09:06,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 159 is completed and loss is 0.9172776937484741\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  57%|#033[34m█████▋    #033[0m| 160/281 [11:58<09:02,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 160 is completed and loss is 1.1349433660507202\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  57%|#033[34m█████▋    #033[0m| 161/281 [12:03<08:57,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 161 is completed and loss is 1.1060885190963745\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  58%|#033[34m█████▊    #033[0m| 162/281 [12:07<08:53,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 162 is completed and loss is 1.0340261459350586\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  58%|#033[34m█████▊    #033[0m| 163/281 [12:12<08:48,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 163 is completed and loss is 1.1173282861709595\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  58%|#033[34m█████▊    #033[0m| 164/281 [12:16<08:44,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 164 is completed and loss is 1.032016396522522\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  59%|#033[34m█████▊    #033[0m| 165/281 [12:21<08:39,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 165 is completed and loss is 0.929705023765564\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  59%|#033[34m█████▉    #033[0m| 166/281 [12:25<08:35,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 166 is completed and loss is 0.8445782661437988\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  59%|#033[34m█████▉    #033[0m| 167/281 [12:30<08:30,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 167 is completed and loss is 0.9272836446762085\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  60%|#033[34m█████▉    #033[0m| 168/281 [12:34<08:26,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 168 is completed and loss is 1.0212790966033936\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  60%|#033[34m██████    #033[0m| 169/281 [12:38<08:22,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 169 is completed and loss is 1.0359081029891968\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  60%|#033[34m██████    #033[0m| 170/281 [12:43<08:17,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 170 is completed and loss is 1.1717039346694946\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  61%|#033[34m██████    #033[0m| 171/281 [12:47<08:13,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 171 is completed and loss is 0.9663717150688171\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  61%|#033[34m██████    #033[0m| 172/281 [12:52<08:08,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 172 is completed and loss is 1.109915852546692\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  62%|#033[34m██████▏   #033[0m| 173/281 [12:56<08:04,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 173 is completed and loss is 0.9979501366615295\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  62%|#033[34m██████▏   #033[0m| 174/281 [13:01<07:59,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 174 is completed and loss is 0.9794611930847168\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  62%|#033[34m██████▏   #033[0m| 175/281 [13:05<07:56,  4.50s/it]\u001b[0m\n",
      "\u001b[34mstep 175 is completed and loss is 0.8406416773796082\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  63%|#033[34m██████▎   #033[0m| 176/281 [13:10<07:51,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 176 is completed and loss is 0.993230402469635\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  63%|#033[34m██████▎   #033[0m| 177/281 [13:14<07:46,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 177 is completed and loss is 1.1393773555755615\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  63%|#033[34m██████▎   #033[0m| 178/281 [13:19<07:42,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 178 is completed and loss is 1.0794779062271118\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  64%|#033[34m██████▎   #033[0m| 179/281 [13:23<07:37,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 179 is completed and loss is 1.218705654144287\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  64%|#033[34m██████▍   #033[0m| 180/281 [13:28<07:32,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 180 is completed and loss is 0.9742282032966614\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  64%|#033[34m██████▍   #033[0m| 181/281 [13:32<07:28,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 181 is completed and loss is 1.0752471685409546\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  65%|#033[34m██████▍   #033[0m| 182/281 [13:37<07:23,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 182 is completed and loss is 1.1074931621551514\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  65%|#033[34m██████▌   #033[0m| 183/281 [13:41<07:19,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 183 is completed and loss is 1.1667912006378174\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  65%|#033[34m██████▌   #033[0m| 184/281 [13:46<07:14,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 184 is completed and loss is 0.9509034156799316\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  66%|#033[34m██████▌   #033[0m| 185/281 [13:50<07:10,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 185 is completed and loss is 0.9361433386802673\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  66%|#033[34m██████▌   #033[0m| 186/281 [13:55<07:05,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 186 is completed and loss is 1.0073531866073608\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  67%|#033[34m██████▋   #033[0m| 187/281 [13:59<07:01,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 187 is completed and loss is 1.0594958066940308\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  67%|#033[34m██████▋   #033[0m| 188/281 [14:04<06:56,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 188 is completed and loss is 1.1333078145980835\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  67%|#033[34m██████▋   #033[0m| 189/281 [14:08<06:52,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 189 is completed and loss is 1.1283208131790161\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  68%|#033[34m██████▊   #033[0m| 190/281 [14:13<06:47,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 190 is completed and loss is 0.9872233867645264\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  68%|#033[34m██████▊   #033[0m| 191/281 [14:17<06:43,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 191 is completed and loss is 1.0464112758636475\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  68%|#033[34m██████▊   #033[0m| 192/281 [14:22<06:38,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 192 is completed and loss is 0.9784444570541382\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  69%|#033[34m██████▊   #033[0m| 193/281 [14:26<06:34,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 193 is completed and loss is 1.0306613445281982\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  69%|#033[34m██████▉   #033[0m| 194/281 [14:31<06:29,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 194 is completed and loss is 0.9496269226074219\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  69%|#033[34m██████▉   #033[0m| 195/281 [14:35<06:25,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 195 is completed and loss is 1.0706199407577515\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  70%|#033[34m██████▉   #033[0m| 196/281 [14:40<06:20,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 196 is completed and loss is 1.0755423307418823\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  70%|#033[34m███████   #033[0m| 197/281 [14:44<06:16,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 197 is completed and loss is 0.9826394319534302\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  70%|#033[34m███████   #033[0m| 198/281 [14:49<06:11,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 198 is completed and loss is 1.11014723777771\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  71%|#033[34m███████   #033[0m| 199/281 [14:53<06:07,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 199 is completed and loss is 0.9625598192214966\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  71%|#033[34m███████   #033[0m| 200/281 [14:57<06:03,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 200 is completed and loss is 1.0696947574615479\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  72%|#033[34m███████▏  #033[0m| 201/281 [15:02<05:58,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 201 is completed and loss is 1.1308233737945557\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  72%|#033[34m███████▏  #033[0m| 202/281 [15:06<05:54,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 202 is completed and loss is 1.040877342224121\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  72%|#033[34m███████▏  #033[0m| 203/281 [15:11<05:49,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 203 is completed and loss is 1.1395132541656494\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  73%|#033[34m███████▎  #033[0m| 204/281 [15:15<05:45,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 204 is completed and loss is 1.1006933450698853\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  73%|#033[34m███████▎  #033[0m| 205/281 [15:20<05:40,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 205 is completed and loss is 1.1424840688705444\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  73%|#033[34m███████▎  #033[0m| 206/281 [15:24<05:36,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 206 is completed and loss is 1.013291835784912\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  74%|#033[34m███████▎  #033[0m| 207/281 [15:29<05:33,  4.50s/it]\u001b[0m\n",
      "\u001b[34mstep 207 is completed and loss is 1.3049628734588623\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  74%|#033[34m███████▍  #033[0m| 208/281 [15:33<05:28,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 208 is completed and loss is 1.0197309255599976\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  74%|#033[34m███████▍  #033[0m| 209/281 [15:38<05:23,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 209 is completed and loss is 0.9649063348770142\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  75%|#033[34m███████▍  #033[0m| 210/281 [15:42<05:18,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 210 is completed and loss is 1.0229004621505737\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  75%|#033[34m███████▌  #033[0m| 211/281 [15:47<05:14,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 211 is completed and loss is 1.0301676988601685\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  75%|#033[34m███████▌  #033[0m| 212/281 [15:51<05:09,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 212 is completed and loss is 1.0382932424545288\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  76%|#033[34m███████▌  #033[0m| 213/281 [15:56<05:04,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 213 is completed and loss is 1.2677186727523804\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  76%|#033[34m███████▌  #033[0m| 214/281 [16:00<05:00,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 214 is completed and loss is 1.0359519720077515\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  77%|#033[34m███████▋  #033[0m| 215/281 [16:05<04:55,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 215 is completed and loss is 1.0683480501174927\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  77%|#033[34m███████▋  #033[0m| 216/281 [16:09<04:51,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 216 is completed and loss is 1.1591405868530273\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  77%|#033[34m███████▋  #033[0m| 217/281 [16:14<04:46,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 217 is completed and loss is 1.1272320747375488\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  78%|#033[34m███████▊  #033[0m| 218/281 [16:18<04:42,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 218 is completed and loss is 0.8547550439834595\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  78%|#033[34m███████▊  #033[0m| 219/281 [16:23<04:37,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 219 is completed and loss is 0.9056669473648071\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  78%|#033[34m███████▊  #033[0m| 220/281 [16:27<04:33,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 220 is completed and loss is 0.9118136167526245\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  79%|#033[34m███████▊  #033[0m| 221/281 [16:32<04:28,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 221 is completed and loss is 1.008402943611145\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  79%|#033[34m███████▉  #033[0m| 222/281 [16:36<04:24,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 222 is completed and loss is 1.027394413948059\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  79%|#033[34m███████▉  #033[0m| 223/281 [16:41<04:19,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 223 is completed and loss is 1.0026466846466064\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  80%|#033[34m███████▉  #033[0m| 224/281 [16:45<04:15,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 224 is completed and loss is 1.0080665349960327\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  80%|#033[34m████████  #033[0m| 225/281 [16:50<04:11,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 225 is completed and loss is 0.9512068033218384\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  80%|#033[34m████████  #033[0m| 226/281 [16:54<04:06,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 226 is completed and loss is 1.2660021781921387\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  81%|#033[34m████████  #033[0m| 227/281 [16:59<04:02,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 227 is completed and loss is 0.8535590171813965\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  81%|#033[34m████████  #033[0m| 228/281 [17:03<03:57,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 228 is completed and loss is 1.1028977632522583\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  81%|#033[34m████████▏ #033[0m| 229/281 [17:08<03:53,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 229 is completed and loss is 0.9302698969841003\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  82%|#033[34m████████▏ #033[0m| 230/281 [17:12<03:48,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 230 is completed and loss is 1.0091023445129395\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  82%|#033[34m████████▏ #033[0m| 231/281 [17:16<03:44,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 231 is completed and loss is 0.9855192303657532\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  83%|#033[34m████████▎ #033[0m| 232/281 [17:21<03:39,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 232 is completed and loss is 1.0546092987060547\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  83%|#033[34m████████▎ #033[0m| 233/281 [17:25<03:35,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 233 is completed and loss is 1.1038622856140137\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  83%|#033[34m████████▎ #033[0m| 234/281 [17:30<03:30,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 234 is completed and loss is 1.029031753540039\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  84%|#033[34m████████▎ #033[0m| 235/281 [17:34<03:26,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 235 is completed and loss is 1.0525238513946533\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  84%|#033[34m████████▍ #033[0m| 236/281 [17:39<03:21,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 236 is completed and loss is 0.906021773815155\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  84%|#033[34m████████▍ #033[0m| 237/281 [17:43<03:17,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 237 is completed and loss is 0.9879384636878967\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  85%|#033[34m████████▍ #033[0m| 238/281 [17:48<03:12,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 238 is completed and loss is 1.0287890434265137\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  85%|#033[34m████████▌ #033[0m| 239/281 [17:52<03:09,  4.50s/it]\u001b[0m\n",
      "\u001b[34mstep 239 is completed and loss is 1.06106698513031\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  85%|#033[34m████████▌ #033[0m| 240/281 [17:57<03:04,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 240 is completed and loss is 0.9930769801139832\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  86%|#033[34m████████▌ #033[0m| 241/281 [18:01<02:59,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 241 is completed and loss is 1.168223261833191\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  86%|#033[34m████████▌ #033[0m| 242/281 [18:06<02:55,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 242 is completed and loss is 1.0377296209335327\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  86%|#033[34m████████▋ #033[0m| 243/281 [18:10<02:50,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 243 is completed and loss is 0.8462530970573425\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  87%|#033[34m████████▋ #033[0m| 244/281 [18:15<02:45,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 244 is completed and loss is 1.0720772743225098\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  87%|#033[34m████████▋ #033[0m| 245/281 [18:19<02:41,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 245 is completed and loss is 0.9452307224273682\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  88%|#033[34m████████▊ #033[0m| 246/281 [18:24<02:36,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 246 is completed and loss is 0.9873185753822327\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  88%|#033[34m████████▊ #033[0m| 247/281 [18:28<02:32,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 247 is completed and loss is 1.1121838092803955\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  88%|#033[34m████████▊ #033[0m| 248/281 [18:33<02:27,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 248 is completed and loss is 1.0486369132995605\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  89%|#033[34m████████▊ #033[0m| 249/281 [18:37<02:23,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 249 is completed and loss is 1.2316784858703613\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  89%|#033[34m████████▉ #033[0m| 250/281 [18:42<02:18,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 250 is completed and loss is 1.1132457256317139\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  89%|#033[34m████████▉ #033[0m| 251/281 [18:46<02:14,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 251 is completed and loss is 1.1031380891799927\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  90%|#033[34m████████▉ #033[0m| 252/281 [18:51<02:09,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 252 is completed and loss is 1.0024094581604004\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  90%|#033[34m█████████ #033[0m| 253/281 [18:55<02:05,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 253 is completed and loss is 1.0292285680770874\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  90%|#033[34m█████████ #033[0m| 254/281 [19:00<02:01,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 254 is completed and loss is 0.7783821225166321\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  91%|#033[34m█████████ #033[0m| 255/281 [19:04<01:56,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 255 is completed and loss is 0.9574202299118042\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  91%|#033[34m█████████ #033[0m| 256/281 [19:09<01:52,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 256 is completed and loss is 1.0262391567230225\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  91%|#033[34m█████████▏#033[0m| 257/281 [19:13<01:47,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 257 is completed and loss is 1.193070411682129\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  92%|#033[34m█████████▏#033[0m| 258/281 [19:18<01:43,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 258 is completed and loss is 1.0642280578613281\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  92%|#033[34m█████████▏#033[0m| 259/281 [19:22<01:38,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 259 is completed and loss is 1.046230673789978\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 260/281 [19:27<01:34,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 260 is completed and loss is 1.105485200881958\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 261/281 [19:31<01:29,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 261 is completed and loss is 0.9999955892562866\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 262/281 [19:35<01:25,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 262 is completed and loss is 1.0263087749481201\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  94%|#033[34m█████████▎#033[0m| 263/281 [19:40<01:20,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 263 is completed and loss is 1.0187278985977173\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  94%|#033[34m█████████▍#033[0m| 264/281 [19:44<01:16,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 264 is completed and loss is 0.9834884405136108\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  94%|#033[34m█████████▍#033[0m| 265/281 [19:49<01:11,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 265 is completed and loss is 0.9432235360145569\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  95%|#033[34m█████████▍#033[0m| 266/281 [19:53<01:07,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 266 is completed and loss is 0.9315500259399414\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  95%|#033[34m█████████▌#033[0m| 267/281 [19:58<01:02,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 267 is completed and loss is 0.8971851468086243\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  95%|#033[34m█████████▌#033[0m| 268/281 [20:02<00:58,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 268 is completed and loss is 1.167547583580017\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  96%|#033[34m█████████▌#033[0m| 269/281 [20:07<00:53,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 269 is completed and loss is 1.0228307247161865\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  96%|#033[34m█████████▌#033[0m| 270/281 [20:11<00:49,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 270 is completed and loss is 0.8772832751274109\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  96%|#033[34m█████████▋#033[0m| 271/281 [20:16<00:44,  4.50s/it]\u001b[0m\n",
      "\u001b[34mstep 271 is completed and loss is 0.9185353517532349\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  97%|#033[34m█████████▋#033[0m| 272/281 [20:20<00:40,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 272 is completed and loss is 1.1287201642990112\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  97%|#033[34m█████████▋#033[0m| 273/281 [20:25<00:35,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 273 is completed and loss is 0.9001947641372681\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  98%|#033[34m█████████▊#033[0m| 274/281 [20:29<00:31,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 274 is completed and loss is 1.080554485321045\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  98%|#033[34m█████████▊#033[0m| 275/281 [20:34<00:26,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 275 is completed and loss is 1.0069034099578857\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  98%|#033[34m█████████▊#033[0m| 276/281 [20:38<00:22,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 276 is completed and loss is 0.9163243174552917\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  99%|#033[34m█████████▊#033[0m| 277/281 [20:43<00:17,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 277 is completed and loss is 1.0397999286651611\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  99%|#033[34m█████████▉#033[0m| 278/281 [20:47<00:13,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 278 is completed and loss is 1.0855659246444702\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  99%|#033[34m█████████▉#033[0m| 279/281 [20:52<00:08,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 279 is completed and loss is 1.066939353942871\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m█████████▉#033[0m| 280/281 [20:56<00:04,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 280 is completed and loss is 1.0397025346755981\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 281/281 [21:01<00:00,  4.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 281/281 [21:01<00:00,  4.49s/it]\u001b[0m\n",
      "\u001b[34mMax CUDA memory allocated was 17 GB\u001b[0m\n",
      "\u001b[34mMax CUDA memory reserved was 18 GB\u001b[0m\n",
      "\u001b[34mPeak active CUDA memory was 17 GB\u001b[0m\n",
      "\u001b[34mCuda Malloc retires : 0\u001b[0m\n",
      "\u001b[34mCPU Total Peak Memory consumed during the train (max): 1 GB\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/282 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 1/282 [00:00<02:01,  2.31it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   1%|#033[32m          #033[0m| 2/282 [00:00<02:00,  2.32it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   1%|#033[32m          #033[0m| 3/282 [00:01<01:59,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   1%|#033[32m▏         #033[0m| 4/282 [00:01<01:59,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   2%|#033[32m▏         #033[0m| 5/282 [00:02<01:59,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   2%|#033[32m▏         #033[0m| 6/282 [00:02<01:58,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   2%|#033[32m▏         #033[0m| 7/282 [00:03<01:57,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 8/282 [00:03<01:57,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 9/282 [00:03<01:57,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 10/282 [00:04<01:56,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▍         #033[0m| 11/282 [00:04<01:56,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▍         #033[0m| 12/282 [00:05<01:55,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   5%|#033[32m▍         #033[0m| 13/282 [00:05<01:55,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   5%|#033[32m▍         #033[0m| 14/282 [00:06<01:54,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   5%|#033[32m▌         #033[0m| 15/282 [00:06<01:54,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   6%|#033[32m▌         #033[0m| 16/282 [00:06<01:53,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   6%|#033[32m▌         #033[0m| 17/282 [00:07<01:53,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   6%|#033[32m▋         #033[0m| 18/282 [00:07<01:53,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 19/282 [00:08<01:52,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 20/282 [00:08<01:52,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 21/282 [00:09<01:51,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   8%|#033[32m▊         #033[0m| 22/282 [00:09<01:51,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   8%|#033[32m▊         #033[0m| 23/282 [00:09<01:50,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   9%|#033[32m▊         #033[0m| 24/282 [00:10<01:50,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   9%|#033[32m▉         #033[0m| 25/282 [00:10<01:50,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   9%|#033[32m▉         #033[0m| 26/282 [00:11<01:49,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  10%|#033[32m▉         #033[0m| 27/282 [00:11<01:49,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  10%|#033[32m▉         #033[0m| 28/282 [00:12<01:48,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  10%|#033[32m█         #033[0m| 29/282 [00:12<01:48,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 30/282 [00:12<01:47,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 31/282 [00:13<01:47,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█▏        #033[0m| 32/282 [00:13<01:47,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  12%|#033[32m█▏        #033[0m| 33/282 [00:14<01:46,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  12%|#033[32m█▏        #033[0m| 34/282 [00:14<01:46,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  12%|#033[32m█▏        #033[0m| 35/282 [00:15<01:45,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  13%|#033[32m█▎        #033[0m| 36/282 [00:15<01:45,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  13%|#033[32m█▎        #033[0m| 37/282 [00:15<01:44,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  13%|#033[32m█▎        #033[0m| 38/282 [00:16<01:44,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 39/282 [00:16<01:44,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 40/282 [00:17<01:43,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  15%|#033[32m█▍        #033[0m| 41/282 [00:17<01:43,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  15%|#033[32m█▍        #033[0m| 42/282 [00:18<01:42,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  15%|#033[32m█▌        #033[0m| 43/282 [00:18<01:42,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  16%|#033[32m█▌        #033[0m| 44/282 [00:18<01:41,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  16%|#033[32m█▌        #033[0m| 45/282 [00:19<01:41,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  16%|#033[32m█▋        #033[0m| 46/282 [00:19<01:41,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 47/282 [00:20<01:40,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 48/282 [00:20<01:40,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 49/282 [00:21<01:39,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 50/282 [00:21<01:39,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 51/282 [00:21<01:38,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 52/282 [00:22<01:38,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  19%|#033[32m█▉        #033[0m| 53/282 [00:22<01:38,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  19%|#033[32m█▉        #033[0m| 54/282 [00:23<01:37,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  20%|#033[32m█▉        #033[0m| 55/282 [00:23<01:37,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  20%|#033[32m█▉        #033[0m| 56/282 [00:23<01:36,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  20%|#033[32m██        #033[0m| 57/282 [00:24<01:36,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██        #033[0m| 58/282 [00:24<01:35,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██        #033[0m| 59/282 [00:25<01:35,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 60/282 [00:25<01:35,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  22%|#033[32m██▏       #033[0m| 61/282 [00:26<01:34,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  22%|#033[32m██▏       #033[0m| 62/282 [00:26<01:34,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  22%|#033[32m██▏       #033[0m| 63/282 [00:26<01:33,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  23%|#033[32m██▎       #033[0m| 64/282 [00:27<01:33,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  23%|#033[32m██▎       #033[0m| 65/282 [00:27<01:33,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  23%|#033[32m██▎       #033[0m| 66/282 [00:28<01:32,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  24%|#033[32m██▍       #033[0m| 67/282 [00:28<01:32,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  24%|#033[32m██▍       #033[0m| 68/282 [00:29<01:31,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  24%|#033[32m██▍       #033[0m| 69/282 [00:29<01:31,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m██▍       #033[0m| 70/282 [00:29<01:30,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 71/282 [00:30<01:30,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  26%|#033[32m██▌       #033[0m| 72/282 [00:30<01:29,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  26%|#033[32m██▌       #033[0m| 73/282 [00:31<01:29,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  26%|#033[32m██▌       #033[0m| 74/282 [00:31<01:29,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  27%|#033[32m██▋       #033[0m| 75/282 [00:32<01:28,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  27%|#033[32m██▋       #033[0m| 76/282 [00:32<01:28,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  27%|#033[32m██▋       #033[0m| 77/282 [00:32<01:27,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  28%|#033[32m██▊       #033[0m| 78/282 [00:33<01:27,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  28%|#033[32m██▊       #033[0m| 79/282 [00:33<01:26,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  28%|#033[32m██▊       #033[0m| 80/282 [00:34<01:26,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 81/282 [00:34<01:26,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▉       #033[0m| 82/282 [00:35<01:25,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▉       #033[0m| 83/282 [00:35<01:25,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  30%|#033[32m██▉       #033[0m| 84/282 [00:35<01:24,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  30%|#033[32m███       #033[0m| 85/282 [00:36<01:24,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  30%|#033[32m███       #033[0m| 86/282 [00:36<01:23,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  31%|#033[32m███       #033[0m| 87/282 [00:37<01:23,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  31%|#033[32m███       #033[0m| 88/282 [00:37<01:23,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 89/282 [00:38<01:22,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 90/282 [00:38<01:22,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 91/282 [00:38<01:21,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  33%|#033[32m███▎      #033[0m| 92/282 [00:39<01:21,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  33%|#033[32m███▎      #033[0m| 93/282 [00:39<01:20,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  33%|#033[32m███▎      #033[0m| 94/282 [00:40<01:20,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▎      #033[0m| 95/282 [00:40<01:20,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▍      #033[0m| 96/282 [00:41<01:19,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▍      #033[0m| 97/282 [00:41<01:19,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  35%|#033[32m███▍      #033[0m| 98/282 [00:41<01:18,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  35%|#033[32m███▌      #033[0m| 99/282 [00:42<01:18,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  35%|#033[32m███▌      #033[0m| 100/282 [00:42<01:17,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 101/282 [00:43<01:17,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 102/282 [00:43<01:17,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  37%|#033[32m███▋      #033[0m| 103/282 [00:44<01:16,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  37%|#033[32m███▋      #033[0m| 104/282 [00:44<01:16,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  37%|#033[32m███▋      #033[0m| 105/282 [00:44<01:15,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  38%|#033[32m███▊      #033[0m| 106/282 [00:45<01:15,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  38%|#033[32m███▊      #033[0m| 107/282 [00:45<01:14,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  38%|#033[32m███▊      #033[0m| 108/282 [00:46<01:14,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▊      #033[0m| 109/282 [00:46<01:14,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 110/282 [00:47<01:13,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 111/282 [00:47<01:13,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  40%|#033[32m███▉      #033[0m| 112/282 [00:47<01:12,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  40%|#033[32m████      #033[0m| 113/282 [00:48<01:12,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  40%|#033[32m████      #033[0m| 114/282 [00:48<01:11,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  41%|#033[32m████      #033[0m| 115/282 [00:49<01:11,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  41%|#033[32m████      #033[0m| 116/282 [00:49<01:11,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  41%|#033[32m████▏     #033[0m| 117/282 [00:50<01:10,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  42%|#033[32m████▏     #033[0m| 118/282 [00:50<01:10,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  42%|#033[32m████▏     #033[0m| 119/282 [00:50<01:09,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 120/282 [00:51<01:09,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 121/282 [00:51<01:09,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 122/282 [00:52<01:08,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  44%|#033[32m████▎     #033[0m| 123/282 [00:52<01:08,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  44%|#033[32m████▍     #033[0m| 124/282 [00:53<01:07,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  44%|#033[32m████▍     #033[0m| 125/282 [00:53<01:07,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m████▍     #033[0m| 126/282 [00:53<01:06,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m████▌     #033[0m| 127/282 [00:54<01:06,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m████▌     #033[0m| 128/282 [00:54<01:05,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▌     #033[0m| 129/282 [00:55<01:05,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▌     #033[0m| 130/282 [00:55<01:05,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 131/282 [00:56<01:04,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  47%|#033[32m████▋     #033[0m| 132/282 [00:56<01:04,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  47%|#033[32m████▋     #033[0m| 133/282 [00:56<01:03,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 134/282 [00:57<01:03,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 135/282 [00:57<01:02,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 136/282 [00:58<01:02,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  49%|#033[32m████▊     #033[0m| 137/282 [00:58<01:02,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  49%|#033[32m████▉     #033[0m| 138/282 [00:59<01:01,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  49%|#033[32m████▉     #033[0m| 139/282 [00:59<01:01,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m████▉     #033[0m| 140/282 [00:59<01:00,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 141/282 [01:00<01:00,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 142/282 [01:00<00:59,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  51%|#033[32m█████     #033[0m| 143/282 [01:01<00:59,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  51%|#033[32m█████     #033[0m| 144/282 [01:01<00:59,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  51%|#033[32m█████▏    #033[0m| 145/282 [01:02<00:58,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 146/282 [01:02<00:58,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 147/282 [01:02<00:57,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 148/282 [01:03<00:57,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  53%|#033[32m█████▎    #033[0m| 149/282 [01:03<00:56,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  53%|#033[32m█████▎    #033[0m| 150/282 [01:04<00:56,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 151/282 [01:04<00:56,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▍    #033[0m| 152/282 [01:05<00:55,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▍    #033[0m| 153/282 [01:05<00:55,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m█████▍    #033[0m| 154/282 [01:05<00:54,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m█████▍    #033[0m| 155/282 [01:06<00:54,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m█████▌    #033[0m| 156/282 [01:06<00:53,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  56%|#033[32m█████▌    #033[0m| 157/282 [01:07<00:53,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  56%|#033[32m█████▌    #033[0m| 158/282 [01:07<00:53,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  56%|#033[32m█████▋    #033[0m| 159/282 [01:08<00:52,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 160/282 [01:08<00:52,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 161/282 [01:08<00:51,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 162/282 [01:09<00:51,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  58%|#033[32m█████▊    #033[0m| 163/282 [01:09<00:51,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  58%|#033[32m█████▊    #033[0m| 164/282 [01:10<00:50,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  59%|#033[32m█████▊    #033[0m| 165/282 [01:10<00:50,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  59%|#033[32m█████▉    #033[0m| 166/282 [01:11<00:49,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  59%|#033[32m█████▉    #033[0m| 167/282 [01:11<00:49,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  60%|#033[32m█████▉    #033[0m| 168/282 [01:11<00:48,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  60%|#033[32m█████▉    #033[0m| 169/282 [01:12<00:48,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  60%|#033[32m██████    #033[0m| 170/282 [01:12<00:47,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 171/282 [01:13<00:47,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 172/282 [01:13<00:47,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████▏   #033[0m| 173/282 [01:14<00:46,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  62%|#033[32m██████▏   #033[0m| 174/282 [01:14<00:46,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  62%|#033[32m██████▏   #033[0m| 175/282 [01:14<00:45,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  62%|#033[32m██████▏   #033[0m| 176/282 [01:15<00:45,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  63%|#033[32m██████▎   #033[0m| 177/282 [01:15<00:44,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  63%|#033[32m██████▎   #033[0m| 178/282 [01:16<00:44,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  63%|#033[32m██████▎   #033[0m| 179/282 [01:16<00:44,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 180/282 [01:17<00:43,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 181/282 [01:17<00:43,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  65%|#033[32m██████▍   #033[0m| 182/282 [01:17<00:42,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  65%|#033[32m██████▍   #033[0m| 183/282 [01:18<00:42,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  65%|#033[32m██████▌   #033[0m| 184/282 [01:18<00:41,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▌   #033[0m| 185/282 [01:19<00:41,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▌   #033[0m| 186/282 [01:19<00:41,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▋   #033[0m| 187/282 [01:20<00:40,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  67%|#033[32m██████▋   #033[0m| 188/282 [01:20<00:40,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  67%|#033[32m██████▋   #033[0m| 189/282 [01:20<00:39,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  67%|#033[32m██████▋   #033[0m| 190/282 [01:21<00:39,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 191/282 [01:21<00:38,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 192/282 [01:22<00:38,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 193/282 [01:22<00:38,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  69%|#033[32m██████▉   #033[0m| 194/282 [01:23<00:37,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  69%|#033[32m██████▉   #033[0m| 195/282 [01:23<00:37,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  70%|#033[32m██████▉   #033[0m| 196/282 [01:23<00:36,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  70%|#033[32m██████▉   #033[0m| 197/282 [01:24<00:36,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  70%|#033[32m███████   #033[0m| 198/282 [01:24<00:35,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████   #033[0m| 199/282 [01:25<00:35,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████   #033[0m| 200/282 [01:25<00:35,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 201/282 [01:26<00:34,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  72%|#033[32m███████▏  #033[0m| 202/282 [01:26<00:34,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  72%|#033[32m███████▏  #033[0m| 203/282 [01:26<00:33,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  72%|#033[32m███████▏  #033[0m| 204/282 [01:27<00:33,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  73%|#033[32m███████▎  #033[0m| 205/282 [01:27<00:33,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  73%|#033[32m███████▎  #033[0m| 206/282 [01:28<00:32,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  73%|#033[32m███████▎  #033[0m| 207/282 [01:28<00:32,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  74%|#033[32m███████▍  #033[0m| 208/282 [01:29<00:31,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  74%|#033[32m███████▍  #033[0m| 209/282 [01:29<00:31,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  74%|#033[32m███████▍  #033[0m| 210/282 [01:29<00:30,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  75%|#033[32m███████▍  #033[0m| 211/282 [01:30<00:30,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 212/282 [01:30<00:29,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  76%|#033[32m███████▌  #033[0m| 213/282 [01:31<00:29,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  76%|#033[32m███████▌  #033[0m| 214/282 [01:31<00:29,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  76%|#033[32m███████▌  #033[0m| 215/282 [01:32<00:28,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  77%|#033[32m███████▋  #033[0m| 216/282 [01:32<00:28,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  77%|#033[32m███████▋  #033[0m| 217/282 [01:32<00:27,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  77%|#033[32m███████▋  #033[0m| 218/282 [01:33<00:27,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  78%|#033[32m███████▊  #033[0m| 219/282 [01:33<00:26,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  78%|#033[32m███████▊  #033[0m| 220/282 [01:34<00:26,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  78%|#033[32m███████▊  #033[0m| 221/282 [01:34<00:26,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 222/282 [01:35<00:25,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▉  #033[0m| 223/282 [01:35<00:25,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▉  #033[0m| 224/282 [01:35<00:24,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  80%|#033[32m███████▉  #033[0m| 225/282 [01:36<00:24,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  80%|#033[32m████████  #033[0m| 226/282 [01:36<00:23,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  80%|#033[32m████████  #033[0m| 227/282 [01:37<00:23,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  81%|#033[32m████████  #033[0m| 228/282 [01:37<00:23,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  81%|#033[32m████████  #033[0m| 229/282 [01:38<00:22,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 230/282 [01:38<00:22,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 231/282 [01:38<00:21,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 232/282 [01:39<00:21,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 233/282 [01:39<00:20,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 234/282 [01:40<00:20,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 235/282 [01:40<00:20,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  84%|#033[32m████████▎ #033[0m| 236/282 [01:41<00:19,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  84%|#033[32m████████▍ #033[0m| 237/282 [01:41<00:19,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  84%|#033[32m████████▍ #033[0m| 238/282 [01:41<00:18,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  85%|#033[32m████████▍ #033[0m| 239/282 [01:42<00:18,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  85%|#033[32m████████▌ #033[0m| 240/282 [01:42<00:18,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  85%|#033[32m████████▌ #033[0m| 241/282 [01:43<00:17,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 242/282 [01:43<00:17,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 243/282 [01:44<00:16,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  87%|#033[32m████████▋ #033[0m| 244/282 [01:44<00:16,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  87%|#033[32m████████▋ #033[0m| 245/282 [01:44<00:15,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  87%|#033[32m████████▋ #033[0m| 246/282 [01:45<00:15,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  88%|#033[32m████████▊ #033[0m| 247/282 [01:45<00:14,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  88%|#033[32m████████▊ #033[0m| 248/282 [01:46<00:14,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  88%|#033[32m████████▊ #033[0m| 249/282 [01:46<00:14,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▊ #033[0m| 250/282 [01:47<00:13,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 251/282 [01:47<00:13,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 252/282 [01:47<00:12,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  90%|#033[32m████████▉ #033[0m| 253/282 [01:48<00:12,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  90%|#033[32m█████████ #033[0m| 254/282 [01:48<00:11,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  90%|#033[32m█████████ #033[0m| 255/282 [01:49<00:11,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  91%|#033[32m█████████ #033[0m| 256/282 [01:49<00:11,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  91%|#033[32m█████████ #033[0m| 257/282 [01:50<00:10,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  91%|#033[32m█████████▏#033[0m| 258/282 [01:50<00:10,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  92%|#033[32m█████████▏#033[0m| 259/282 [01:50<00:09,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  92%|#033[32m█████████▏#033[0m| 260/282 [01:51<00:09,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 261/282 [01:51<00:08,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 262/282 [01:52<00:08,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 263/282 [01:52<00:08,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  94%|#033[32m█████████▎#033[0m| 264/282 [01:53<00:07,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  94%|#033[32m█████████▍#033[0m| 265/282 [01:53<00:07,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  94%|#033[32m█████████▍#033[0m| 266/282 [01:53<00:06,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  95%|#033[32m█████████▍#033[0m| 267/282 [01:54<00:06,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  95%|#033[32m█████████▌#033[0m| 268/282 [01:54<00:05,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  95%|#033[32m█████████▌#033[0m| 269/282 [01:55<00:05,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▌#033[0m| 270/282 [01:55<00:05,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▌#033[0m| 271/282 [01:56<00:04,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 272/282 [01:56<00:04,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  97%|#033[32m█████████▋#033[0m| 273/282 [01:56<00:03,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  97%|#033[32m█████████▋#033[0m| 274/282 [01:57<00:03,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  98%|#033[32m█████████▊#033[0m| 275/282 [01:57<00:03,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  98%|#033[32m█████████▊#033[0m| 276/282 [01:58<00:02,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  98%|#033[32m█████████▊#033[0m| 277/282 [01:58<00:02,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  99%|#033[32m█████████▊#033[0m| 278/282 [01:59<00:01,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  99%|#033[32m█████████▉#033[0m| 279/282 [01:59<00:01,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  99%|#033[32m█████████▉#033[0m| 280/282 [01:59<00:00,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m█████████▉#033[0m| 281/282 [02:00<00:00,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 282/282 [02:00<00:00,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 282/282 [02:00<00:00,  2.33it/s]\u001b[0m\n",
      "\u001b[34meval_ppl=tensor(2.8849, device='cuda:0') eval_epoch_loss=tensor(1.0595, device='cuda:0')\u001b[0m\n",
      "\u001b[34mwe are about to save the PEFT modules\u001b[0m\n",
      "\u001b[34mPEFT modules are saved in saved_peft_model directory\u001b[0m\n",
      "\u001b[34mbest eval loss on epoch 0 is 1.0594983100891113\u001b[0m\n",
      "\u001b[34mEpoch 1: train_perplexity=2.9886, train_epoch_loss=1.0948, epcoh time 1261.4845127540002s\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   0%|#033[34m          #033[0m| 0/281 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mstep 0 is completed and loss is 1.0354236364364624\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   0%|#033[34m          #033[0m| 1/281 [00:04<20:57,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 1 is completed and loss is 0.9722933173179626\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   1%|#033[34m          #033[0m| 2/281 [00:08<20:51,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 2 is completed and loss is 0.9992308020591736\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   1%|#033[34m          #033[0m| 3/281 [00:13<20:46,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 3 is completed and loss is 0.972752571105957\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   1%|#033[34m▏         #033[0m| 4/281 [00:17<20:42,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 4 is completed and loss is 0.9807548522949219\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   2%|#033[34m▏         #033[0m| 5/281 [00:22<20:37,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 5 is completed and loss is 1.1553043127059937\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   2%|#033[34m▏         #033[0m| 6/281 [00:26<20:33,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 6 is completed and loss is 1.025158166885376\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   2%|#033[34m▏         #033[0m| 7/281 [00:31<20:28,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 7 is completed and loss is 0.9920641183853149\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   3%|#033[34m▎         #033[0m| 8/281 [00:35<20:23,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 8 is completed and loss is 0.9983949065208435\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   3%|#033[34m▎         #033[0m| 9/281 [00:40<20:19,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 9 is completed and loss is 0.9356402158737183\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   4%|#033[34m▎         #033[0m| 10/281 [00:44<20:14,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 10 is completed and loss is 1.2481237649917603\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   4%|#033[34m▍         #033[0m| 11/281 [00:49<20:10,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 11 is completed and loss is 1.1997065544128418\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   4%|#033[34m▍         #033[0m| 12/281 [00:53<20:06,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 12 is completed and loss is 1.0198934078216553\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   5%|#033[34m▍         #033[0m| 13/281 [00:58<20:01,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 13 is completed and loss is 1.1254076957702637\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   5%|#033[34m▍         #033[0m| 14/281 [01:02<19:57,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 14 is completed and loss is 0.9217645525932312\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   5%|#033[34m▌         #033[0m| 15/281 [01:07<19:52,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 15 is completed and loss is 0.9460064768791199\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   6%|#033[34m▌         #033[0m| 16/281 [01:11<19:48,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 16 is completed and loss is 0.8707846999168396\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   6%|#033[34m▌         #033[0m| 17/281 [01:16<19:43,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 17 is completed and loss is 1.0993539094924927\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   6%|#033[34m▋         #033[0m| 18/281 [01:20<19:39,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 18 is completed and loss is 0.9982371926307678\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   7%|#033[34m▋         #033[0m| 19/281 [01:25<19:34,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 19 is completed and loss is 1.046419620513916\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   7%|#033[34m▋         #033[0m| 20/281 [01:29<19:30,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 20 is completed and loss is 1.1370923519134521\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   7%|#033[34m▋         #033[0m| 21/281 [01:34<19:25,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 21 is completed and loss is 1.0031622648239136\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   8%|#033[34m▊         #033[0m| 22/281 [01:38<19:21,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 22 is completed and loss is 0.9811878800392151\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   8%|#033[34m▊         #033[0m| 23/281 [01:43<19:16,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 23 is completed and loss is 1.033440113067627\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   9%|#033[34m▊         #033[0m| 24/281 [01:47<19:12,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 24 is completed and loss is 1.02543306350708\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   9%|#033[34m▉         #033[0m| 25/281 [01:52<19:07,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 25 is completed and loss is 0.9314866662025452\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   9%|#033[34m▉         #033[0m| 26/281 [01:56<19:03,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 26 is completed and loss is 1.1180840730667114\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  10%|#033[34m▉         #033[0m| 27/281 [02:01<18:58,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 27 is completed and loss is 0.8215563893318176\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  10%|#033[34m▉         #033[0m| 28/281 [02:05<18:54,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 28 is completed and loss is 1.0526186227798462\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  10%|#033[34m█         #033[0m| 29/281 [02:10<18:49,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 29 is completed and loss is 1.0497713088989258\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  11%|#033[34m█         #033[0m| 30/281 [02:14<18:45,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 30 is completed and loss is 1.032474398612976\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  11%|#033[34m█         #033[0m| 31/281 [02:18<18:40,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 31 is completed and loss is 0.9036411046981812\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  11%|#033[34m█▏        #033[0m| 32/281 [02:23<18:36,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 32 is completed and loss is 1.0931566953659058\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  12%|#033[34m█▏        #033[0m| 33/281 [02:27<18:31,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 33 is completed and loss is 1.0189762115478516\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  12%|#033[34m█▏        #033[0m| 34/281 [02:32<18:27,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 34 is completed and loss is 0.8971266150474548\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  12%|#033[34m█▏        #033[0m| 35/281 [02:36<18:22,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 35 is completed and loss is 1.1086198091506958\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  13%|#033[34m█▎        #033[0m| 36/281 [02:41<18:18,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 36 is completed and loss is 1.0155599117279053\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  13%|#033[34m█▎        #033[0m| 37/281 [02:45<18:14,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 37 is completed and loss is 1.1082099676132202\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  14%|#033[34m█▎        #033[0m| 38/281 [02:50<18:09,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 38 is completed and loss is 0.9103391766548157\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  14%|#033[34m█▍        #033[0m| 39/281 [02:54<18:05,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 39 is completed and loss is 1.0699739456176758\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  14%|#033[34m█▍        #033[0m| 40/281 [02:59<18:00,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 40 is completed and loss is 0.962250828742981\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  15%|#033[34m█▍        #033[0m| 41/281 [03:03<17:56,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 41 is completed and loss is 1.274158000946045\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  15%|#033[34m█▍        #033[0m| 42/281 [03:08<17:51,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 42 is completed and loss is 0.9998809695243835\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  15%|#033[34m█▌        #033[0m| 43/281 [03:12<17:47,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 43 is completed and loss is 1.0866053104400635\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  16%|#033[34m█▌        #033[0m| 44/281 [03:17<17:42,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 44 is completed and loss is 0.954001784324646\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  16%|#033[34m█▌        #033[0m| 45/281 [03:21<17:38,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 45 is completed and loss is 1.176407814025879\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  16%|#033[34m█▋        #033[0m| 46/281 [03:26<17:33,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 46 is completed and loss is 0.8969812989234924\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  17%|#033[34m█▋        #033[0m| 47/281 [03:30<17:29,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 47 is completed and loss is 0.8884082436561584\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  17%|#033[34m█▋        #033[0m| 48/281 [03:35<17:24,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 48 is completed and loss is 0.8545333743095398\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  17%|#033[34m█▋        #033[0m| 49/281 [03:39<17:20,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 49 is completed and loss is 1.0120595693588257\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  18%|#033[34m█▊        #033[0m| 50/281 [03:44<17:15,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 50 is completed and loss is 0.869377076625824\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  18%|#033[34m█▊        #033[0m| 51/281 [03:48<17:11,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 51 is completed and loss is 0.8051566481590271\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  19%|#033[34m█▊        #033[0m| 52/281 [03:53<17:06,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 52 is completed and loss is 1.068189024925232\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  19%|#033[34m█▉        #033[0m| 53/281 [03:57<17:02,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 53 is completed and loss is 1.204667329788208\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  19%|#033[34m█▉        #033[0m| 54/281 [04:02<16:57,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 54 is completed and loss is 1.0398478507995605\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  20%|#033[34m█▉        #033[0m| 55/281 [04:06<16:53,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 55 is completed and loss is 0.927162230014801\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  20%|#033[34m█▉        #033[0m| 56/281 [04:11<16:48,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 56 is completed and loss is 1.1676815748214722\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  20%|#033[34m██        #033[0m| 57/281 [04:15<16:44,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 57 is completed and loss is 1.1553698778152466\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  21%|#033[34m██        #033[0m| 58/281 [04:20<16:39,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 58 is completed and loss is 0.9261888265609741\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  21%|#033[34m██        #033[0m| 59/281 [04:24<16:35,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 59 is completed and loss is 0.92732173204422\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  21%|#033[34m██▏       #033[0m| 60/281 [04:29<16:30,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 60 is completed and loss is 0.9827818274497986\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  22%|#033[34m██▏       #033[0m| 61/281 [04:33<16:26,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 61 is completed and loss is 0.9518024325370789\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  22%|#033[34m██▏       #033[0m| 62/281 [04:37<16:21,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 62 is completed and loss is 0.8920507431030273\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  22%|#033[34m██▏       #033[0m| 63/281 [04:42<16:17,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 63 is completed and loss is 0.9196029305458069\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  23%|#033[34m██▎       #033[0m| 64/281 [04:46<16:12,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 64 is completed and loss is 0.8779022097587585\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  23%|#033[34m██▎       #033[0m| 65/281 [04:51<16:08,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 65 is completed and loss is 1.1001784801483154\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  23%|#033[34m██▎       #033[0m| 66/281 [04:55<16:03,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 66 is completed and loss is 0.9116483926773071\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  24%|#033[34m██▍       #033[0m| 67/281 [05:00<15:59,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 67 is completed and loss is 1.0223239660263062\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  24%|#033[34m██▍       #033[0m| 68/281 [05:04<15:54,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 68 is completed and loss is 0.9325582385063171\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  25%|#033[34m██▍       #033[0m| 69/281 [05:09<15:50,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 69 is completed and loss is 0.8519075512886047\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  25%|#033[34m██▍       #033[0m| 70/281 [05:13<15:45,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 70 is completed and loss is 1.0022978782653809\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  25%|#033[34m██▌       #033[0m| 71/281 [05:18<15:41,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 71 is completed and loss is 0.9167141318321228\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  26%|#033[34m██▌       #033[0m| 72/281 [05:22<15:36,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 72 is completed and loss is 0.978551983833313\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  26%|#033[34m██▌       #033[0m| 73/281 [05:27<15:32,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 73 is completed and loss is 0.9226053357124329\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  26%|#033[34m██▋       #033[0m| 74/281 [05:31<15:27,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 74 is completed and loss is 1.0939300060272217\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  27%|#033[34m██▋       #033[0m| 75/281 [05:36<15:23,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 75 is completed and loss is 0.8152334094047546\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  27%|#033[34m██▋       #033[0m| 76/281 [05:40<15:18,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 76 is completed and loss is 0.9502021670341492\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  27%|#033[34m██▋       #033[0m| 77/281 [05:45<15:14,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 77 is completed and loss is 1.2939897775650024\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  28%|#033[34m██▊       #033[0m| 78/281 [05:49<15:10,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 78 is completed and loss is 1.0086215734481812\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  28%|#033[34m██▊       #033[0m| 79/281 [05:54<15:09,  4.50s/it]\u001b[0m\n",
      "\u001b[34mstep 79 is completed and loss is 0.9764025807380676\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  28%|#033[34m██▊       #033[0m| 80/281 [05:58<15:03,  4.50s/it]\u001b[0m\n",
      "\u001b[34mstep 80 is completed and loss is 1.0732810497283936\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  29%|#033[34m██▉       #033[0m| 81/281 [06:03<14:58,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 81 is completed and loss is 1.0221980810165405\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  29%|#033[34m██▉       #033[0m| 82/281 [06:07<14:53,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 82 is completed and loss is 0.9212702512741089\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  30%|#033[34m██▉       #033[0m| 83/281 [06:12<14:48,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 83 is completed and loss is 0.9426212906837463\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  30%|#033[34m██▉       #033[0m| 84/281 [06:16<14:43,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 84 is completed and loss is 0.9868262410163879\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  30%|#033[34m███       #033[0m| 85/281 [06:21<14:39,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 85 is completed and loss is 1.0261025428771973\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  31%|#033[34m███       #033[0m| 86/281 [06:25<14:34,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 86 is completed and loss is 0.9833653569221497\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  31%|#033[34m███       #033[0m| 87/281 [06:30<14:29,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 87 is completed and loss is 0.9432384371757507\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  31%|#033[34m███▏      #033[0m| 88/281 [06:34<14:25,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 88 is completed and loss is 0.9841488599777222\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  32%|#033[34m███▏      #033[0m| 89/281 [06:39<14:20,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 89 is completed and loss is 0.9727646112442017\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  32%|#033[34m███▏      #033[0m| 90/281 [06:43<14:16,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 90 is completed and loss is 1.0540134906768799\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  32%|#033[34m███▏      #033[0m| 91/281 [06:48<14:11,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 91 is completed and loss is 1.1767709255218506\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  33%|#033[34m███▎      #033[0m| 92/281 [06:52<14:07,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 92 is completed and loss is 0.912426233291626\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  33%|#033[34m███▎      #033[0m| 93/281 [06:57<14:02,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 93 is completed and loss is 0.9169794917106628\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  33%|#033[34m███▎      #033[0m| 94/281 [07:01<13:58,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 94 is completed and loss is 0.9086476564407349\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  34%|#033[34m███▍      #033[0m| 95/281 [07:05<13:53,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 95 is completed and loss is 1.0327351093292236\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  34%|#033[34m███▍      #033[0m| 96/281 [07:10<13:49,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 96 is completed and loss is 0.8581674098968506\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  35%|#033[34m███▍      #033[0m| 97/281 [07:14<13:44,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 97 is completed and loss is 1.1029443740844727\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  35%|#033[34m███▍      #033[0m| 98/281 [07:19<13:40,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 98 is completed and loss is 1.0064823627471924\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  35%|#033[34m███▌      #033[0m| 99/281 [07:23<13:35,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 99 is completed and loss is 0.9515498280525208\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  36%|#033[34m███▌      #033[0m| 100/281 [07:28<13:31,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 100 is completed and loss is 1.1714940071105957\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  36%|#033[34m███▌      #033[0m| 101/281 [07:32<13:26,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 101 is completed and loss is 1.1921260356903076\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  36%|#033[34m███▋      #033[0m| 102/281 [07:37<13:22,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 102 is completed and loss is 1.1161929368972778\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  37%|#033[34m███▋      #033[0m| 103/281 [07:41<13:17,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 103 is completed and loss is 1.0377583503723145\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  37%|#033[34m███▋      #033[0m| 104/281 [07:46<13:13,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 104 is completed and loss is 1.129598617553711\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  37%|#033[34m███▋      #033[0m| 105/281 [07:50<13:08,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 105 is completed and loss is 1.0548237562179565\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  38%|#033[34m███▊      #033[0m| 106/281 [07:55<13:04,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 106 is completed and loss is 1.0902345180511475\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  38%|#033[34m███▊      #033[0m| 107/281 [07:59<13:00,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 107 is completed and loss is 0.9972334504127502\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  38%|#033[34m███▊      #033[0m| 108/281 [08:04<12:55,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 108 is completed and loss is 0.8722742199897766\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  39%|#033[34m███▉      #033[0m| 109/281 [08:08<12:51,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 109 is completed and loss is 1.018198013305664\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  39%|#033[34m███▉      #033[0m| 110/281 [08:13<12:46,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 110 is completed and loss is 0.866584300994873\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  40%|#033[34m███▉      #033[0m| 111/281 [08:17<12:45,  4.50s/it]\u001b[0m\n",
      "\u001b[34mstep 111 is completed and loss is 1.079202651977539\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  40%|#033[34m███▉      #033[0m| 112/281 [08:22<12:39,  4.50s/it]\u001b[0m\n",
      "\u001b[34mstep 112 is completed and loss is 1.0570062398910522\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  40%|#033[34m████      #033[0m| 113/281 [08:26<12:34,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 113 is completed and loss is 1.0228511095046997\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  41%|#033[34m████      #033[0m| 114/281 [08:31<12:29,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 114 is completed and loss is 1.02021324634552\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  41%|#033[34m████      #033[0m| 115/281 [08:35<12:24,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 115 is completed and loss is 0.9198932647705078\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  41%|#033[34m████▏     #033[0m| 116/281 [08:40<12:20,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 116 is completed and loss is 1.1398587226867676\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  42%|#033[34m████▏     #033[0m| 117/281 [08:44<12:15,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 117 is completed and loss is 0.9978905916213989\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  42%|#033[34m████▏     #033[0m| 118/281 [08:49<12:10,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 118 is completed and loss is 1.1120258569717407\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  42%|#033[34m████▏     #033[0m| 119/281 [08:53<12:06,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 119 is completed and loss is 0.9441431760787964\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  43%|#033[34m████▎     #033[0m| 120/281 [08:58<12:01,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 120 is completed and loss is 0.9740415811538696\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  43%|#033[34m████▎     #033[0m| 121/281 [09:02<11:57,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 121 is completed and loss is 1.09563148021698\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  43%|#033[34m████▎     #033[0m| 122/281 [09:07<11:52,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 122 is completed and loss is 1.2870361804962158\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  44%|#033[34m████▍     #033[0m| 123/281 [09:11<11:48,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 123 is completed and loss is 1.0135858058929443\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  44%|#033[34m████▍     #033[0m| 124/281 [09:16<11:43,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 124 is completed and loss is 1.1646654605865479\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  44%|#033[34m████▍     #033[0m| 125/281 [09:20<11:39,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 125 is completed and loss is 0.857097864151001\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  45%|#033[34m████▍     #033[0m| 126/281 [09:24<11:34,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 126 is completed and loss is 1.0378915071487427\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  45%|#033[34m████▌     #033[0m| 127/281 [09:29<11:30,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 127 is completed and loss is 0.9491865038871765\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  46%|#033[34m████▌     #033[0m| 128/281 [09:33<11:25,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 128 is completed and loss is 0.8694424629211426\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  46%|#033[34m████▌     #033[0m| 129/281 [09:38<11:21,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 129 is completed and loss is 1.080568552017212\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  46%|#033[34m████▋     #033[0m| 130/281 [09:42<11:16,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 130 is completed and loss is 1.2447041273117065\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  47%|#033[34m████▋     #033[0m| 131/281 [09:47<11:12,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 131 is completed and loss is 0.9106999635696411\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  47%|#033[34m████▋     #033[0m| 132/281 [09:51<11:07,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 132 is completed and loss is 0.8591544032096863\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  47%|#033[34m████▋     #033[0m| 133/281 [09:56<11:03,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 133 is completed and loss is 0.9196964502334595\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  48%|#033[34m████▊     #033[0m| 134/281 [10:00<10:59,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 134 is completed and loss is 0.9692001342773438\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  48%|#033[34m████▊     #033[0m| 135/281 [10:05<10:54,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 135 is completed and loss is 0.9538477063179016\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  48%|#033[34m████▊     #033[0m| 136/281 [10:09<10:50,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 136 is completed and loss is 1.054624080657959\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  49%|#033[34m████▉     #033[0m| 137/281 [10:14<10:45,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 137 is completed and loss is 0.947942852973938\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  49%|#033[34m████▉     #033[0m| 138/281 [10:18<10:41,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 138 is completed and loss is 0.9594887495040894\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  49%|#033[34m████▉     #033[0m| 139/281 [10:23<10:36,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 139 is completed and loss is 1.2000324726104736\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  50%|#033[34m████▉     #033[0m| 140/281 [10:27<10:32,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 140 is completed and loss is 0.9781398177146912\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  50%|#033[34m█████     #033[0m| 141/281 [10:32<10:27,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 141 is completed and loss is 1.0874178409576416\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  51%|#033[34m█████     #033[0m| 142/281 [10:36<10:23,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 142 is completed and loss is 0.8383613228797913\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  51%|#033[34m█████     #033[0m| 143/281 [10:41<10:21,  4.50s/it]\u001b[0m\n",
      "\u001b[34mstep 143 is completed and loss is 0.8179620504379272\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  51%|#033[34m█████     #033[0m| 144/281 [10:45<10:16,  4.50s/it]\u001b[0m\n",
      "\u001b[34mstep 144 is completed and loss is 1.155720829963684\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  52%|#033[34m█████▏    #033[0m| 145/281 [10:50<10:11,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 145 is completed and loss is 1.0501296520233154\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  52%|#033[34m█████▏    #033[0m| 146/281 [10:54<10:06,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 146 is completed and loss is 1.0153229236602783\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  52%|#033[34m█████▏    #033[0m| 147/281 [10:59<10:01,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 147 is completed and loss is 0.993144690990448\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  53%|#033[34m█████▎    #033[0m| 148/281 [11:03<09:56,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 148 is completed and loss is 0.9042813181877136\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  53%|#033[34m█████▎    #033[0m| 149/281 [11:08<09:52,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 149 is completed and loss is 1.007133960723877\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  53%|#033[34m█████▎    #033[0m| 150/281 [11:12<09:47,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 150 is completed and loss is 0.9181928634643555\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  54%|#033[34m█████▎    #033[0m| 151/281 [11:17<09:42,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 151 is completed and loss is 1.148443341255188\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  54%|#033[34m█████▍    #033[0m| 152/281 [11:21<09:38,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 152 is completed and loss is 1.0705255270004272\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  54%|#033[34m█████▍    #033[0m| 153/281 [11:26<09:33,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 153 is completed and loss is 0.924632728099823\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  55%|#033[34m█████▍    #033[0m| 154/281 [11:30<09:29,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 154 is completed and loss is 0.9310718178749084\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  55%|#033[34m█████▌    #033[0m| 155/281 [11:35<09:24,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 155 is completed and loss is 1.0273574590682983\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  56%|#033[34m█████▌    #033[0m| 156/281 [11:39<09:20,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 156 is completed and loss is 1.0430220365524292\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  56%|#033[34m█████▌    #033[0m| 157/281 [11:44<09:15,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 157 is completed and loss is 1.168759822845459\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  56%|#033[34m█████▌    #033[0m| 158/281 [11:48<09:11,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 158 is completed and loss is 0.8949714303016663\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  57%|#033[34m█████▋    #033[0m| 159/281 [11:53<09:06,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 159 is completed and loss is 0.8659593462944031\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  57%|#033[34m█████▋    #033[0m| 160/281 [11:57<09:02,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 160 is completed and loss is 1.0795491933822632\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  57%|#033[34m█████▋    #033[0m| 161/281 [12:01<08:57,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 161 is completed and loss is 1.054155707359314\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  58%|#033[34m█████▊    #033[0m| 162/281 [12:06<08:53,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 162 is completed and loss is 0.9831218123435974\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  58%|#033[34m█████▊    #033[0m| 163/281 [12:10<08:49,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 163 is completed and loss is 1.0345780849456787\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  58%|#033[34m█████▊    #033[0m| 164/281 [12:15<08:44,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 164 is completed and loss is 0.9548086524009705\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  59%|#033[34m█████▊    #033[0m| 165/281 [12:19<08:40,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 165 is completed and loss is 0.8846982717514038\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  59%|#033[34m█████▉    #033[0m| 166/281 [12:24<08:35,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 166 is completed and loss is 0.8026972413063049\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  59%|#033[34m█████▉    #033[0m| 167/281 [12:28<08:31,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 167 is completed and loss is 0.8562310338020325\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  60%|#033[34m█████▉    #033[0m| 168/281 [12:33<08:26,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 168 is completed and loss is 0.9858824610710144\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  60%|#033[34m██████    #033[0m| 169/281 [12:37<08:22,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 169 is completed and loss is 0.9701663255691528\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  60%|#033[34m██████    #033[0m| 170/281 [12:42<08:17,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 170 is completed and loss is 1.1306431293487549\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  61%|#033[34m██████    #033[0m| 171/281 [12:46<08:13,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 171 is completed and loss is 0.9128885269165039\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  61%|#033[34m██████    #033[0m| 172/281 [12:51<08:08,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 172 is completed and loss is 1.0521039962768555\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  62%|#033[34m██████▏   #033[0m| 173/281 [12:55<08:04,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 173 is completed and loss is 0.9363664984703064\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  62%|#033[34m██████▏   #033[0m| 174/281 [13:00<07:59,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 174 is completed and loss is 0.9055365920066833\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  62%|#033[34m██████▏   #033[0m| 175/281 [13:04<07:57,  4.50s/it]\u001b[0m\n",
      "\u001b[34mstep 175 is completed and loss is 0.8002928495407104\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  63%|#033[34m██████▎   #033[0m| 176/281 [13:09<07:52,  4.50s/it]\u001b[0m\n",
      "\u001b[34mstep 176 is completed and loss is 0.9372162222862244\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  63%|#033[34m██████▎   #033[0m| 177/281 [13:13<07:47,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 177 is completed and loss is 1.0684826374053955\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  63%|#033[34m██████▎   #033[0m| 178/281 [13:18<07:42,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 178 is completed and loss is 1.0291485786437988\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  64%|#033[34m██████▎   #033[0m| 179/281 [13:22<07:37,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 179 is completed and loss is 1.1757656335830688\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  64%|#033[34m██████▍   #033[0m| 180/281 [13:27<07:33,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 180 is completed and loss is 0.905259907245636\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  64%|#033[34m██████▍   #033[0m| 181/281 [13:31<07:28,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 181 is completed and loss is 1.0132228136062622\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  65%|#033[34m██████▍   #033[0m| 182/281 [13:36<07:24,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 182 is completed and loss is 1.0621906518936157\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  65%|#033[34m██████▌   #033[0m| 183/281 [13:40<07:19,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 183 is completed and loss is 1.104339599609375\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  65%|#033[34m██████▌   #033[0m| 184/281 [13:45<07:14,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 184 is completed and loss is 0.8920599818229675\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  66%|#033[34m██████▌   #033[0m| 185/281 [13:49<07:10,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 185 is completed and loss is 0.8926028609275818\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  66%|#033[34m██████▌   #033[0m| 186/281 [13:54<07:05,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 186 is completed and loss is 0.9578214287757874\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  67%|#033[34m██████▋   #033[0m| 187/281 [13:58<07:01,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 187 is completed and loss is 1.0079797506332397\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  67%|#033[34m██████▋   #033[0m| 188/281 [14:03<06:56,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 188 is completed and loss is 1.0847418308258057\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  67%|#033[34m██████▋   #033[0m| 189/281 [14:07<06:52,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 189 is completed and loss is 1.0783956050872803\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  68%|#033[34m██████▊   #033[0m| 190/281 [14:12<06:48,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 190 is completed and loss is 0.9347029328346252\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  68%|#033[34m██████▊   #033[0m| 191/281 [14:16<06:43,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 191 is completed and loss is 0.9972001910209656\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  68%|#033[34m██████▊   #033[0m| 192/281 [14:21<06:39,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 192 is completed and loss is 0.9338690042495728\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  69%|#033[34m██████▊   #033[0m| 193/281 [14:25<06:34,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 193 is completed and loss is 0.9504576921463013\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  69%|#033[34m██████▉   #033[0m| 194/281 [14:29<06:30,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 194 is completed and loss is 0.8984914422035217\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  69%|#033[34m██████▉   #033[0m| 195/281 [14:34<06:25,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 195 is completed and loss is 1.025261640548706\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  70%|#033[34m██████▉   #033[0m| 196/281 [14:38<06:21,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 196 is completed and loss is 1.0307272672653198\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  70%|#033[34m███████   #033[0m| 197/281 [14:43<06:16,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 197 is completed and loss is 0.9321534633636475\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  70%|#033[34m███████   #033[0m| 198/281 [14:47<06:12,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 198 is completed and loss is 1.0384811162948608\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  71%|#033[34m███████   #033[0m| 199/281 [14:52<06:07,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 199 is completed and loss is 0.8896262049674988\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  71%|#033[34m███████   #033[0m| 200/281 [14:56<06:03,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 200 is completed and loss is 0.9937691688537598\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  72%|#033[34m███████▏  #033[0m| 201/281 [15:01<05:58,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 201 is completed and loss is 1.067827820777893\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  72%|#033[34m███████▏  #033[0m| 202/281 [15:05<05:54,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 202 is completed and loss is 0.9973574280738831\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  72%|#033[34m███████▏  #033[0m| 203/281 [15:10<05:49,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 203 is completed and loss is 1.0670307874679565\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  73%|#033[34m███████▎  #033[0m| 204/281 [15:14<05:45,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 204 is completed and loss is 1.0370458364486694\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  73%|#033[34m███████▎  #033[0m| 205/281 [15:19<05:40,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 205 is completed and loss is 1.0860501527786255\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  73%|#033[34m███████▎  #033[0m| 206/281 [15:23<05:36,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 206 is completed and loss is 0.9720091223716736\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  74%|#033[34m███████▎  #033[0m| 207/281 [15:28<05:32,  4.50s/it]\u001b[0m\n",
      "\u001b[34mstep 207 is completed and loss is 1.247718095779419\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  74%|#033[34m███████▍  #033[0m| 208/281 [15:32<05:28,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 208 is completed and loss is 0.9411599040031433\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  74%|#033[34m███████▍  #033[0m| 209/281 [15:37<05:23,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 209 is completed and loss is 0.9320855140686035\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  75%|#033[34m███████▍  #033[0m| 210/281 [15:41<05:18,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 210 is completed and loss is 0.985244870185852\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  75%|#033[34m███████▌  #033[0m| 211/281 [15:46<05:14,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 211 is completed and loss is 0.9761176705360413\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  75%|#033[34m███████▌  #033[0m| 212/281 [15:50<05:09,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 212 is completed and loss is 1.0022403001785278\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  76%|#033[34m███████▌  #033[0m| 213/281 [15:55<05:05,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 213 is completed and loss is 1.23183012008667\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  76%|#033[34m███████▌  #033[0m| 214/281 [15:59<05:00,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 214 is completed and loss is 0.9999520182609558\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  77%|#033[34m███████▋  #033[0m| 215/281 [16:04<04:55,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 215 is completed and loss is 1.0242993831634521\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  77%|#033[34m███████▋  #033[0m| 216/281 [16:08<04:51,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 216 is completed and loss is 1.078249454498291\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  77%|#033[34m███████▋  #033[0m| 217/281 [16:13<04:46,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 217 is completed and loss is 1.084758996963501\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  78%|#033[34m███████▊  #033[0m| 218/281 [16:17<04:42,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 218 is completed and loss is 0.8146008253097534\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  78%|#033[34m███████▊  #033[0m| 219/281 [16:22<04:38,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 219 is completed and loss is 0.8450998067855835\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  78%|#033[34m███████▊  #033[0m| 220/281 [16:26<04:33,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 220 is completed and loss is 0.8695745468139648\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  79%|#033[34m███████▊  #033[0m| 221/281 [16:31<04:29,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 221 is completed and loss is 0.956597626209259\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  79%|#033[34m███████▉  #033[0m| 222/281 [16:35<04:24,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 222 is completed and loss is 0.9883787631988525\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  79%|#033[34m███████▉  #033[0m| 223/281 [16:40<04:20,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 223 is completed and loss is 0.953254759311676\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  80%|#033[34m███████▉  #033[0m| 224/281 [16:44<04:15,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 224 is completed and loss is 0.9647151827812195\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  80%|#033[34m████████  #033[0m| 225/281 [16:49<04:11,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 225 is completed and loss is 0.8943923115730286\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  80%|#033[34m████████  #033[0m| 226/281 [16:53<04:06,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 226 is completed and loss is 1.2153728008270264\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  81%|#033[34m████████  #033[0m| 227/281 [16:58<04:02,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 227 is completed and loss is 0.8064887523651123\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  81%|#033[34m████████  #033[0m| 228/281 [17:02<03:57,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 228 is completed and loss is 1.0462814569473267\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  81%|#033[34m████████▏ #033[0m| 229/281 [17:06<03:53,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 229 is completed and loss is 0.8906630277633667\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  82%|#033[34m████████▏ #033[0m| 230/281 [17:11<03:48,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 230 is completed and loss is 0.970812201499939\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  82%|#033[34m████████▏ #033[0m| 231/281 [17:15<03:44,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 231 is completed and loss is 0.9376675486564636\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  83%|#033[34m████████▎ #033[0m| 232/281 [17:20<03:39,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 232 is completed and loss is 0.9956686496734619\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  83%|#033[34m████████▎ #033[0m| 233/281 [17:24<03:35,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 233 is completed and loss is 1.0448576211929321\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  83%|#033[34m████████▎ #033[0m| 234/281 [17:29<03:30,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 234 is completed and loss is 0.978840708732605\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  84%|#033[34m████████▎ #033[0m| 235/281 [17:33<03:26,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 235 is completed and loss is 1.0184277296066284\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  84%|#033[34m████████▍ #033[0m| 236/281 [17:38<03:21,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 236 is completed and loss is 0.8576019406318665\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  84%|#033[34m████████▍ #033[0m| 237/281 [17:42<03:17,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 237 is completed and loss is 0.9510213136672974\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  85%|#033[34m████████▍ #033[0m| 238/281 [17:47<03:12,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 238 is completed and loss is 0.9933579564094543\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  85%|#033[34m████████▌ #033[0m| 239/281 [17:51<03:09,  4.50s/it]\u001b[0m\n",
      "\u001b[34mstep 239 is completed and loss is 1.0048843622207642\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  85%|#033[34m████████▌ #033[0m| 240/281 [17:56<03:04,  4.50s/it]\u001b[0m\n",
      "\u001b[34mstep 240 is completed and loss is 0.9576883912086487\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  86%|#033[34m████████▌ #033[0m| 241/281 [18:00<02:59,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 241 is completed and loss is 1.1286225318908691\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  86%|#033[34m████████▌ #033[0m| 242/281 [18:05<02:55,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 242 is completed and loss is 0.9919595718383789\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  86%|#033[34m████████▋ #033[0m| 243/281 [18:09<02:50,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 243 is completed and loss is 0.7977665662765503\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  87%|#033[34m████████▋ #033[0m| 244/281 [18:14<02:46,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 244 is completed and loss is 1.0283379554748535\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  87%|#033[34m████████▋ #033[0m| 245/281 [18:18<02:41,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 245 is completed and loss is 0.8919214010238647\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  88%|#033[34m████████▊ #033[0m| 246/281 [18:23<02:36,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 246 is completed and loss is 0.9491085410118103\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  88%|#033[34m████████▊ #033[0m| 247/281 [18:27<02:32,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 247 is completed and loss is 1.063652515411377\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  88%|#033[34m████████▊ #033[0m| 248/281 [18:32<02:27,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 248 is completed and loss is 0.9717950224876404\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  89%|#033[34m████████▊ #033[0m| 249/281 [18:36<02:23,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 249 is completed and loss is 1.1328816413879395\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  89%|#033[34m████████▉ #033[0m| 250/281 [18:41<02:18,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 250 is completed and loss is 1.06717848777771\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  89%|#033[34m████████▉ #033[0m| 251/281 [18:45<02:14,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 251 is completed and loss is 1.0810433626174927\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  90%|#033[34m████████▉ #033[0m| 252/281 [18:50<02:10,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 252 is completed and loss is 0.9438902139663696\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  90%|#033[34m█████████ #033[0m| 253/281 [18:54<02:05,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 253 is completed and loss is 0.9705960750579834\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  90%|#033[34m█████████ #033[0m| 254/281 [18:59<02:01,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 254 is completed and loss is 0.7455533742904663\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  91%|#033[34m█████████ #033[0m| 255/281 [19:03<01:56,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 255 is completed and loss is 0.9291375279426575\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  91%|#033[34m█████████ #033[0m| 256/281 [19:08<01:52,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 256 is completed and loss is 0.9712549448013306\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  91%|#033[34m█████████▏#033[0m| 257/281 [19:12<01:47,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 257 is completed and loss is 1.1367913484573364\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  92%|#033[34m█████████▏#033[0m| 258/281 [19:17<01:43,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 258 is completed and loss is 1.0331071615219116\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  92%|#033[34m█████████▏#033[0m| 259/281 [19:21<01:38,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 259 is completed and loss is 0.9945117831230164\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  93%|#033[34m█████████▎#033[0m| 260/281 [19:26<01:34,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 260 is completed and loss is 1.052228331565857\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  93%|#033[34m█████████▎#033[0m| 261/281 [19:30<01:29,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 261 is completed and loss is 0.9315974116325378\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  93%|#033[34m█████████▎#033[0m| 262/281 [19:34<01:25,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 262 is completed and loss is 0.9890429973602295\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  94%|#033[34m█████████▎#033[0m| 263/281 [19:39<01:20,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 263 is completed and loss is 0.9764540195465088\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  94%|#033[34m█████████▍#033[0m| 264/281 [19:43<01:16,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 264 is completed and loss is 0.9523413181304932\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  94%|#033[34m█████████▍#033[0m| 265/281 [19:48<01:11,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 265 is completed and loss is 0.9075000882148743\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  95%|#033[34m█████████▍#033[0m| 266/281 [19:52<01:07,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 266 is completed and loss is 0.8834580183029175\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  95%|#033[34m█████████▌#033[0m| 267/281 [19:57<01:02,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 267 is completed and loss is 0.8675736784934998\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  95%|#033[34m█████████▌#033[0m| 268/281 [20:01<00:58,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 268 is completed and loss is 1.1331254243850708\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  96%|#033[34m█████████▌#033[0m| 269/281 [20:06<00:53,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 269 is completed and loss is 0.9810454845428467\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  96%|#033[34m█████████▌#033[0m| 270/281 [20:10<00:49,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 270 is completed and loss is 0.8421711921691895\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  96%|#033[34m█████████▋#033[0m| 271/281 [20:15<00:44,  4.50s/it]\u001b[0m\n",
      "\u001b[34mstep 271 is completed and loss is 0.8671494126319885\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  97%|#033[34m█████████▋#033[0m| 272/281 [20:19<00:40,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 272 is completed and loss is 1.0726608037948608\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  97%|#033[34m█████████▋#033[0m| 273/281 [20:24<00:35,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 273 is completed and loss is 0.86275714635849\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  98%|#033[34m█████████▊#033[0m| 274/281 [20:28<00:31,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 274 is completed and loss is 1.0283223390579224\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  98%|#033[34m█████████▊#033[0m| 275/281 [20:33<00:26,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 275 is completed and loss is 0.9678745269775391\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  98%|#033[34m█████████▊#033[0m| 276/281 [20:37<00:22,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 276 is completed and loss is 0.8651174306869507\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  99%|#033[34m█████████▊#033[0m| 277/281 [20:42<00:17,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 277 is completed and loss is 1.014988660812378\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  99%|#033[34m█████████▉#033[0m| 278/281 [20:46<00:13,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 278 is completed and loss is 1.0539348125457764\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  99%|#033[34m█████████▉#033[0m| 279/281 [20:51<00:08,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 279 is completed and loss is 1.0091874599456787\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m█████████▉#033[0m| 280/281 [20:55<00:04,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 280 is completed and loss is 1.0100380182266235\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 281/281 [21:00<00:00,  4.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 281/281 [21:00<00:00,  4.48s/it]\u001b[0m\n",
      "\u001b[34mMax CUDA memory allocated was 17 GB\u001b[0m\n",
      "\u001b[34mMax CUDA memory reserved was 18 GB\u001b[0m\n",
      "\u001b[34mPeak active CUDA memory was 17 GB\u001b[0m\n",
      "\u001b[34mCuda Malloc retires : 63\u001b[0m\n",
      "\u001b[34mCPU Total Peak Memory consumed during the train (max): 2 GB\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/282 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 1/282 [00:00<02:01,  2.32it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   1%|#033[32m          #033[0m| 2/282 [00:00<02:00,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   1%|#033[32m          #033[0m| 3/282 [00:01<01:59,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   1%|#033[32m▏         #033[0m| 4/282 [00:01<01:59,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   2%|#033[32m▏         #033[0m| 5/282 [00:02<01:58,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   2%|#033[32m▏         #033[0m| 6/282 [00:02<01:58,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   2%|#033[32m▏         #033[0m| 7/282 [00:03<01:57,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 8/282 [00:03<01:57,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 9/282 [00:03<01:56,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 10/282 [00:04<01:56,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▍         #033[0m| 11/282 [00:04<01:56,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▍         #033[0m| 12/282 [00:05<01:55,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   5%|#033[32m▍         #033[0m| 13/282 [00:05<01:55,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   5%|#033[32m▍         #033[0m| 14/282 [00:06<01:54,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   5%|#033[32m▌         #033[0m| 15/282 [00:06<01:54,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   6%|#033[32m▌         #033[0m| 16/282 [00:06<01:53,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   6%|#033[32m▌         #033[0m| 17/282 [00:07<01:53,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   6%|#033[32m▋         #033[0m| 18/282 [00:07<01:53,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 19/282 [00:08<01:52,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 20/282 [00:08<01:52,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 21/282 [00:09<01:51,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   8%|#033[32m▊         #033[0m| 22/282 [00:09<01:51,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   8%|#033[32m▊         #033[0m| 23/282 [00:09<01:50,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   9%|#033[32m▊         #033[0m| 24/282 [00:10<01:50,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   9%|#033[32m▉         #033[0m| 25/282 [00:10<01:50,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   9%|#033[32m▉         #033[0m| 26/282 [00:11<01:49,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  10%|#033[32m▉         #033[0m| 27/282 [00:11<01:49,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  10%|#033[32m▉         #033[0m| 28/282 [00:12<01:48,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  10%|#033[32m█         #033[0m| 29/282 [00:12<01:48,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 30/282 [00:12<01:47,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 31/282 [00:13<01:47,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█▏        #033[0m| 32/282 [00:13<01:47,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  12%|#033[32m█▏        #033[0m| 33/282 [00:14<01:46,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  12%|#033[32m█▏        #033[0m| 34/282 [00:14<01:46,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  12%|#033[32m█▏        #033[0m| 35/282 [00:14<01:45,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  13%|#033[32m█▎        #033[0m| 36/282 [00:15<01:45,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  13%|#033[32m█▎        #033[0m| 37/282 [00:15<01:44,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  13%|#033[32m█▎        #033[0m| 38/282 [00:16<01:44,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 39/282 [00:16<01:44,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 40/282 [00:17<01:43,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  15%|#033[32m█▍        #033[0m| 41/282 [00:17<01:43,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  15%|#033[32m█▍        #033[0m| 42/282 [00:17<01:42,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  15%|#033[32m█▌        #033[0m| 43/282 [00:18<01:42,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  16%|#033[32m█▌        #033[0m| 44/282 [00:18<01:41,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  16%|#033[32m█▌        #033[0m| 45/282 [00:19<01:41,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  16%|#033[32m█▋        #033[0m| 46/282 [00:19<01:41,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 47/282 [00:20<01:40,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 48/282 [00:20<01:40,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 49/282 [00:20<01:39,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 50/282 [00:21<01:39,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 51/282 [00:21<01:38,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 52/282 [00:22<01:38,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  19%|#033[32m█▉        #033[0m| 53/282 [00:22<01:38,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  19%|#033[32m█▉        #033[0m| 54/282 [00:23<01:37,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  20%|#033[32m█▉        #033[0m| 55/282 [00:23<01:37,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  20%|#033[32m█▉        #033[0m| 56/282 [00:23<01:36,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  20%|#033[32m██        #033[0m| 57/282 [00:24<01:36,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██        #033[0m| 58/282 [00:24<01:35,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██        #033[0m| 59/282 [00:25<01:35,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 60/282 [00:25<01:35,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  22%|#033[32m██▏       #033[0m| 61/282 [00:26<01:34,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  22%|#033[32m██▏       #033[0m| 62/282 [00:26<01:34,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  22%|#033[32m██▏       #033[0m| 63/282 [00:26<01:33,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  23%|#033[32m██▎       #033[0m| 64/282 [00:27<01:33,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  23%|#033[32m██▎       #033[0m| 65/282 [00:27<01:32,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  23%|#033[32m██▎       #033[0m| 66/282 [00:28<01:32,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  24%|#033[32m██▍       #033[0m| 67/282 [00:28<01:32,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  24%|#033[32m██▍       #033[0m| 68/282 [00:29<01:31,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  24%|#033[32m██▍       #033[0m| 69/282 [00:29<01:31,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m██▍       #033[0m| 70/282 [00:29<01:30,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 71/282 [00:30<01:30,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  26%|#033[32m██▌       #033[0m| 72/282 [00:30<01:29,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  26%|#033[32m██▌       #033[0m| 73/282 [00:31<01:29,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  26%|#033[32m██▌       #033[0m| 74/282 [00:31<01:29,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  27%|#033[32m██▋       #033[0m| 75/282 [00:32<01:28,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  27%|#033[32m██▋       #033[0m| 76/282 [00:32<01:28,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  27%|#033[32m██▋       #033[0m| 77/282 [00:32<01:27,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  28%|#033[32m██▊       #033[0m| 78/282 [00:33<01:27,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  28%|#033[32m██▊       #033[0m| 79/282 [00:33<01:26,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  28%|#033[32m██▊       #033[0m| 80/282 [00:34<01:26,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 81/282 [00:34<01:26,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▉       #033[0m| 82/282 [00:35<01:25,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▉       #033[0m| 83/282 [00:35<01:25,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  30%|#033[32m██▉       #033[0m| 84/282 [00:35<01:24,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  30%|#033[32m███       #033[0m| 85/282 [00:36<01:24,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  30%|#033[32m███       #033[0m| 86/282 [00:36<01:23,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  31%|#033[32m███       #033[0m| 87/282 [00:37<01:23,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  31%|#033[32m███       #033[0m| 88/282 [00:37<01:23,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 89/282 [00:38<01:22,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 90/282 [00:38<01:22,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 91/282 [00:38<01:21,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  33%|#033[32m███▎      #033[0m| 92/282 [00:39<01:21,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  33%|#033[32m███▎      #033[0m| 93/282 [00:39<01:20,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  33%|#033[32m███▎      #033[0m| 94/282 [00:40<01:20,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▎      #033[0m| 95/282 [00:40<01:20,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▍      #033[0m| 96/282 [00:41<01:19,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▍      #033[0m| 97/282 [00:41<01:19,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  35%|#033[32m███▍      #033[0m| 98/282 [00:41<01:18,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  35%|#033[32m███▌      #033[0m| 99/282 [00:42<01:18,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  35%|#033[32m███▌      #033[0m| 100/282 [00:42<01:17,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 101/282 [00:43<01:17,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 102/282 [00:43<01:17,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  37%|#033[32m███▋      #033[0m| 103/282 [00:44<01:16,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  37%|#033[32m███▋      #033[0m| 104/282 [00:44<01:16,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  37%|#033[32m███▋      #033[0m| 105/282 [00:44<01:15,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  38%|#033[32m███▊      #033[0m| 106/282 [00:45<01:15,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  38%|#033[32m███▊      #033[0m| 107/282 [00:45<01:14,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  38%|#033[32m███▊      #033[0m| 108/282 [00:46<01:14,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▊      #033[0m| 109/282 [00:46<01:14,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 110/282 [00:47<01:13,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 111/282 [00:47<01:13,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  40%|#033[32m███▉      #033[0m| 112/282 [00:47<01:12,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  40%|#033[32m████      #033[0m| 113/282 [00:48<01:12,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  40%|#033[32m████      #033[0m| 114/282 [00:48<01:12,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  41%|#033[32m████      #033[0m| 115/282 [00:49<01:11,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  41%|#033[32m████      #033[0m| 116/282 [00:49<01:11,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  41%|#033[32m████▏     #033[0m| 117/282 [00:50<01:10,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  42%|#033[32m████▏     #033[0m| 118/282 [00:50<01:10,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  42%|#033[32m████▏     #033[0m| 119/282 [00:50<01:09,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 120/282 [00:51<01:09,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 121/282 [00:51<01:09,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 122/282 [00:52<01:08,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  44%|#033[32m████▎     #033[0m| 123/282 [00:52<01:08,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  44%|#033[32m████▍     #033[0m| 124/282 [00:53<01:07,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  44%|#033[32m████▍     #033[0m| 125/282 [00:53<01:07,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m████▍     #033[0m| 126/282 [00:53<01:06,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m████▌     #033[0m| 127/282 [00:54<01:06,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m████▌     #033[0m| 128/282 [00:54<01:05,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▌     #033[0m| 129/282 [00:55<01:05,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▌     #033[0m| 130/282 [00:55<01:05,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 131/282 [00:56<01:04,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  47%|#033[32m████▋     #033[0m| 132/282 [00:56<01:04,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  47%|#033[32m████▋     #033[0m| 133/282 [00:56<01:03,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 134/282 [00:57<01:03,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 135/282 [00:57<01:02,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 136/282 [00:58<01:02,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  49%|#033[32m████▊     #033[0m| 137/282 [00:58<01:02,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  49%|#033[32m████▉     #033[0m| 138/282 [00:59<01:01,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  49%|#033[32m████▉     #033[0m| 139/282 [00:59<01:01,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m████▉     #033[0m| 140/282 [00:59<01:00,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 141/282 [01:00<01:00,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 142/282 [01:00<00:59,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  51%|#033[32m█████     #033[0m| 143/282 [01:01<00:59,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  51%|#033[32m█████     #033[0m| 144/282 [01:01<00:59,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  51%|#033[32m█████▏    #033[0m| 145/282 [01:02<00:58,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 146/282 [01:02<00:58,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 147/282 [01:02<00:57,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 148/282 [01:03<00:57,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  53%|#033[32m█████▎    #033[0m| 149/282 [01:03<00:56,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  53%|#033[32m█████▎    #033[0m| 150/282 [01:04<00:56,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 151/282 [01:04<00:56,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▍    #033[0m| 152/282 [01:05<00:55,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▍    #033[0m| 153/282 [01:05<00:55,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m█████▍    #033[0m| 154/282 [01:05<00:54,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m█████▍    #033[0m| 155/282 [01:06<00:54,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m█████▌    #033[0m| 156/282 [01:06<00:53,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  56%|#033[32m█████▌    #033[0m| 157/282 [01:07<00:53,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  56%|#033[32m█████▌    #033[0m| 158/282 [01:07<00:53,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  56%|#033[32m█████▋    #033[0m| 159/282 [01:08<00:52,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 160/282 [01:08<00:52,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 161/282 [01:08<00:51,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 162/282 [01:09<00:51,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  58%|#033[32m█████▊    #033[0m| 163/282 [01:09<00:50,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  58%|#033[32m█████▊    #033[0m| 164/282 [01:10<00:50,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  59%|#033[32m█████▊    #033[0m| 165/282 [01:10<00:50,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  59%|#033[32m█████▉    #033[0m| 166/282 [01:11<00:49,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  59%|#033[32m█████▉    #033[0m| 167/282 [01:11<00:49,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  60%|#033[32m█████▉    #033[0m| 168/282 [01:11<00:48,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  60%|#033[32m█████▉    #033[0m| 169/282 [01:12<00:48,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  60%|#033[32m██████    #033[0m| 170/282 [01:12<00:47,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 171/282 [01:13<00:47,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 172/282 [01:13<00:47,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████▏   #033[0m| 173/282 [01:14<00:46,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  62%|#033[32m██████▏   #033[0m| 174/282 [01:14<00:46,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  62%|#033[32m██████▏   #033[0m| 175/282 [01:14<00:45,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  62%|#033[32m██████▏   #033[0m| 176/282 [01:15<00:45,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  63%|#033[32m██████▎   #033[0m| 177/282 [01:15<00:44,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  63%|#033[32m██████▎   #033[0m| 178/282 [01:16<00:44,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  63%|#033[32m██████▎   #033[0m| 179/282 [01:16<00:44,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 180/282 [01:17<00:43,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 181/282 [01:17<00:43,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  65%|#033[32m██████▍   #033[0m| 182/282 [01:17<00:42,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  65%|#033[32m██████▍   #033[0m| 183/282 [01:18<00:42,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  65%|#033[32m██████▌   #033[0m| 184/282 [01:18<00:41,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▌   #033[0m| 185/282 [01:19<00:41,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▌   #033[0m| 186/282 [01:19<00:41,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▋   #033[0m| 187/282 [01:20<00:40,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  67%|#033[32m██████▋   #033[0m| 188/282 [01:20<00:40,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  67%|#033[32m██████▋   #033[0m| 189/282 [01:20<00:39,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  67%|#033[32m██████▋   #033[0m| 190/282 [01:21<00:39,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 191/282 [01:21<00:39,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 192/282 [01:22<00:38,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 193/282 [01:22<00:38,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  69%|#033[32m██████▉   #033[0m| 194/282 [01:23<00:37,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  69%|#033[32m██████▉   #033[0m| 195/282 [01:23<00:37,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  70%|#033[32m██████▉   #033[0m| 196/282 [01:23<00:36,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  70%|#033[32m██████▉   #033[0m| 197/282 [01:24<00:36,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  70%|#033[32m███████   #033[0m| 198/282 [01:24<00:35,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████   #033[0m| 199/282 [01:25<00:35,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████   #033[0m| 200/282 [01:25<00:35,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 201/282 [01:26<00:34,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  72%|#033[32m███████▏  #033[0m| 202/282 [01:26<00:34,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  72%|#033[32m███████▏  #033[0m| 203/282 [01:26<00:33,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  72%|#033[32m███████▏  #033[0m| 204/282 [01:27<00:33,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  73%|#033[32m███████▎  #033[0m| 205/282 [01:27<00:32,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  73%|#033[32m███████▎  #033[0m| 206/282 [01:28<00:32,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  73%|#033[32m███████▎  #033[0m| 207/282 [01:28<00:32,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  74%|#033[32m███████▍  #033[0m| 208/282 [01:29<00:31,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  74%|#033[32m███████▍  #033[0m| 209/282 [01:29<00:31,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  74%|#033[32m███████▍  #033[0m| 210/282 [01:29<00:30,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  75%|#033[32m███████▍  #033[0m| 211/282 [01:30<00:30,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 212/282 [01:30<00:29,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  76%|#033[32m███████▌  #033[0m| 213/282 [01:31<00:29,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  76%|#033[32m███████▌  #033[0m| 214/282 [01:31<00:29,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  76%|#033[32m███████▌  #033[0m| 215/282 [01:32<00:28,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  77%|#033[32m███████▋  #033[0m| 216/282 [01:32<00:28,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  77%|#033[32m███████▋  #033[0m| 217/282 [01:32<00:27,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  77%|#033[32m███████▋  #033[0m| 218/282 [01:33<00:27,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  78%|#033[32m███████▊  #033[0m| 219/282 [01:33<00:27,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  78%|#033[32m███████▊  #033[0m| 220/282 [01:34<00:26,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  78%|#033[32m███████▊  #033[0m| 221/282 [01:34<00:26,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 222/282 [01:35<00:25,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▉  #033[0m| 223/282 [01:35<00:25,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▉  #033[0m| 224/282 [01:35<00:24,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  80%|#033[32m███████▉  #033[0m| 225/282 [01:36<00:24,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  80%|#033[32m████████  #033[0m| 226/282 [01:36<00:24,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  80%|#033[32m████████  #033[0m| 227/282 [01:37<00:23,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  81%|#033[32m████████  #033[0m| 228/282 [01:37<00:23,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  81%|#033[32m████████  #033[0m| 229/282 [01:38<00:22,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 230/282 [01:38<00:22,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 231/282 [01:38<00:21,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 232/282 [01:39<00:21,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 233/282 [01:39<00:21,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 234/282 [01:40<00:20,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 235/282 [01:40<00:20,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  84%|#033[32m████████▎ #033[0m| 236/282 [01:41<00:19,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  84%|#033[32m████████▍ #033[0m| 237/282 [01:41<00:19,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  84%|#033[32m████████▍ #033[0m| 238/282 [01:41<00:18,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  85%|#033[32m████████▍ #033[0m| 239/282 [01:42<00:18,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  85%|#033[32m████████▌ #033[0m| 240/282 [01:42<00:18,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  85%|#033[32m████████▌ #033[0m| 241/282 [01:43<00:17,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 242/282 [01:43<00:17,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 243/282 [01:44<00:16,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  87%|#033[32m████████▋ #033[0m| 244/282 [01:44<00:16,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  87%|#033[32m████████▋ #033[0m| 245/282 [01:44<00:15,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  87%|#033[32m████████▋ #033[0m| 246/282 [01:45<00:15,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  88%|#033[32m████████▊ #033[0m| 247/282 [01:45<00:15,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  88%|#033[32m████████▊ #033[0m| 248/282 [01:46<00:14,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  88%|#033[32m████████▊ #033[0m| 249/282 [01:46<00:14,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▊ #033[0m| 250/282 [01:47<00:13,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 251/282 [01:47<00:13,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 252/282 [01:47<00:12,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  90%|#033[32m████████▉ #033[0m| 253/282 [01:48<00:12,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  90%|#033[32m█████████ #033[0m| 254/282 [01:48<00:11,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  90%|#033[32m█████████ #033[0m| 255/282 [01:49<00:11,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  91%|#033[32m█████████ #033[0m| 256/282 [01:49<00:11,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  91%|#033[32m█████████ #033[0m| 257/282 [01:50<00:10,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  91%|#033[32m█████████▏#033[0m| 258/282 [01:50<00:10,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  92%|#033[32m█████████▏#033[0m| 259/282 [01:50<00:09,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  92%|#033[32m█████████▏#033[0m| 260/282 [01:51<00:09,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 261/282 [01:51<00:08,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 262/282 [01:52<00:08,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 263/282 [01:52<00:08,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  94%|#033[32m█████████▎#033[0m| 264/282 [01:53<00:07,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  94%|#033[32m█████████▍#033[0m| 265/282 [01:53<00:07,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  94%|#033[32m█████████▍#033[0m| 266/282 [01:53<00:06,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  95%|#033[32m█████████▍#033[0m| 267/282 [01:54<00:06,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  95%|#033[32m█████████▌#033[0m| 268/282 [01:54<00:05,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  95%|#033[32m█████████▌#033[0m| 269/282 [01:55<00:05,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▌#033[0m| 270/282 [01:55<00:05,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▌#033[0m| 271/282 [01:56<00:04,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 272/282 [01:56<00:04,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  97%|#033[32m█████████▋#033[0m| 273/282 [01:56<00:03,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  97%|#033[32m█████████▋#033[0m| 274/282 [01:57<00:03,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  98%|#033[32m█████████▊#033[0m| 275/282 [01:57<00:02,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  98%|#033[32m█████████▊#033[0m| 276/282 [01:58<00:02,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  98%|#033[32m█████████▊#033[0m| 277/282 [01:58<00:02,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  99%|#033[32m█████████▊#033[0m| 278/282 [01:59<00:01,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  99%|#033[32m█████████▉#033[0m| 279/282 [01:59<00:01,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  99%|#033[32m█████████▉#033[0m| 280/282 [01:59<00:00,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m█████████▉#033[0m| 281/282 [02:00<00:00,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 282/282 [02:00<00:00,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 282/282 [02:00<00:00,  2.33it/s]\u001b[0m\n",
      "\u001b[34meval_ppl=tensor(2.8273, device='cuda:0') eval_epoch_loss=tensor(1.0393, device='cuda:0')\u001b[0m\n",
      "\u001b[34mwe are about to save the PEFT modules\u001b[0m\n",
      "\u001b[34mPEFT modules are saved in saved_peft_model directory\u001b[0m\n",
      "\u001b[34mbest eval loss on epoch 1 is 1.0393345355987549\u001b[0m\n",
      "\u001b[34mEpoch 2: train_perplexity=2.7107, train_epoch_loss=0.9972, epcoh time 1260.6809641370003s\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   0%|#033[34m          #033[0m| 0/281 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mstep 0 is completed and loss is 0.990621030330658\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   0%|#033[34m          #033[0m| 1/281 [00:04<20:57,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 1 is completed and loss is 0.93431156873703\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   1%|#033[34m          #033[0m| 2/281 [00:08<20:51,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 2 is completed and loss is 0.9420557618141174\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   1%|#033[34m          #033[0m| 3/281 [00:13<20:46,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 3 is completed and loss is 0.9302260279655457\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   1%|#033[34m▏         #033[0m| 4/281 [00:17<20:42,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 4 is completed and loss is 0.9387094378471375\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   2%|#033[34m▏         #033[0m| 5/281 [00:22<20:37,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 5 is completed and loss is 1.1150567531585693\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   2%|#033[34m▏         #033[0m| 6/281 [00:26<20:33,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 6 is completed and loss is 0.9718299508094788\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   2%|#033[34m▏         #033[0m| 7/281 [00:31<20:28,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 7 is completed and loss is 0.9600430130958557\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   3%|#033[34m▎         #033[0m| 8/281 [00:35<20:24,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 8 is completed and loss is 0.9520307183265686\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   3%|#033[34m▎         #033[0m| 9/281 [00:40<20:19,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 9 is completed and loss is 0.874123752117157\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   4%|#033[34m▎         #033[0m| 10/281 [00:44<20:15,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 10 is completed and loss is 1.193969964981079\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   4%|#033[34m▍         #033[0m| 11/281 [00:49<20:10,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 11 is completed and loss is 1.1623111963272095\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   4%|#033[34m▍         #033[0m| 12/281 [00:53<20:06,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 12 is completed and loss is 0.9735732674598694\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   5%|#033[34m▍         #033[0m| 13/281 [00:58<20:01,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 13 is completed and loss is 1.0894509553909302\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   5%|#033[34m▍         #033[0m| 14/281 [01:02<19:57,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 14 is completed and loss is 0.8782668113708496\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   5%|#033[34m▌         #033[0m| 15/281 [01:07<19:52,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 15 is completed and loss is 0.9220950603485107\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   6%|#033[34m▌         #033[0m| 16/281 [01:11<19:48,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 16 is completed and loss is 0.8385391235351562\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   6%|#033[34m▌         #033[0m| 17/281 [01:16<19:43,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 17 is completed and loss is 1.0690139532089233\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   6%|#033[34m▋         #033[0m| 18/281 [01:20<19:39,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 18 is completed and loss is 0.9504897594451904\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   7%|#033[34m▋         #033[0m| 19/281 [01:25<19:34,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 19 is completed and loss is 0.991921067237854\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   7%|#033[34m▋         #033[0m| 20/281 [01:29<19:30,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 20 is completed and loss is 1.0892609357833862\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   7%|#033[34m▋         #033[0m| 21/281 [01:34<19:25,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 21 is completed and loss is 0.949864387512207\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   8%|#033[34m▊         #033[0m| 22/281 [01:38<19:21,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 22 is completed and loss is 0.9283905625343323\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   8%|#033[34m▊         #033[0m| 23/281 [01:43<19:16,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 23 is completed and loss is 0.986404299736023\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   9%|#033[34m▊         #033[0m| 24/281 [01:47<19:12,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 24 is completed and loss is 0.9989144802093506\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   9%|#033[34m▉         #033[0m| 25/281 [01:52<19:07,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 25 is completed and loss is 0.89551842212677\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   9%|#033[34m▉         #033[0m| 26/281 [01:56<19:03,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 26 is completed and loss is 1.0808624029159546\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  10%|#033[34m▉         #033[0m| 27/281 [02:01<18:58,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 27 is completed and loss is 0.7746214270591736\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  10%|#033[34m▉         #033[0m| 28/281 [02:05<18:54,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 28 is completed and loss is 1.021412968635559\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  10%|#033[34m█         #033[0m| 29/281 [02:10<18:49,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 29 is completed and loss is 1.0165568590164185\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  11%|#033[34m█         #033[0m| 30/281 [02:14<18:45,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 30 is completed and loss is 1.0067481994628906\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  11%|#033[34m█         #033[0m| 31/281 [02:18<18:40,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 31 is completed and loss is 0.8755674958229065\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  11%|#033[34m█▏        #033[0m| 32/281 [02:23<18:36,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 32 is completed and loss is 1.052660346031189\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  12%|#033[34m█▏        #033[0m| 33/281 [02:27<18:31,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 33 is completed and loss is 0.969896674156189\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  12%|#033[34m█▏        #033[0m| 34/281 [02:32<18:27,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 34 is completed and loss is 0.8582596778869629\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  12%|#033[34m█▏        #033[0m| 35/281 [02:36<18:22,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 35 is completed and loss is 1.0730477571487427\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  13%|#033[34m█▎        #033[0m| 36/281 [02:41<18:18,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 36 is completed and loss is 0.9652888774871826\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  13%|#033[34m█▎        #033[0m| 37/281 [02:45<18:13,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 37 is completed and loss is 1.085161805152893\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  14%|#033[34m█▎        #033[0m| 38/281 [02:50<18:09,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 38 is completed and loss is 0.8834347128868103\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  14%|#033[34m█▍        #033[0m| 39/281 [02:54<18:04,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 39 is completed and loss is 1.0385640859603882\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  14%|#033[34m█▍        #033[0m| 40/281 [02:59<18:00,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 40 is completed and loss is 0.9172560572624207\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  15%|#033[34m█▍        #033[0m| 41/281 [03:03<17:56,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 41 is completed and loss is 1.2272144556045532\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  15%|#033[34m█▍        #033[0m| 42/281 [03:08<17:51,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 42 is completed and loss is 0.9450424909591675\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  15%|#033[34m█▌        #033[0m| 43/281 [03:12<17:47,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 43 is completed and loss is 1.0560249090194702\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  16%|#033[34m█▌        #033[0m| 44/281 [03:17<17:42,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 44 is completed and loss is 0.9269382357597351\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  16%|#033[34m█▌        #033[0m| 45/281 [03:21<17:38,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 45 is completed and loss is 1.116669774055481\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  16%|#033[34m█▋        #033[0m| 46/281 [03:26<17:33,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 46 is completed and loss is 0.8504477143287659\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  17%|#033[34m█▋        #033[0m| 47/281 [03:30<17:29,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 47 is completed and loss is 0.8405377268791199\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  17%|#033[34m█▋        #033[0m| 48/281 [03:35<17:24,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 48 is completed and loss is 0.8048684000968933\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  17%|#033[34m█▋        #033[0m| 49/281 [03:39<17:20,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 49 is completed and loss is 0.9795364737510681\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  18%|#033[34m█▊        #033[0m| 50/281 [03:44<17:15,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 50 is completed and loss is 0.8247449398040771\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  18%|#033[34m█▊        #033[0m| 51/281 [03:48<17:11,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 51 is completed and loss is 0.7781382203102112\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  19%|#033[34m█▊        #033[0m| 52/281 [03:53<17:06,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 52 is completed and loss is 1.0348137617111206\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  19%|#033[34m█▉        #033[0m| 53/281 [03:57<17:02,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 53 is completed and loss is 1.156754970550537\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  19%|#033[34m█▉        #033[0m| 54/281 [04:02<16:57,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 54 is completed and loss is 1.0068491697311401\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  20%|#033[34m█▉        #033[0m| 55/281 [04:06<16:53,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 55 is completed and loss is 0.8839210271835327\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  20%|#033[34m█▉        #033[0m| 56/281 [04:11<16:48,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 56 is completed and loss is 1.1333119869232178\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  20%|#033[34m██        #033[0m| 57/281 [04:15<16:44,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 57 is completed and loss is 1.1124521493911743\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  21%|#033[34m██        #033[0m| 58/281 [04:20<16:39,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 58 is completed and loss is 0.8950140476226807\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  21%|#033[34m██        #033[0m| 59/281 [04:24<16:35,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 59 is completed and loss is 0.8947713375091553\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  21%|#033[34m██▏       #033[0m| 60/281 [04:29<16:30,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 60 is completed and loss is 0.9516912698745728\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  22%|#033[34m██▏       #033[0m| 61/281 [04:33<16:26,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 61 is completed and loss is 0.9164994955062866\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  22%|#033[34m██▏       #033[0m| 62/281 [04:37<16:21,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 62 is completed and loss is 0.8550195693969727\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  22%|#033[34m██▏       #033[0m| 63/281 [04:42<16:17,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 63 is completed and loss is 0.8907170295715332\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  23%|#033[34m██▎       #033[0m| 64/281 [04:46<16:12,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 64 is completed and loss is 0.8345930576324463\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  23%|#033[34m██▎       #033[0m| 65/281 [04:51<16:08,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 65 is completed and loss is 1.068724274635315\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  23%|#033[34m██▎       #033[0m| 66/281 [04:55<16:03,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 66 is completed and loss is 0.8649551272392273\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  24%|#033[34m██▍       #033[0m| 67/281 [05:00<15:59,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 67 is completed and loss is 0.9865718483924866\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  24%|#033[34m██▍       #033[0m| 68/281 [05:04<15:55,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 68 is completed and loss is 0.9010983109474182\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  25%|#033[34m██▍       #033[0m| 69/281 [05:09<15:50,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 69 is completed and loss is 0.8092436194419861\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  25%|#033[34m██▍       #033[0m| 70/281 [05:13<15:46,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 70 is completed and loss is 0.9642339944839478\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  25%|#033[34m██▌       #033[0m| 71/281 [05:18<15:41,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 71 is completed and loss is 0.8758366107940674\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  26%|#033[34m██▌       #033[0m| 72/281 [05:22<15:37,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 72 is completed and loss is 0.9455490112304688\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  26%|#033[34m██▌       #033[0m| 73/281 [05:27<15:32,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 73 is completed and loss is 0.8771251440048218\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  26%|#033[34m██▋       #033[0m| 74/281 [05:31<15:28,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 74 is completed and loss is 1.0444865226745605\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  27%|#033[34m██▋       #033[0m| 75/281 [05:36<15:23,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 75 is completed and loss is 0.776265561580658\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  27%|#033[34m██▋       #033[0m| 76/281 [05:40<15:19,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 76 is completed and loss is 0.9198757410049438\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  27%|#033[34m██▋       #033[0m| 77/281 [05:45<15:14,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 77 is completed and loss is 1.252878189086914\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  28%|#033[34m██▊       #033[0m| 78/281 [05:49<15:10,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 78 is completed and loss is 0.9757792949676514\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  28%|#033[34m██▊       #033[0m| 79/281 [05:54<15:08,  4.50s/it]\u001b[0m\n",
      "\u001b[34mstep 79 is completed and loss is 0.9516581296920776\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  28%|#033[34m██▊       #033[0m| 80/281 [05:58<15:03,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 80 is completed and loss is 1.0445245504379272\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  29%|#033[34m██▉       #033[0m| 81/281 [06:03<14:58,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 81 is completed and loss is 0.9914422035217285\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  29%|#033[34m██▉       #033[0m| 82/281 [06:07<14:53,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 82 is completed and loss is 0.8924276232719421\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  30%|#033[34m██▉       #033[0m| 83/281 [06:12<14:48,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 83 is completed and loss is 0.9013133645057678\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  30%|#033[34m██▉       #033[0m| 84/281 [06:16<14:43,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 84 is completed and loss is 0.9369744062423706\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  30%|#033[34m███       #033[0m| 85/281 [06:21<14:39,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 85 is completed and loss is 0.9874168038368225\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  31%|#033[34m███       #033[0m| 86/281 [06:25<14:34,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 86 is completed and loss is 0.95121169090271\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  31%|#033[34m███       #033[0m| 87/281 [06:30<14:30,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 87 is completed and loss is 0.9045811891555786\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  31%|#033[34m███▏      #033[0m| 88/281 [06:34<14:25,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 88 is completed and loss is 0.93827885389328\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  32%|#033[34m███▏      #033[0m| 89/281 [06:39<14:20,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 89 is completed and loss is 0.9279943108558655\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  32%|#033[34m███▏      #033[0m| 90/281 [06:43<14:16,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 90 is completed and loss is 1.0155028104782104\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  32%|#033[34m███▏      #033[0m| 91/281 [06:48<14:11,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 91 is completed and loss is 1.129265308380127\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  33%|#033[34m███▎      #033[0m| 92/281 [06:52<14:07,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 92 is completed and loss is 0.8649253845214844\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  33%|#033[34m███▎      #033[0m| 93/281 [06:57<14:02,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 93 is completed and loss is 0.8878562450408936\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  33%|#033[34m███▎      #033[0m| 94/281 [07:01<13:58,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 94 is completed and loss is 0.8779157400131226\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  34%|#033[34m███▍      #033[0m| 95/281 [07:05<13:54,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 95 is completed and loss is 0.9910992383956909\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  34%|#033[34m███▍      #033[0m| 96/281 [07:10<13:49,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 96 is completed and loss is 0.8131535053253174\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  35%|#033[34m███▍      #033[0m| 97/281 [07:14<13:45,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 97 is completed and loss is 1.0547072887420654\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  35%|#033[34m███▍      #033[0m| 98/281 [07:19<13:40,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 98 is completed and loss is 0.9590378403663635\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  35%|#033[34m███▌      #033[0m| 99/281 [07:23<13:36,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 99 is completed and loss is 0.9114028811454773\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  36%|#033[34m███▌      #033[0m| 100/281 [07:28<13:31,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 100 is completed and loss is 1.1327581405639648\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  36%|#033[34m███▌      #033[0m| 101/281 [07:32<13:27,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 101 is completed and loss is 1.1636440753936768\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  36%|#033[34m███▋      #033[0m| 102/281 [07:37<13:22,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 102 is completed and loss is 1.0811443328857422\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  37%|#033[34m███▋      #033[0m| 103/281 [07:41<13:18,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 103 is completed and loss is 1.0095924139022827\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  37%|#033[34m███▋      #033[0m| 104/281 [07:46<13:13,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 104 is completed and loss is 1.0961692333221436\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  37%|#033[34m███▋      #033[0m| 105/281 [07:50<13:09,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 105 is completed and loss is 1.0167378187179565\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  38%|#033[34m███▊      #033[0m| 106/281 [07:55<13:04,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 106 is completed and loss is 1.0188227891921997\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  38%|#033[34m███▊      #033[0m| 107/281 [07:59<13:00,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 107 is completed and loss is 0.955913782119751\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  38%|#033[34m███▊      #033[0m| 108/281 [08:04<12:55,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 108 is completed and loss is 0.8333646059036255\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  39%|#033[34m███▉      #033[0m| 109/281 [08:08<12:51,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 109 is completed and loss is 0.9873500466346741\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  39%|#033[34m███▉      #033[0m| 110/281 [08:13<12:46,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 110 is completed and loss is 0.8385673761367798\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  40%|#033[34m███▉      #033[0m| 111/281 [08:17<12:45,  4.50s/it]\u001b[0m\n",
      "\u001b[34mstep 111 is completed and loss is 1.0437415838241577\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  40%|#033[34m███▉      #033[0m| 112/281 [08:22<12:39,  4.50s/it]\u001b[0m\n",
      "\u001b[34mstep 112 is completed and loss is 1.0229905843734741\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  40%|#033[34m████      #033[0m| 113/281 [08:26<12:34,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 113 is completed and loss is 0.9981909990310669\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  41%|#033[34m████      #033[0m| 114/281 [08:31<12:29,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 114 is completed and loss is 0.980338454246521\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  41%|#033[34m████      #033[0m| 115/281 [08:35<12:24,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 115 is completed and loss is 0.8872995376586914\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  41%|#033[34m████▏     #033[0m| 116/281 [08:40<12:20,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 116 is completed and loss is 1.0918983221054077\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  42%|#033[34m████▏     #033[0m| 117/281 [08:44<12:15,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 117 is completed and loss is 0.9523791670799255\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  42%|#033[34m████▏     #033[0m| 118/281 [08:49<12:11,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 118 is completed and loss is 1.0760904550552368\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  42%|#033[34m████▏     #033[0m| 119/281 [08:53<12:06,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 119 is completed and loss is 0.8999629020690918\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  43%|#033[34m████▎     #033[0m| 120/281 [08:58<12:02,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 120 is completed and loss is 0.9408863186836243\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  43%|#033[34m████▎     #033[0m| 121/281 [09:02<11:57,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 121 is completed and loss is 1.0486725568771362\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  43%|#033[34m████▎     #033[0m| 122/281 [09:07<11:53,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 122 is completed and loss is 1.2522544860839844\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  44%|#033[34m████▍     #033[0m| 123/281 [09:11<11:48,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 123 is completed and loss is 0.9730764627456665\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  44%|#033[34m████▍     #033[0m| 124/281 [09:16<11:43,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 124 is completed and loss is 1.1267591714859009\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  44%|#033[34m████▍     #033[0m| 125/281 [09:20<11:39,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 125 is completed and loss is 0.8242420554161072\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  45%|#033[34m████▍     #033[0m| 126/281 [09:25<11:34,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 126 is completed and loss is 0.9921844601631165\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  45%|#033[34m████▌     #033[0m| 127/281 [09:29<11:30,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 127 is completed and loss is 0.9196299910545349\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  46%|#033[34m████▌     #033[0m| 128/281 [09:34<11:26,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 128 is completed and loss is 0.8404499888420105\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  46%|#033[34m████▌     #033[0m| 129/281 [09:38<11:21,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 129 is completed and loss is 1.058358073234558\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  46%|#033[34m████▋     #033[0m| 130/281 [09:42<11:17,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 130 is completed and loss is 1.211737036705017\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  47%|#033[34m████▋     #033[0m| 131/281 [09:47<11:12,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 131 is completed and loss is 0.8836365938186646\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  47%|#033[34m████▋     #033[0m| 132/281 [09:51<11:08,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 132 is completed and loss is 0.8154578804969788\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  47%|#033[34m████▋     #033[0m| 133/281 [09:56<11:03,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 133 is completed and loss is 0.8774048089981079\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  48%|#033[34m████▊     #033[0m| 134/281 [10:00<10:59,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 134 is completed and loss is 0.9354665279388428\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  48%|#033[34m████▊     #033[0m| 135/281 [10:05<10:54,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 135 is completed and loss is 0.9050896167755127\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  48%|#033[34m████▊     #033[0m| 136/281 [10:09<10:50,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 136 is completed and loss is 1.0194458961486816\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  49%|#033[34m████▉     #033[0m| 137/281 [10:14<10:45,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 137 is completed and loss is 0.9095972776412964\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  49%|#033[34m████▉     #033[0m| 138/281 [10:18<10:41,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 138 is completed and loss is 0.9075374603271484\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  49%|#033[34m████▉     #033[0m| 139/281 [10:23<10:36,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 139 is completed and loss is 1.1535438299179077\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  50%|#033[34m████▉     #033[0m| 140/281 [10:27<10:32,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 140 is completed and loss is 0.9418783187866211\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  50%|#033[34m█████     #033[0m| 141/281 [10:32<10:27,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 141 is completed and loss is 1.0506107807159424\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  51%|#033[34m█████     #033[0m| 142/281 [10:36<10:23,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 142 is completed and loss is 0.8038479685783386\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  51%|#033[34m█████     #033[0m| 143/281 [10:41<10:21,  4.50s/it]\u001b[0m\n",
      "\u001b[34mstep 143 is completed and loss is 0.7893695831298828\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  51%|#033[34m█████     #033[0m| 144/281 [10:45<10:15,  4.50s/it]\u001b[0m\n",
      "\u001b[34mstep 144 is completed and loss is 1.1238232851028442\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  52%|#033[34m█████▏    #033[0m| 145/281 [10:50<10:10,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 145 is completed and loss is 1.0186833143234253\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  52%|#033[34m█████▏    #033[0m| 146/281 [10:54<10:06,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 146 is completed and loss is 0.9682856798171997\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  52%|#033[34m█████▏    #033[0m| 147/281 [10:59<10:01,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 147 is completed and loss is 0.9538542628288269\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  53%|#033[34m█████▎    #033[0m| 148/281 [11:03<09:56,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 148 is completed and loss is 0.8639491200447083\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  53%|#033[34m█████▎    #033[0m| 149/281 [11:08<09:52,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 149 is completed and loss is 0.9746078252792358\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  53%|#033[34m█████▎    #033[0m| 150/281 [11:12<09:47,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 150 is completed and loss is 0.8755863904953003\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  54%|#033[34m█████▎    #033[0m| 151/281 [11:17<09:42,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 151 is completed and loss is 1.1196424961090088\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  54%|#033[34m█████▍    #033[0m| 152/281 [11:21<09:38,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 152 is completed and loss is 1.0406664609909058\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  54%|#033[34m█████▍    #033[0m| 153/281 [11:26<09:33,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 153 is completed and loss is 0.8811929225921631\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  55%|#033[34m█████▍    #033[0m| 154/281 [11:30<09:29,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 154 is completed and loss is 0.8825620412826538\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  55%|#033[34m█████▌    #033[0m| 155/281 [11:35<09:25,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 155 is completed and loss is 0.9999287724494934\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  56%|#033[34m█████▌    #033[0m| 156/281 [11:39<09:20,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 156 is completed and loss is 1.0186564922332764\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  56%|#033[34m█████▌    #033[0m| 157/281 [11:44<09:16,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 157 is completed and loss is 1.1320079565048218\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  56%|#033[34m█████▌    #033[0m| 158/281 [11:48<09:11,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 158 is completed and loss is 0.8661900758743286\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  57%|#033[34m█████▋    #033[0m| 159/281 [11:53<09:07,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 159 is completed and loss is 0.8343438506126404\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  57%|#033[34m█████▋    #033[0m| 160/281 [11:57<09:02,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 160 is completed and loss is 1.039709210395813\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  57%|#033[34m█████▋    #033[0m| 161/281 [12:02<08:58,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 161 is completed and loss is 1.0206693410873413\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  58%|#033[34m█████▊    #033[0m| 162/281 [12:06<08:53,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 162 is completed and loss is 0.9561384320259094\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  58%|#033[34m█████▊    #033[0m| 163/281 [12:10<08:49,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 163 is completed and loss is 0.981280505657196\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  58%|#033[34m█████▊    #033[0m| 164/281 [12:15<08:44,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 164 is completed and loss is 0.9115688800811768\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  59%|#033[34m█████▊    #033[0m| 165/281 [12:19<08:40,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 165 is completed and loss is 0.8539519309997559\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  59%|#033[34m█████▉    #033[0m| 166/281 [12:24<08:35,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 166 is completed and loss is 0.7741315960884094\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  59%|#033[34m█████▉    #033[0m| 167/281 [12:28<08:31,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 167 is completed and loss is 0.8140578269958496\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  60%|#033[34m█████▉    #033[0m| 168/281 [12:33<08:26,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 168 is completed and loss is 0.9628696441650391\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  60%|#033[34m██████    #033[0m| 169/281 [12:37<08:22,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 169 is completed and loss is 0.9306662678718567\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  60%|#033[34m██████    #033[0m| 170/281 [12:42<08:17,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 170 is completed and loss is 1.1005048751831055\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  61%|#033[34m██████    #033[0m| 171/281 [12:46<08:13,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 171 is completed and loss is 0.8789293169975281\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  61%|#033[34m██████    #033[0m| 172/281 [12:51<08:08,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 172 is completed and loss is 1.016032099723816\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  62%|#033[34m██████▏   #033[0m| 173/281 [12:55<08:04,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 173 is completed and loss is 0.9031594395637512\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  62%|#033[34m██████▏   #033[0m| 174/281 [13:00<07:59,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 174 is completed and loss is 0.8560661673545837\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  62%|#033[34m██████▏   #033[0m| 175/281 [13:04<07:57,  4.50s/it]\u001b[0m\n",
      "\u001b[34mstep 175 is completed and loss is 0.7721677422523499\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  63%|#033[34m██████▎   #033[0m| 176/281 [13:09<07:52,  4.50s/it]\u001b[0m\n",
      "\u001b[34mstep 176 is completed and loss is 0.9037381410598755\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  63%|#033[34m██████▎   #033[0m| 177/281 [13:13<07:47,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 177 is completed and loss is 1.0230979919433594\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  63%|#033[34m██████▎   #033[0m| 178/281 [13:18<07:42,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 178 is completed and loss is 0.9981897473335266\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  64%|#033[34m██████▎   #033[0m| 179/281 [13:22<07:37,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 179 is completed and loss is 1.144461989402771\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  64%|#033[34m██████▍   #033[0m| 180/281 [13:27<07:33,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 180 is completed and loss is 0.8606652021408081\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  64%|#033[34m██████▍   #033[0m| 181/281 [13:31<07:28,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 181 is completed and loss is 0.96758633852005\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  65%|#033[34m██████▍   #033[0m| 182/281 [13:36<07:24,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 182 is completed and loss is 1.0284833908081055\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  65%|#033[34m██████▌   #033[0m| 183/281 [13:40<07:19,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 183 is completed and loss is 1.0663650035858154\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  65%|#033[34m██████▌   #033[0m| 184/281 [13:45<07:14,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 184 is completed and loss is 0.8590033054351807\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  66%|#033[34m██████▌   #033[0m| 185/281 [13:49<07:10,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 185 is completed and loss is 0.8616241812705994\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  66%|#033[34m██████▌   #033[0m| 186/281 [13:54<07:05,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 186 is completed and loss is 0.9288854002952576\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  67%|#033[34m██████▋   #033[0m| 187/281 [13:58<07:01,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 187 is completed and loss is 0.9729199409484863\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  67%|#033[34m██████▋   #033[0m| 188/281 [14:03<06:57,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 188 is completed and loss is 1.0498371124267578\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  67%|#033[34m██████▋   #033[0m| 189/281 [14:07<06:52,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 189 is completed and loss is 1.0431480407714844\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  68%|#033[34m██████▊   #033[0m| 190/281 [14:12<06:48,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 190 is completed and loss is 0.9044269919395447\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  68%|#033[34m██████▊   #033[0m| 191/281 [14:16<06:43,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 191 is completed and loss is 0.9699183106422424\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  68%|#033[34m██████▊   #033[0m| 192/281 [14:21<06:39,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 192 is completed and loss is 0.9053860306739807\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  69%|#033[34m██████▊   #033[0m| 193/281 [14:25<06:34,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 193 is completed and loss is 0.9100461602210999\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  69%|#033[34m██████▉   #033[0m| 194/281 [14:30<06:30,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 194 is completed and loss is 0.8687109351158142\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  69%|#033[34m██████▉   #033[0m| 195/281 [14:34<06:25,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 195 is completed and loss is 0.9949369430541992\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  70%|#033[34m██████▉   #033[0m| 196/281 [14:39<06:21,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 196 is completed and loss is 0.9997686147689819\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  70%|#033[34m███████   #033[0m| 197/281 [14:43<06:16,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 197 is completed and loss is 0.8953906893730164\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  70%|#033[34m███████   #033[0m| 198/281 [14:47<06:12,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 198 is completed and loss is 0.9895137548446655\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  71%|#033[34m███████   #033[0m| 199/281 [14:52<06:07,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 199 is completed and loss is 0.8446489572525024\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  71%|#033[34m███████   #033[0m| 200/281 [14:56<06:03,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 200 is completed and loss is 0.941422700881958\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  72%|#033[34m███████▏  #033[0m| 201/281 [15:01<05:58,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 201 is completed and loss is 1.022476315498352\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  72%|#033[34m███████▏  #033[0m| 202/281 [15:05<05:54,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 202 is completed and loss is 0.9648507833480835\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  72%|#033[34m███████▏  #033[0m| 203/281 [15:10<05:49,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 203 is completed and loss is 1.0166325569152832\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  73%|#033[34m███████▎  #033[0m| 204/281 [15:14<05:45,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 204 is completed and loss is 0.9878575205802917\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  73%|#033[34m███████▎  #033[0m| 205/281 [15:19<05:40,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 205 is completed and loss is 1.0525773763656616\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  73%|#033[34m███████▎  #033[0m| 206/281 [15:23<05:36,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 206 is completed and loss is 0.9451456665992737\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  74%|#033[34m███████▎  #033[0m| 207/281 [15:28<05:33,  4.50s/it]\u001b[0m\n",
      "\u001b[34mstep 207 is completed and loss is 1.2046085596084595\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  74%|#033[34m███████▍  #033[0m| 208/281 [15:32<05:28,  4.50s/it]\u001b[0m\n",
      "\u001b[34mstep 208 is completed and loss is 0.8963897824287415\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  74%|#033[34m███████▍  #033[0m| 209/281 [15:37<05:23,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 209 is completed and loss is 0.9060896039009094\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  75%|#033[34m███████▍  #033[0m| 210/281 [15:41<05:18,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 210 is completed and loss is 0.9560326337814331\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  75%|#033[34m███████▌  #033[0m| 211/281 [15:46<05:14,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 211 is completed and loss is 0.936603844165802\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  75%|#033[34m███████▌  #033[0m| 212/281 [15:50<05:09,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 212 is completed and loss is 0.9732354879379272\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  76%|#033[34m███████▌  #033[0m| 213/281 [15:55<05:05,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 213 is completed and loss is 1.199686884880066\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  76%|#033[34m███████▌  #033[0m| 214/281 [15:59<05:00,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 214 is completed and loss is 0.9747630953788757\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  77%|#033[34m███████▋  #033[0m| 215/281 [16:04<04:55,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 215 is completed and loss is 0.9919876456260681\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  77%|#033[34m███████▋  #033[0m| 216/281 [16:08<04:51,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 216 is completed and loss is 1.024309515953064\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  77%|#033[34m███████▋  #033[0m| 217/281 [16:13<04:46,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 217 is completed and loss is 1.0534496307373047\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  78%|#033[34m███████▊  #033[0m| 218/281 [16:17<04:42,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 218 is completed and loss is 0.7891653776168823\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  78%|#033[34m███████▊  #033[0m| 219/281 [16:22<04:37,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 219 is completed and loss is 0.8124940991401672\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  78%|#033[34m███████▊  #033[0m| 220/281 [16:26<04:33,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 220 is completed and loss is 0.8410765528678894\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  79%|#033[34m███████▊  #033[0m| 221/281 [16:31<04:29,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 221 is completed and loss is 0.9239204525947571\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  79%|#033[34m███████▉  #033[0m| 222/281 [16:35<04:24,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 222 is completed and loss is 0.9591628909111023\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  79%|#033[34m███████▉  #033[0m| 223/281 [16:40<04:20,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 223 is completed and loss is 0.9189921617507935\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  80%|#033[34m███████▉  #033[0m| 224/281 [16:44<04:15,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 224 is completed and loss is 0.9309251308441162\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  80%|#033[34m████████  #033[0m| 225/281 [16:49<04:11,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 225 is completed and loss is 0.8575656414031982\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  80%|#033[34m████████  #033[0m| 226/281 [16:53<04:06,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 226 is completed and loss is 1.1733579635620117\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  81%|#033[34m████████  #033[0m| 227/281 [16:58<04:02,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 227 is completed and loss is 0.7746190428733826\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  81%|#033[34m████████  #033[0m| 228/281 [17:02<03:57,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 228 is completed and loss is 1.0093047618865967\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  81%|#033[34m████████▏ #033[0m| 229/281 [17:07<03:53,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 229 is completed and loss is 0.8607635498046875\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  82%|#033[34m████████▏ #033[0m| 230/281 [17:11<03:48,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 230 is completed and loss is 0.9424850940704346\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  82%|#033[34m████████▏ #033[0m| 231/281 [17:15<03:44,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 231 is completed and loss is 0.9015662670135498\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  83%|#033[34m████████▎ #033[0m| 232/281 [17:20<03:39,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 232 is completed and loss is 0.9540577530860901\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  83%|#033[34m████████▎ #033[0m| 233/281 [17:24<03:35,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 233 is completed and loss is 1.0026881694793701\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  83%|#033[34m████████▎ #033[0m| 234/281 [17:29<03:30,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 234 is completed and loss is 0.9407071471214294\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  84%|#033[34m████████▎ #033[0m| 235/281 [17:33<03:26,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 235 is completed and loss is 0.99010169506073\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  84%|#033[34m████████▍ #033[0m| 236/281 [17:38<03:21,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 236 is completed and loss is 0.8229186534881592\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  84%|#033[34m████████▍ #033[0m| 237/281 [17:42<03:17,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 237 is completed and loss is 0.9244728684425354\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  85%|#033[34m████████▍ #033[0m| 238/281 [17:47<03:12,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 238 is completed and loss is 0.9660945534706116\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  85%|#033[34m████████▌ #033[0m| 239/281 [17:51<03:08,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 239 is completed and loss is 0.9614816308021545\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  85%|#033[34m████████▌ #033[0m| 240/281 [17:56<03:03,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 240 is completed and loss is 0.9295516610145569\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  86%|#033[34m████████▌ #033[0m| 241/281 [18:00<02:59,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 241 is completed and loss is 1.1008532047271729\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  86%|#033[34m████████▌ #033[0m| 242/281 [18:05<02:54,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 242 is completed and loss is 0.953995406627655\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  86%|#033[34m████████▋ #033[0m| 243/281 [18:09<02:50,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 243 is completed and loss is 0.7625323534011841\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  87%|#033[34m████████▋ #033[0m| 244/281 [18:14<02:45,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 244 is completed and loss is 0.9989860653877258\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  87%|#033[34m████████▋ #033[0m| 245/281 [18:18<02:41,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 245 is completed and loss is 0.853980541229248\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  88%|#033[34m████████▊ #033[0m| 246/281 [18:23<02:36,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 246 is completed and loss is 0.9187266230583191\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  88%|#033[34m████████▊ #033[0m| 247/281 [18:27<02:32,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 247 is completed and loss is 1.024280309677124\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  88%|#033[34m████████▊ #033[0m| 248/281 [18:32<02:27,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 248 is completed and loss is 0.9241374135017395\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  89%|#033[34m████████▊ #033[0m| 249/281 [18:36<02:23,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 249 is completed and loss is 1.066248893737793\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  89%|#033[34m████████▉ #033[0m| 250/281 [18:41<02:19,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 250 is completed and loss is 1.033936619758606\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  89%|#033[34m████████▉ #033[0m| 251/281 [18:45<02:14,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 251 is completed and loss is 1.0589176416397095\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  90%|#033[34m████████▉ #033[0m| 252/281 [18:50<02:10,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 252 is completed and loss is 0.9005488753318787\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  90%|#033[34m█████████ #033[0m| 253/281 [18:54<02:05,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 253 is completed and loss is 0.9273070693016052\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  90%|#033[34m█████████ #033[0m| 254/281 [18:59<02:01,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 254 is completed and loss is 0.7194365859031677\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  91%|#033[34m█████████ #033[0m| 255/281 [19:03<01:56,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 255 is completed and loss is 0.9043076038360596\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  91%|#033[34m█████████ #033[0m| 256/281 [19:08<01:52,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 256 is completed and loss is 0.9320439696311951\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  91%|#033[34m█████████▏#033[0m| 257/281 [19:12<01:47,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 257 is completed and loss is 1.0943530797958374\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  92%|#033[34m█████████▏#033[0m| 258/281 [19:17<01:43,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 258 is completed and loss is 1.0079787969589233\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  92%|#033[34m█████████▏#033[0m| 259/281 [19:21<01:38,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 259 is completed and loss is 0.9600773453712463\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  93%|#033[34m█████████▎#033[0m| 260/281 [19:26<01:34,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 260 is completed and loss is 1.0105966329574585\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  93%|#033[34m█████████▎#033[0m| 261/281 [19:30<01:30,  4.50s/it]\u001b[0m\n",
      "\u001b[34mstep 261 is completed and loss is 0.885664165019989\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  93%|#033[34m█████████▎#033[0m| 262/281 [19:35<01:25,  4.50s/it]\u001b[0m\n",
      "\u001b[34mstep 262 is completed and loss is 0.9581587910652161\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  94%|#033[34m█████████▎#033[0m| 263/281 [19:39<01:20,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 263 is completed and loss is 0.9465228915214539\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  94%|#033[34m█████████▍#033[0m| 264/281 [19:44<01:16,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 264 is completed and loss is 0.9264047145843506\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  94%|#033[34m█████████▍#033[0m| 265/281 [19:48<01:11,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 265 is completed and loss is 0.8774075508117676\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  95%|#033[34m█████████▍#033[0m| 266/281 [19:52<01:07,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 266 is completed and loss is 0.850674569606781\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  95%|#033[34m█████████▌#033[0m| 267/281 [19:57<01:02,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 267 is completed and loss is 0.8399167060852051\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  95%|#033[34m█████████▌#033[0m| 268/281 [20:01<00:58,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 268 is completed and loss is 1.1039271354675293\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  96%|#033[34m█████████▌#033[0m| 269/281 [20:06<00:53,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 269 is completed and loss is 0.9465033411979675\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  96%|#033[34m█████████▌#033[0m| 270/281 [20:10<00:49,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 270 is completed and loss is 0.8138645887374878\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  96%|#033[34m█████████▋#033[0m| 271/281 [20:15<00:44,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 271 is completed and loss is 0.8314648866653442\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  97%|#033[34m█████████▋#033[0m| 272/281 [20:19<00:40,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 272 is completed and loss is 1.0343948602676392\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  97%|#033[34m█████████▋#033[0m| 273/281 [20:24<00:35,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 273 is completed and loss is 0.8367315530776978\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  98%|#033[34m█████████▊#033[0m| 274/281 [20:28<00:31,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 274 is completed and loss is 0.9919254183769226\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  98%|#033[34m█████████▊#033[0m| 275/281 [20:33<00:26,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 275 is completed and loss is 0.9412922263145447\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  98%|#033[34m█████████▊#033[0m| 276/281 [20:37<00:22,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 276 is completed and loss is 0.8288459181785583\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  99%|#033[34m█████████▊#033[0m| 277/281 [20:42<00:17,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 277 is completed and loss is 0.993478000164032\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  99%|#033[34m█████████▉#033[0m| 278/281 [20:46<00:13,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 278 is completed and loss is 1.028693437576294\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  99%|#033[34m█████████▉#033[0m| 279/281 [20:51<00:08,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 279 is completed and loss is 0.9661486148834229\u001b[0m\n",
      "\u001b[34mTraining Epoch2: 100%|#033[34m█████████▉#033[0m| 280/281 [20:55<00:04,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 280 is completed and loss is 0.9826765060424805\u001b[0m\n",
      "\u001b[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 281/281 [21:00<00:00,  4.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 281/281 [21:00<00:00,  4.48s/it]\u001b[0m\n",
      "\u001b[34mMax CUDA memory allocated was 17 GB\u001b[0m\n",
      "\u001b[34mMax CUDA memory reserved was 18 GB\u001b[0m\n",
      "\u001b[34mPeak active CUDA memory was 17 GB\u001b[0m\n",
      "\u001b[34mCuda Malloc retires : 126\u001b[0m\n",
      "\u001b[34mCPU Total Peak Memory consumed during the train (max): 2 GB\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/282 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 1/282 [00:00<02:01,  2.32it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   1%|#033[32m          #033[0m| 2/282 [00:00<02:00,  2.32it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   1%|#033[32m          #033[0m| 3/282 [00:01<01:59,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   1%|#033[32m▏         #033[0m| 4/282 [00:01<01:59,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   2%|#033[32m▏         #033[0m| 5/282 [00:02<01:58,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   2%|#033[32m▏         #033[0m| 6/282 [00:02<01:58,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   2%|#033[32m▏         #033[0m| 7/282 [00:03<01:57,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 8/282 [00:03<01:57,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 9/282 [00:03<01:57,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 10/282 [00:04<01:56,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▍         #033[0m| 11/282 [00:04<01:56,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▍         #033[0m| 12/282 [00:05<01:55,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   5%|#033[32m▍         #033[0m| 13/282 [00:05<01:55,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   5%|#033[32m▍         #033[0m| 14/282 [00:06<01:54,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   5%|#033[32m▌         #033[0m| 15/282 [00:06<01:54,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   6%|#033[32m▌         #033[0m| 16/282 [00:06<01:53,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   6%|#033[32m▌         #033[0m| 17/282 [00:07<01:53,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   6%|#033[32m▋         #033[0m| 18/282 [00:07<01:53,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 19/282 [00:08<01:52,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 20/282 [00:08<01:52,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 21/282 [00:09<01:51,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   8%|#033[32m▊         #033[0m| 22/282 [00:09<01:51,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   8%|#033[32m▊         #033[0m| 23/282 [00:09<01:50,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   9%|#033[32m▊         #033[0m| 24/282 [00:10<01:50,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   9%|#033[32m▉         #033[0m| 25/282 [00:10<01:50,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   9%|#033[32m▉         #033[0m| 26/282 [00:11<01:49,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  10%|#033[32m▉         #033[0m| 27/282 [00:11<01:49,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  10%|#033[32m▉         #033[0m| 28/282 [00:12<01:48,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  10%|#033[32m█         #033[0m| 29/282 [00:12<01:48,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 30/282 [00:12<01:47,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 31/282 [00:13<01:47,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█▏        #033[0m| 32/282 [00:13<01:47,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  12%|#033[32m█▏        #033[0m| 33/282 [00:14<01:46,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  12%|#033[32m█▏        #033[0m| 34/282 [00:14<01:46,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  12%|#033[32m█▏        #033[0m| 35/282 [00:15<01:45,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  13%|#033[32m█▎        #033[0m| 36/282 [00:15<01:45,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  13%|#033[32m█▎        #033[0m| 37/282 [00:15<01:44,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  13%|#033[32m█▎        #033[0m| 38/282 [00:16<01:44,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 39/282 [00:16<01:44,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 40/282 [00:17<01:43,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  15%|#033[32m█▍        #033[0m| 41/282 [00:17<01:43,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  15%|#033[32m█▍        #033[0m| 42/282 [00:18<01:42,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  15%|#033[32m█▌        #033[0m| 43/282 [00:18<01:42,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  16%|#033[32m█▌        #033[0m| 44/282 [00:18<01:41,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  16%|#033[32m█▌        #033[0m| 45/282 [00:19<01:41,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  16%|#033[32m█▋        #033[0m| 46/282 [00:19<01:41,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 47/282 [00:20<01:40,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 48/282 [00:20<01:40,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 49/282 [00:21<01:39,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 50/282 [00:21<01:39,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 51/282 [00:21<01:38,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 52/282 [00:22<01:38,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  19%|#033[32m█▉        #033[0m| 53/282 [00:22<01:38,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  19%|#033[32m█▉        #033[0m| 54/282 [00:23<01:37,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  20%|#033[32m█▉        #033[0m| 55/282 [00:23<01:37,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  20%|#033[32m█▉        #033[0m| 56/282 [00:24<01:36,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  20%|#033[32m██        #033[0m| 57/282 [00:24<01:36,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██        #033[0m| 58/282 [00:24<01:35,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██        #033[0m| 59/282 [00:25<01:35,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 60/282 [00:25<01:35,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  22%|#033[32m██▏       #033[0m| 61/282 [00:26<01:34,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  22%|#033[32m██▏       #033[0m| 62/282 [00:26<01:34,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  22%|#033[32m██▏       #033[0m| 63/282 [00:27<01:33,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  23%|#033[32m██▎       #033[0m| 64/282 [00:27<01:33,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  23%|#033[32m██▎       #033[0m| 65/282 [00:27<01:32,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  23%|#033[32m██▎       #033[0m| 66/282 [00:28<01:32,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  24%|#033[32m██▍       #033[0m| 67/282 [00:28<01:32,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  24%|#033[32m██▍       #033[0m| 68/282 [00:29<01:31,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  24%|#033[32m██▍       #033[0m| 69/282 [00:29<01:31,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m██▍       #033[0m| 70/282 [00:30<01:30,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 71/282 [00:30<01:30,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  26%|#033[32m██▌       #033[0m| 72/282 [00:30<01:30,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  26%|#033[32m██▌       #033[0m| 73/282 [00:31<01:29,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  26%|#033[32m██▌       #033[0m| 74/282 [00:31<01:29,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  27%|#033[32m██▋       #033[0m| 75/282 [00:32<01:28,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  27%|#033[32m██▋       #033[0m| 76/282 [00:32<01:28,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  27%|#033[32m██▋       #033[0m| 77/282 [00:33<01:27,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  28%|#033[32m██▊       #033[0m| 78/282 [00:33<01:27,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  28%|#033[32m██▊       #033[0m| 79/282 [00:33<01:27,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  28%|#033[32m██▊       #033[0m| 80/282 [00:34<01:26,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 81/282 [00:34<01:26,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▉       #033[0m| 82/282 [00:35<01:25,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▉       #033[0m| 83/282 [00:35<01:25,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  30%|#033[32m██▉       #033[0m| 84/282 [00:36<01:24,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  30%|#033[32m███       #033[0m| 85/282 [00:36<01:24,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  30%|#033[32m███       #033[0m| 86/282 [00:36<01:24,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  31%|#033[32m███       #033[0m| 87/282 [00:37<01:23,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  31%|#033[32m███       #033[0m| 88/282 [00:37<01:23,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 89/282 [00:38<01:22,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 90/282 [00:38<01:22,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 91/282 [00:39<01:21,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  33%|#033[32m███▎      #033[0m| 92/282 [00:39<01:21,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  33%|#033[32m███▎      #033[0m| 93/282 [00:39<01:20,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  33%|#033[32m███▎      #033[0m| 94/282 [00:40<01:20,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▎      #033[0m| 95/282 [00:40<01:20,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▍      #033[0m| 96/282 [00:41<01:19,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▍      #033[0m| 97/282 [00:41<01:19,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  35%|#033[32m███▍      #033[0m| 98/282 [00:42<01:18,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  35%|#033[32m███▌      #033[0m| 99/282 [00:42<01:18,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  35%|#033[32m███▌      #033[0m| 100/282 [00:42<01:18,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 101/282 [00:43<01:17,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 102/282 [00:43<01:17,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  37%|#033[32m███▋      #033[0m| 103/282 [00:44<01:16,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  37%|#033[32m███▋      #033[0m| 104/282 [00:44<01:16,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  37%|#033[32m███▋      #033[0m| 105/282 [00:45<01:15,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  38%|#033[32m███▊      #033[0m| 106/282 [00:45<01:15,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  38%|#033[32m███▊      #033[0m| 107/282 [00:45<01:14,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  38%|#033[32m███▊      #033[0m| 108/282 [00:46<01:14,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▊      #033[0m| 109/282 [00:46<01:14,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 110/282 [00:47<01:13,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 111/282 [00:47<01:13,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  40%|#033[32m███▉      #033[0m| 112/282 [00:48<01:12,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  40%|#033[32m████      #033[0m| 113/282 [00:48<01:12,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  40%|#033[32m████      #033[0m| 114/282 [00:48<01:12,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  41%|#033[32m████      #033[0m| 115/282 [00:49<01:11,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  41%|#033[32m████      #033[0m| 116/282 [00:49<01:11,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  41%|#033[32m████▏     #033[0m| 117/282 [00:50<01:10,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  42%|#033[32m████▏     #033[0m| 118/282 [00:50<01:10,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  42%|#033[32m████▏     #033[0m| 119/282 [00:51<01:09,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 120/282 [00:51<01:09,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 121/282 [00:51<01:08,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 122/282 [00:52<01:08,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  44%|#033[32m████▎     #033[0m| 123/282 [00:52<01:08,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  44%|#033[32m████▍     #033[0m| 124/282 [00:53<01:07,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  44%|#033[32m████▍     #033[0m| 125/282 [00:53<01:07,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m████▍     #033[0m| 126/282 [00:54<01:06,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m████▌     #033[0m| 127/282 [00:54<01:06,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m████▌     #033[0m| 128/282 [00:54<01:06,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▌     #033[0m| 129/282 [00:55<01:05,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▌     #033[0m| 130/282 [00:55<01:05,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 131/282 [00:56<01:04,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  47%|#033[32m████▋     #033[0m| 132/282 [00:56<01:04,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  47%|#033[32m████▋     #033[0m| 133/282 [00:57<01:03,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 134/282 [00:57<01:03,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 135/282 [00:57<01:02,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 136/282 [00:58<01:02,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  49%|#033[32m████▊     #033[0m| 137/282 [00:58<01:02,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  49%|#033[32m████▉     #033[0m| 138/282 [00:59<01:01,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  49%|#033[32m████▉     #033[0m| 139/282 [00:59<01:01,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m████▉     #033[0m| 140/282 [01:00<01:00,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 141/282 [01:00<01:00,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 142/282 [01:00<01:00,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  51%|#033[32m█████     #033[0m| 143/282 [01:01<00:59,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  51%|#033[32m█████     #033[0m| 144/282 [01:01<00:59,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  51%|#033[32m█████▏    #033[0m| 145/282 [01:02<00:58,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 146/282 [01:02<00:58,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 147/282 [01:03<00:57,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 148/282 [01:03<00:57,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  53%|#033[32m█████▎    #033[0m| 149/282 [01:03<00:57,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  53%|#033[32m█████▎    #033[0m| 150/282 [01:04<00:56,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 151/282 [01:04<00:56,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▍    #033[0m| 152/282 [01:05<00:55,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▍    #033[0m| 153/282 [01:05<00:55,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m█████▍    #033[0m| 154/282 [01:06<00:54,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m█████▍    #033[0m| 155/282 [01:06<00:54,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m█████▌    #033[0m| 156/282 [01:06<00:53,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  56%|#033[32m█████▌    #033[0m| 157/282 [01:07<00:53,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  56%|#033[32m█████▌    #033[0m| 158/282 [01:07<00:53,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  56%|#033[32m█████▋    #033[0m| 159/282 [01:08<00:52,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 160/282 [01:08<00:52,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 161/282 [01:09<00:51,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 162/282 [01:09<00:51,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  58%|#033[32m█████▊    #033[0m| 163/282 [01:09<00:51,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  58%|#033[32m█████▊    #033[0m| 164/282 [01:10<00:50,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  59%|#033[32m█████▊    #033[0m| 165/282 [01:10<00:50,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  59%|#033[32m█████▉    #033[0m| 166/282 [01:11<00:49,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  59%|#033[32m█████▉    #033[0m| 167/282 [01:11<00:49,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  60%|#033[32m█████▉    #033[0m| 168/282 [01:12<00:48,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  60%|#033[32m█████▉    #033[0m| 169/282 [01:12<00:48,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  60%|#033[32m██████    #033[0m| 170/282 [01:12<00:47,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 171/282 [01:13<00:47,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 172/282 [01:13<00:47,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████▏   #033[0m| 173/282 [01:14<00:46,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  62%|#033[32m██████▏   #033[0m| 174/282 [01:14<00:46,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  62%|#033[32m██████▏   #033[0m| 175/282 [01:15<00:45,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  62%|#033[32m██████▏   #033[0m| 176/282 [01:15<00:45,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  63%|#033[32m██████▎   #033[0m| 177/282 [01:15<00:44,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  63%|#033[32m██████▎   #033[0m| 178/282 [01:16<00:44,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  63%|#033[32m██████▎   #033[0m| 179/282 [01:16<00:44,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 180/282 [01:17<00:43,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 181/282 [01:17<00:43,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  65%|#033[32m██████▍   #033[0m| 182/282 [01:18<00:42,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  65%|#033[32m██████▍   #033[0m| 183/282 [01:18<00:42,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  65%|#033[32m██████▌   #033[0m| 184/282 [01:18<00:42,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▌   #033[0m| 185/282 [01:19<00:41,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▌   #033[0m| 186/282 [01:19<00:41,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▋   #033[0m| 187/282 [01:20<00:40,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  67%|#033[32m██████▋   #033[0m| 188/282 [01:20<00:40,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  67%|#033[32m██████▋   #033[0m| 189/282 [01:21<00:39,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  67%|#033[32m██████▋   #033[0m| 190/282 [01:21<00:39,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 191/282 [01:21<00:39,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 192/282 [01:22<00:38,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 193/282 [01:22<00:38,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  69%|#033[32m██████▉   #033[0m| 194/282 [01:23<00:37,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  69%|#033[32m██████▉   #033[0m| 195/282 [01:23<00:37,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  70%|#033[32m██████▉   #033[0m| 196/282 [01:24<00:36,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  70%|#033[32m██████▉   #033[0m| 197/282 [01:24<00:36,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  70%|#033[32m███████   #033[0m| 198/282 [01:24<00:36,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████   #033[0m| 199/282 [01:25<00:35,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████   #033[0m| 200/282 [01:25<00:35,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 201/282 [01:26<00:34,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  72%|#033[32m███████▏  #033[0m| 202/282 [01:26<00:34,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  72%|#033[32m███████▏  #033[0m| 203/282 [01:27<00:33,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  72%|#033[32m███████▏  #033[0m| 204/282 [01:27<00:33,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  73%|#033[32m███████▎  #033[0m| 205/282 [01:27<00:32,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  73%|#033[32m███████▎  #033[0m| 206/282 [01:28<00:32,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  73%|#033[32m███████▎  #033[0m| 207/282 [01:28<00:32,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  74%|#033[32m███████▍  #033[0m| 208/282 [01:29<00:31,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  74%|#033[32m███████▍  #033[0m| 209/282 [01:29<00:31,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  74%|#033[32m███████▍  #033[0m| 210/282 [01:30<00:30,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  75%|#033[32m███████▍  #033[0m| 211/282 [01:30<00:30,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 212/282 [01:30<00:29,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  76%|#033[32m███████▌  #033[0m| 213/282 [01:31<00:29,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  76%|#033[32m███████▌  #033[0m| 214/282 [01:31<00:29,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  76%|#033[32m███████▌  #033[0m| 215/282 [01:32<00:28,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  77%|#033[32m███████▋  #033[0m| 216/282 [01:32<00:28,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  77%|#033[32m███████▋  #033[0m| 217/282 [01:33<00:27,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  77%|#033[32m███████▋  #033[0m| 218/282 [01:33<00:27,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  78%|#033[32m███████▊  #033[0m| 219/282 [01:33<00:27,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  78%|#033[32m███████▊  #033[0m| 220/282 [01:34<00:26,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  78%|#033[32m███████▊  #033[0m| 221/282 [01:34<00:26,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 222/282 [01:35<00:25,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▉  #033[0m| 223/282 [01:35<00:25,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▉  #033[0m| 224/282 [01:36<00:24,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  80%|#033[32m███████▉  #033[0m| 225/282 [01:36<00:24,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  80%|#033[32m████████  #033[0m| 226/282 [01:36<00:24,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  80%|#033[32m████████  #033[0m| 227/282 [01:37<00:23,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  81%|#033[32m████████  #033[0m| 228/282 [01:37<00:23,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  81%|#033[32m████████  #033[0m| 229/282 [01:38<00:22,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 230/282 [01:38<00:22,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 231/282 [01:39<00:21,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 232/282 [01:39<00:21,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 233/282 [01:39<00:20,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 234/282 [01:40<00:20,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 235/282 [01:40<00:20,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  84%|#033[32m████████▎ #033[0m| 236/282 [01:41<00:19,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  84%|#033[32m████████▍ #033[0m| 237/282 [01:41<00:19,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  84%|#033[32m████████▍ #033[0m| 238/282 [01:42<00:18,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  85%|#033[32m████████▍ #033[0m| 239/282 [01:42<00:18,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  85%|#033[32m████████▌ #033[0m| 240/282 [01:42<00:17,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  85%|#033[32m████████▌ #033[0m| 241/282 [01:43<00:17,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 242/282 [01:43<00:17,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 243/282 [01:44<00:16,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  87%|#033[32m████████▋ #033[0m| 244/282 [01:44<00:16,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  87%|#033[32m████████▋ #033[0m| 245/282 [01:45<00:15,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  87%|#033[32m████████▋ #033[0m| 246/282 [01:45<00:15,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  88%|#033[32m████████▊ #033[0m| 247/282 [01:45<00:14,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  88%|#033[32m████████▊ #033[0m| 248/282 [01:46<00:14,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  88%|#033[32m████████▊ #033[0m| 249/282 [01:46<00:14,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▊ #033[0m| 250/282 [01:47<00:13,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 251/282 [01:47<00:13,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 252/282 [01:48<00:12,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  90%|#033[32m████████▉ #033[0m| 253/282 [01:48<00:12,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  90%|#033[32m█████████ #033[0m| 254/282 [01:48<00:12,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  90%|#033[32m█████████ #033[0m| 255/282 [01:49<00:11,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  91%|#033[32m█████████ #033[0m| 256/282 [01:49<00:11,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  91%|#033[32m█████████ #033[0m| 257/282 [01:50<00:10,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  91%|#033[32m█████████▏#033[0m| 258/282 [01:50<00:10,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  92%|#033[32m█████████▏#033[0m| 259/282 [01:51<00:09,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  92%|#033[32m█████████▏#033[0m| 260/282 [01:51<00:09,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 261/282 [01:51<00:08,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 262/282 [01:52<00:08,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 263/282 [01:52<00:08,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  94%|#033[32m█████████▎#033[0m| 264/282 [01:53<00:07,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  94%|#033[32m█████████▍#033[0m| 265/282 [01:53<00:07,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  94%|#033[32m█████████▍#033[0m| 266/282 [01:54<00:06,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  95%|#033[32m█████████▍#033[0m| 267/282 [01:54<00:06,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  95%|#033[32m█████████▌#033[0m| 268/282 [01:54<00:06,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  95%|#033[32m█████████▌#033[0m| 269/282 [01:55<00:05,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▌#033[0m| 270/282 [01:55<00:05,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▌#033[0m| 271/282 [01:56<00:04,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 272/282 [01:56<00:04,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  97%|#033[32m█████████▋#033[0m| 273/282 [01:57<00:03,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  97%|#033[32m█████████▋#033[0m| 274/282 [01:57<00:03,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  98%|#033[32m█████████▊#033[0m| 275/282 [01:57<00:02,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  98%|#033[32m█████████▊#033[0m| 276/282 [01:58<00:02,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  98%|#033[32m█████████▊#033[0m| 277/282 [01:58<00:02,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  99%|#033[32m█████████▊#033[0m| 278/282 [01:59<00:01,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  99%|#033[32m█████████▉#033[0m| 279/282 [01:59<00:01,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  99%|#033[32m█████████▉#033[0m| 280/282 [02:00<00:00,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m█████████▉#033[0m| 281/282 [02:00<00:00,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 282/282 [02:00<00:00,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 282/282 [02:00<00:00,  2.33it/s]\u001b[0m\n",
      "\u001b[34meval_ppl=tensor(2.8073, device='cuda:0') eval_epoch_loss=tensor(1.0322, device='cuda:0')\u001b[0m\n",
      "\u001b[34mwe are about to save the PEFT modules\u001b[0m\n",
      "\u001b[34mPEFT modules are saved in saved_peft_model directory\u001b[0m\n",
      "\u001b[34mbest eval loss on epoch 2 is 1.0322399139404297\u001b[0m\n",
      "\u001b[34mEpoch 3: train_perplexity=2.6122, train_epoch_loss=0.9602, epcoh time 1260.6937624769998s\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_train_prep, Value: 2.7705228328704834\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_train_loss, Value: 1.0174100399017334\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_eval_prep, Value: 2.8398685455322266\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_eval_loss, Value: 1.0436909198760986\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_epoch_time, Value: 1260.9530797893333\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_checkpoint_time, Value: 0.8263891030000347\u001b[0m\n",
      "\u001b[34mINFO:root:Combining pre-trained base model with the PEFT adapter module.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:29<00:29, 29.76s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:35<00:00, 15.46s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:35<00:00, 17.61s/it]\u001b[0m\n",
      "\u001b[34mINFO:root:Saving the combined model in safetensors format.\u001b[0m\n",
      "\n",
      "2023-09-24 05:08:45 Uploading - Uploading generated training model\u001b[34mINFO:root:Saving complete.\u001b[0m\n",
      "\u001b[34mINFO:root:Copying tokenizer to the output directory.\u001b[0m\n",
      "\u001b[34mINFO:root:Putting inference code with the fine-tuned model directory.\u001b[0m\n",
      "\u001b[34m2023-09-24 05:08:41,468 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-09-24 05:08:41,468 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-09-24 05:08:41,468 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-09-24 05:09:31 Completed - Training job completed\n",
      "Training seconds: 4926\n",
      "Billable seconds: 4926\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "\n",
    "\n",
    "estimator = JumpStartEstimator(\n",
    "    model_id=model_id,\n",
    "    environment={\"accept_eula\": \"true\"},\n",
    "    disable_output_compression=True,  # For Llama-2-70b, add instance_type = \"ml.g5.48xlarge\"\n",
    "    instance_type = \"ml.g5.2xlarge\"\n",
    ")\n",
    "# By default, instruction tuning is set to false. Thus, to use instruction tuning dataset you use\n",
    "estimator.set_hyperparameters(instruction_tuned=\"True\", epoch=\"3\", max_input_length=\"1024\")\n",
    "estimator.fit({\"training\": train_data_location})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3889d9-1567-41ad-9375-fb738db629fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "Studio Kernel Dying issue:  If your studio kernel dies and you lose reference to the estimator object, please see section [6. Studio Kernel Dead/Creating JumpStart Model from the training Job](#6.-Studio-Kernel-Dead/Creating-JumpStart-Model-from-the-training-Job) on how to deploy endpoint using the training job name and the model id. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9decbf-08c6-4cb4-8644-4a96afb5bebf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Deploy the fine-tuned model\n",
    "---\n",
    "Next, we deploy fine-tuned model. We will compare the performance of fine-tuned and pre-trained model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "016e591b-63f8-4e0f-941c-4b4e0b9dc6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.jumpstart:No instance type selected for inference hosting endpoint. Defaulting to ml.g5.2xlarge.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary Python version: py39.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: ml.g5.2xlarge.\n",
      "INFO:sagemaker:Creating model with name: meta-textgeneration-llama-2-7b-2023-09-24-05-11-52-442\n",
      "INFO:sagemaker:Creating endpoint-config with name meta-textgeneration-llama-2-7b-2023-09-24-05-11-52-431\n",
      "INFO:sagemaker:Creating endpoint with name meta-textgeneration-llama-2-7b-2023-09-24-05-11-52-431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------!"
     ]
    }
   ],
   "source": [
    "finetuned_predictor = estimator.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb57904a-9631-45fe-bc3f-ae2fbb992960",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluate the pre-trained and fine-tuned model\n",
    "---\n",
    "Next, we use the test data to evaluate the performance of the fine-tuned model and compare it with the pre-trained model. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "87085bf6-dc7e-46f3-8563-d2e4aafd0820",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Inputs</th>\n",
       "      <th>Ground Truth</th>\n",
       "      <th>Response from fine-tuned model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGenerate a recipe for the dish spaghetti met kerstomaten en basilicum\\n\\n### Input:\\nspaghetti met kerstomaten en basilicum with spaghetti, kipfilet, kerstomaten, basilicum, margarine, knorr kruidenpasta spaghetti bolognese\\n\\n\\n\\n### Response:\\n</td>\n",
       "      <td>Ingredients :\\n 300.0 gram spaghetti, 300.0 gram kipfilet, 1.0 container kerstomaten, 0.5 bunch basilicum, 25.0 gram margarine, 1.0 container knorr kruidenpasta spaghetti bolognese\\nSteps:\\nkook de spaghetti beetgaar volgens de aanwijzingen op de verpakking . snijd de kipfilet in reepjes en halveer de kerstomaten . snijd de basilicum in reepjes . verhit de margarine en bak de kipreepjes goudbruin . bak de gehalveerde kerstomaten even mee . voeg 400 ml water, de kruidenpasta en de basilicum toe . breng de saus al roerend aan de kook en laat alles 2 3 minuten zachtjes doorkoken . serveer de spaghetti met de saus.</td>\n",
       "      <td>Ingredients :\\n 300.0 gram spaghetti, 2.0 kipfilet, 2.0 kerstomaten, 6.0 bunch basilicum, 1.0 tablespoon margarine, 1.0 bouillonknorr kruidenpasta spaghetti bolognese\\nSteps:\\nkook de spaghetti volgens de bereidingswijze op de verpakking. verwarm de margarine in een hapje en bak hierin de kipfilets en de tomaten. bak ze in ca. 15 20 min. gaar en gaar. hak de basilicum fijn en snijd de spaghetti in dikke ringen. schep de spaghetti uiteen op vier eetvouwsallades, bak de pasta in ca. 6 8 min. gaar, bestrooi er dan de kipfilets, de kerstomaten en de basilicum doorheen en strooi ruim tussendoor wat peper en zout. giet ondertussen de pasta bolognese erruild door en eet heerlijke spaghetti met kerstomaten.\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGenerate a recipe for the dish honey herb roasted chicken\\n\\n### Input:\\nhoney herb roasted chicken juicy roast chicken with herb honey and white wine sauce . kosher, meat, rosh hashanah, passover, gluten free.\\n\\n\\n\\n### Response:\\n</td>\n",
       "      <td>Ingredients :\\n 3 1/2 4 lbs whole chicken without giblets, 1 small handful of fresh rosemary sprigs, 1 small handful of fresh thyme sprigs, peel from one small lemon, sliced, 2 garlic cloves, 2 large onions, peeled and sliced, salt and pepper, 1/4 cup olive oil, 3 tbsp honey, divided, 1/2 cup white wine, 1/4 cup chicken broth, fresh rosemary and thyme sprigs for garnish optional\\nSteps:\\npreheat oven to 400 degrees . whisk together 1/4 cup of olive oil, 1 tbsp of honey, and 1 tbsp fresh lemon juice . this is your basting mixture . assemble your chicken, herbs, lemon peel, garlic, and a few small slices of onion . season the cavity of the chicken with salt and pepper and brush some of the basting mixture inside the chicken go light on the salt if you're using a kosher chicken, which will already be salted . stuff the cavity with half of the fresh rosemary and thyme sprigs, sliced lemon peel, garlic cloves, and a few pieces of the sliced onion . don't overstuff the cavity pack it loosely with room to breathe . truss the chicken click here for instructions . season the chicken with salt and pepper . line the bottom of your roasting pan with foil . take remaining sliced onions and place them in an even layer on the bottom of your roasting pan . drizzle 2 tbsp of olive oil over the onions . remove leaves from the rest of the rosemary and thyme sprigs, then put the leaves into the roasting pan and discard stems . use a large spoon to toss and coat the onion slices with herbs and olive oil . place the chicken breast side down onto the bed of onions . pour half of the basting mixture over the chicken, using a brush to coat it evenly . cover the roasting pan with foil nonstick foil is best . pierce a few small vents into the outer edges of the foil with a knife . place covered roasting pan into the preheated oven and cook for 45 minutes . take roasting pan out of the oven, remove the foil reserve, and flip the chicken to breast side up . brush off any onions or herbs that cling to the top of the chicken . brush the rest of the basting mixture evenly onto the top of the chicken . cover the roasting pan again with the vented foil . place back into the oven for 45 minutes longer.remove foil from the top of the roasting pan and reduce oven heat to 375 degrees . let the chicken continue to roast another 20 30 minutes until the skin is brown and crisp . the skin may get quite dark in places this is from the honey in the basting liquid . watch the chicken carefully, as it can go from brown to black very quickly . blackened parts of the skin will not affect the flavor in a negative way . chicken is done when the skin is nicely browned, and the internal temperature reaches at least 170 degrees f as measured on a food thermometer at the thickest part of the thigh.take the chicken out of the roasting pan and place it on a carving board . cover it with foil to keep the heat in and let it rest for 15 minutes before carving . meanwhile, use a mesh strainer to strain the pan drippings from the roasting pan into a small saucepan . let the drippings settle for a moment you will see brown bits settle to the bottom of the pan and lighter fat and oil rise to the top . skim about half of the fat/oil from the top of the liquid . add 1/2 cup of white wine, 1/4 cup of chicken broth, and 2 tbsp of honey to the pan . whisk to blend . heat the sauce over medium to a light boil and let it simmer for about 2 minutes, whisking constantly.remove the honey sauce from heat and season with salt and pepper to taste . carve the chicken . drizzled each serving of meat with some of the honey sauce . garnish with fresh thyme or rosemary, if desired, for a pretty and aromatic presentation.</td>\n",
       "      <td>Ingredients :\\n 4 lbs boneless chicken thighs, 2 tbsp butter, 1 tbsp olive oil, 1 whole garlic clove, 2 tsp salt, 1/4 tsp black pepper, 1/4 tsp ground coriander, 1/4 tsp dried thyme, 1/2 tsp poultry seasoning, 1 tbsp honey, 2 tsbsp white wine, 1 tbsp lemon juice, 1 tbsp fresh tarragon chopped, 4 sprigs fresh rosemary, 1 tbsp parsley, cooked rice or pasta\\nSteps:\\nin a large bowl, place the entire chicken thighs. in another bowl, add all ingredients. mix together until combined. pour into your bowl and cover. refrigerate for 20 24 hours. at least an hour before cooking, remove the herb mixture from the fridge. discard the marinade. set aside. preheat your oven to 400degf. roast chicken for about 45 60 minutes on 400 degf. depending on the size it will cook longer check halfway through and add an additional time. check internal temperature of the thighs with a meat thermometer for doneness. if you are serving skin on, cook until skin is golden brown. note if you like it slightly more cooked, increase oven to 425degf. take out of the oven and let rest for about 10 15 minutes before slicing. serve over rice, pasta or on its own. the leftovers will keep in the fridge for up to 7 days and in the freezer for up to 3 months.\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGenerate a recipe for the dish fig and honey cocktail\\n\\n### Input:\\nfig and honey cocktail recipe, perfect for serving at rosh hashanah, sukkot, or just because . includes recipe for fresh fig puree.\\n\\n\\n\\n### Response:\\n</td>\n",
       "      <td>Ingredients :\\n 2 cups fresh figs, rinsed and halved, 1 tbsp sugar, 2 tsp filtered water, 1 tsp freshly squeezed lemon juice, 1 1/2 tbsp fig puree, 1/2 tsp honey, 2 oz ginger ale, 1 1/2 oz vodka, 2 tsp freshly squeezed lemon juice, ice, 1 1/3 cups fig puree, 1/4 cup honey, 28 oz ginger ale, 21 oz vodka, 1/2 cup freshly squeezed lemon juice\\nSteps:\\ncombine ingredients in a blender and pulse until smooth . transfer to a container and refrigerate for 2 3 days, or use immediately to make cocktails . this recipe makes about 1 cup 8 oz . puree, which will make about 10 cocktails.</td>\n",
       "      <td>Ingredients :\\n 2.0 fresh figs, 2.0 honey, 2.0 cup freshly squeezed orange juice, 2.0 cup club soda\\nSteps:\\nif using a store bought fig purée, skip this step. if not, see here for recipe for fresh fig purée. to make fresh fig purée, combine the fresh figs, 1 1/2 teaspoons honey, orange juice, and water. purée in a blender. in another large bowl, mix the freshly puréed figs with the club soda. place in your pitcher and serve chilled. note if you're only making a pitcher or two, feel free to purée just enough figs to yield the amount of fig purée you need. the recipe above is designed for a large batch. you can also make this cocktail with pre made fig purée. simply substitute a store bought fig purée for the fresh variety\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGenerate a recipe for the dish spicy breakfast fajitas with eggs and guacamole\\n\\n### Input:\\nspicy breakfast fajitas with eggs and guacamole is my not just delicious for breakfast, you can also serve them for lunch or dinner\\n\\n\\n\\n### Response:\\n</td>\n",
       "      <td>Ingredients :\\n quick guacamole, 6 ounces avocado, 1 tablespoon lime juice, kosher salt, veggies, 1 tablespoon extra virgin olive oil, 2 red, 1 /2 medium white onion, 1 teaspoon ground cumin, 1 /2 teaspoon chili powder, pinch of red pepper flakes, kosher salt, 2 cloves garlic, 2 teaspoons fresh lime juice, fajitas and garnishes, 6 corn tortillas, olive oil spray, 6 large eggs, 1 ounce crumbled feta or queso fresco cheese, handful of fresh cilantro, freshly ground black pepper, hot sauce and/or your favorite salsa\\nSteps:\\nusing a spoon, scoop the flesh of the avocados into a small bowl . add the lime juice and 1/4 teaspoon salt and mash with a pastry cutter, potato masher, or fork until the mixture is blended and no longer chunky . taste and add additional lime juice and/or salt, if necessary.</td>\n",
       "      <td>Ingredients :\\n 6.0 eggs, 1.0 tablespoon olive oil, 0.2 teaspoon garlic powder, 1.0 bell pepper, 2.0 ounce olive oil spray, 0.2 teaspoon cumin, 2.0 ounce olive oil spray, 0.1 teaspoon garlic powder, 7.2 ounce cooked beef, 0.2 cup cheddar cheese, 1.0 tablespoon chopped cilantro, 3.0 tablespoon chopped onion, 0.5 cup guacamole\\nSteps:\\nheat olive oil in a large nonstick skillet over medium heat and cook eggs until cooked to your liking. top with chopped onion and cilantro before serving. spray a second large skillet with olive oil and cook the peppers and chorizo on medium high heat until browned and cooked through. spray the beans with olive oil while cooking to prevent sticking and cook with the other ingredients until heated through. top eggs with red peppers, chorizo and cheese. top with guacamole.\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGenerate a recipe for the dish tandoori met tilapia, garnalen en kokosrijst\\n\\n### Input:\\ntandoori met tilapia, garnalen en kokosrijst with tilapiafilet, paprika, paprika, bladselderij, rijst, zout, conimex romige kokosmelk, knorr chicken tonight milde tandoori, zonnebloemolie, garnalen\\n\\n\\n\\n### Response:\\n</td>\n",
       "      <td>Ingredients :\\n 350.0 gram tilapiafilet, 1.0 paprika, 1.0 paprika, 1.0 bunch bladselderij, 400.0 gram rijst, zout, 1.0 packet conimex romige kokosmelk, 1.0 jar knorr chicken tonight milde tandoori, 2.0 tablespoon zonnebloemolie, 125.0 gram garnalen\\nSteps:\\n1. snij de tilapiafilet in stukken . maak de paprika's schoon en snijd deze in repen . hak de bladselderij fijn . kook de rijst gaar volgens de gebruiksaanwijzing in 0, 5 liter water met zout en de kokosmelk . 2. verhit de olie in een ruime koekepan en bak de tilapiafilet ongeveer 2 minuten op hoog vuur . voeg de paprika toe en roerbak nog 3 minuten . voeg de tandoori saus toe en verwarm het geheel 3 minuten op middelhoog vuur . meng de garnalen door de saus . 3. meng de bladselderij door de kokosrijst en serveer bij de tandoori.</td>\n",
       "      <td>Ingredients :\\n 1.0 tilapiafilet, 1.0 paprika, 1.0 paprika, 2.0 tablespoon bladselderij, 1.0 package rijst, zout, 1.0 containers conimex romige kokosmelk, 1.0 bag knorr chicken tonight milde tandoori, zonnebloemolie, 350.0 gram garnalen\\nSteps:\\nschil de paprika, snijd het dekseltje af en de paprikapoot in boven de paprika in enkele ringen uitlekken. kook de 2 teugen rijst, zout en de kokosmelk goudblauw op. kook de tilapia in 3 4 minute goudblauw gaar. meng de tandoori en de knorr chicken tonight mix goed door elkaar. bestrooi de bodem van de hapjesbowl met zand. snijd de paprika en de bladselderij in reepjes. verdeel de paprika rondom de plaat. rijk afmeten de rijstmengsel. verdeel de tilapia over de rijst. bestrooi vervolgens de paprika en de bladselderij over het vlees. schep tandoori, helpt met het smaken. bak de tilapia 5 a 8 minuten goudblauw, met de koken in zicht. giet de inhoud van de canje af en strooi er in het vlees. serveer de tilapia direct uit de ovenschaal. laat er een vuur in het vuur komen. doe de garnalen in een diepe pan en zet het vuur hoog. kook nog even de garnalen aan gemaakt met 4 5 minuten. verwijder het uiteinde van een knorr kruidentje of cumin en bestrooi de garnalen er met. bak de garnalen nog even op. meng het afgegoten rijsttegel met de helft van de conimex romig kokosmelk, peper en zout, ruikt naar de garnalen en voeg er wat zonnemolentje bij. verdeel de garnalen over de rijstmix en voeg de rest van het kokosmelk er aan toe.\\n\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "test_dataset = newset[\"test\"]\n",
    "\n",
    "inputs, ground_truth_responses, responses_before_finetuning, responses_after_finetuning = (\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    ")\n",
    "\n",
    "\n",
    "def predict_and_print(datapoint):\n",
    "    # For instruction fine-tuning, we insert a special key between input and output\n",
    "    input_output_demarkation_key = \"\\n\\n### Response:\\n\"\n",
    "\n",
    "    payload = {\n",
    "        \"inputs\": template[\"prompt\"].format(\n",
    "            instruction=datapoint[\"instruction\"], context=datapoint[\"context\"]\n",
    "        )\n",
    "        + input_output_demarkation_key,\n",
    "        \"parameters\": {\"max_new_tokens\": 1000},\n",
    "    }\n",
    "    inputs.append(payload[\"inputs\"])\n",
    "    ground_truth_responses.append(datapoint[\"response\"])\n",
    "\n",
    "    # Please change the following line to \"accept_eula=True\"\n",
    "    finetuned_response = finetuned_predictor.predict(payload, custom_attributes=\"accept_eula=true\")\n",
    "    responses_after_finetuning.append(finetuned_response[0][\"generation\"])\n",
    "\n",
    "\n",
    "try:\n",
    "    for i, datapoint in enumerate(test_dataset.select(range(5))):\n",
    "        predict_and_print(datapoint)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"Inputs\": inputs,\n",
    "            \"Ground Truth\": ground_truth_responses,\n",
    "            \"Response from fine-tuned model\": responses_after_finetuning,\n",
    "        }\n",
    "    )\n",
    "    display(HTML(df.to_html()))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4b622019-fba5-4e81-aa63-1bb468bfd945",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.jumpstart:No instance type selected for inference hosting endpoint. Defaulting to ml.g5.2xlarge.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary Python version: py39.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: ml.g5.2xlarge.\n",
      "INFO:sagemaker:Creating model with name: meta-textgeneration-llama-2-7b-2023-09-24-05-48-50-461\n",
      "INFO:sagemaker:Creating endpoint-config with name meta-textgeneration-llama-2-7b-2023-09-24-05-48-50-555\n",
      "INFO:sagemaker:Creating endpoint with name meta-textgeneration-llama-2-7b-2023-09-24-05-48-50-555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "pretrained_model = JumpStartModel(model_id=model_id)\n",
    "pretrained_predictor = pretrained_model.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6172cc4f-dd95-43c5-87e6-ee48c0bb5e5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def p2new(recipe):\n",
    "    # For instruction fine-tuning, we insert a special key between input and output\n",
    "    input_output_demarkation_key = \"\\n\\n### Response:\\n\"\n",
    "\n",
    "    payload = {\n",
    "        \"inputs\": template[\"prompt\"].format(\n",
    "            instruction=\"Below is an instruction that describes a task, \"+\n",
    "            \"paired with an input that provides further context. \"+\n",
    "            \"Write a response that appropriately completes the request.\\n\"+\n",
    "            \"### Instruction:\\nGenerate a recipe for the dish \"+recipe,\n",
    "            context=\"\"\n",
    "        )\n",
    "        + input_output_demarkation_key,\n",
    "        \"parameters\": {\"max_new_tokens\": 1000},\n",
    "    }\n",
    "    # Please change the following line to \"accept_eula=True\"\n",
    "    finetuned_response = finetuned_predictor.predict(payload, custom_attributes=\"accept_eula=true\")\n",
    "    \n",
    "    pretrained_response = pretrained_predictor.predict(\n",
    "        payload, custom_attributes=\"accept_eula=true\"\n",
    "    )\n",
    "    print(\"finetuned response -> \"+finetuned_response[0]['generation'])\n",
    "    print(\"\\n==================================\\n\")\n",
    "    print(\"Pretrained response -> \"+pretrained_response[0]['generation'])\n",
    "    return finetuned_response\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8792e91-5212-4a2d-92b4-a723b6622ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_response(payload, response):\n",
    "    print(payload[\"inputs\"])\n",
    "    print(f\"> {response[0]['generation']}\")\n",
    "    print(\"\\n==================================\\n\")\n",
    "    pretrained_response = pretrained_predictor.predict(\n",
    "        payload, custom_attributes=\"accept_eula=true\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d1a7ec90-35eb-45cc-917c-2aa7ba65c9a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finetuned response ->  Ingredients :\n",
      " 2 tbsp extra virgin olive oil, 1 tsp balsamic, 1/4 cup white balsamic, 3/4 cup baby spinach, 1/4 cup red pepper strips, 1/4 cup cilantro\n",
      "Steps:\n",
      "toss in a large bowl, drizzle dressing and add salt and pepper to taste. garnish with crumbled blue cheese or toasted pine nuts\n",
      "\n",
      "\n",
      "\n",
      "==================================\n",
      "\n",
      "Pretrained response -> \n",
      "\n",
      "\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "Create a recipe for your favourite dish of the week. You can either write the recipe out at the end of this file, or by following the instructions at [Write Recipes](https://github.com/vant/vant-ant-design-starter/blob/master/recipe/recipe.md).\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "\n",
      "```jsx\n",
      "---\n",
      "author: 'Ant Design'\n",
      "cover: ''\n",
      "description: Write a recipe\n",
      "instructions:\n",
      "  - Include steps to follow to make the dish\n",
      "  - Include one image per step of the dish if it calls for complex steps\n",
      "  - The recipe should end with a call to action, encouraging your reader to go and try it! Don’t forget to link to your project.\n",
      "keywords:\n",
      "  - Recipes\n",
      "learn:\n",
      "  - Recipes\n",
      "  - Cooking\n",
      "tags:\n",
      "  - Curriculum\n",
      "showcase: false\n",
      "steps:\n",
      "  - Prepare 1 tsp allspice, 1 tsp sage, 3 cloves, ½ a head of garlic, 2 cups of olive oil, 1 can of tuna and 10 baby carrots.\n",
      "  - Set up your workspace\n",
      "  - Prepare 3 tbsp sage, 3 tbsp allspice, 6 cloves, 2 tbsp chopped garlic and 2 cups olive oil and 10 baby carrots.\n",
      "  - Season your fish with allspice and garlic powder. Pour 5 tbsp water and 6 tbsp lemon juice.\n",
      "  - Slice each head of the garlic\n",
      "  - Add 1 cup baby carrots, 10 baby carrots or 1 cup baby carrots with sage and ¼ tsp salt in a bowl.\n",
      "  - Mix 1 cup baby carrots, 1 tsp allspice, 1 tbsp chopped bay leaf and ¼ tsp salt in a bowl.\n",
      "  - Add the garlic and 3 peeled tbsp tuna.\n",
      "  - Dice 3 peeled garlic with 1 tsp of sage.\n",
      "    - Slice 6 tbsp of garlic with ¼ tsp of salt.\n",
      "    - Whisk 6 tbsp of garlic, 3 tbs chopped bay leaf with 1 tbsp sage with 3 tbsp lemon juice\n",
      "    - Whisk 6 tbsp chopped garlic and 1 tsp allspice with ¼ tsp salt\n",
      "    - Whisk 6 tbsp chopped garlic with 6 cloves, 6 tbsp oil with 3 tbsp sage\n",
      "    - Slice 3 garlic cloves, 3 tbsp sage, 3 tbsp chopped bay leaves, 3 tbsp salt and 6 cloves with 5 tbsp of water and 5 tbsp lemon juice\n",
      "    - Slice 3 chopped garlic cloves and add 3 peeled tbsp tuna.\n",
      "  - Chop 3 onions\n",
      "    - Slice 3 tbsp onions\n",
      "  - Mix 4 chopped garlic, 2 peeled carrots, 8 tbsp lemon juice and 1 tbsp oil.\n",
      "    - Mix 1 chopped onion with 1 garlic clove.\n",
      "  - Add the seasoned garlic.\n",
      "    - Add the carrots, bay leaves and ¼ tsp salt.\n",
      "    - Add the tuna and 6 cloves.\n",
      "    - Add the seasoned garlic.\n",
      "    - Add the tuna and bay leaves.\n",
      "    - Add 2 tbsp sage to the tuna, bay leaves and carrots and salt and pepper.\n",
      "    - Add the seasoned garlic and sage.\n",
      "    - Add the chopped garlic, 1 tsp sage and 1 tsp allspice.\n",
      "  - Toss\n",
      "  - Cover your fish with sage and add 2 more tbsp of lemon juice and a pinch of salt.\n",
      "  - Add your peeled garlic and 6 cloves and 3 cups of boiling water; mix it well.\n",
      "    - Add the\n"
     ]
    }
   ],
   "source": [
    "retval = p2new(\"Italian veg salad with extra hot spice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "315f0a11-ee24-40ff-8e16-5e980bf52337",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint configuration with name: meta-textgeneration-llama-2-7b-2023-09-24-05-48-50-555\n",
      "INFO:sagemaker:Deleting endpoint with name: meta-textgeneration-llama-2-7b-2023-09-24-05-48-50-555\n"
     ]
    }
   ],
   "source": [
    "pretrained_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b67ee580-5d84-491f-865b-ef8680e3a4c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint configuration with name: meta-textgeneration-llama-2-7b-2023-09-24-05-11-52-431\n",
      "INFO:sagemaker:Deleting endpoint with name: meta-textgeneration-llama-2-7b-2023-09-24-05-11-52-431\n"
     ]
    }
   ],
   "source": [
    "finetuned_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b0a0f5-ef34-40db-8ab7-c24a5d14b525",
   "metadata": {},
   "source": [
    "### Clean up resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73ab2da-d00f-46db-90eb-81812898653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete resources\n",
    "#pretrained_predictor.delete_model()\n",
    "#pretrained_predictor.delete_endpoint()\n",
    "#finetuned_predictor.delete_model()\n",
    "#finetuned_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759ce98f-a35a-4c64-9fae-50894b5e9f37",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1c8c86-bfe2-4828-a7aa-dbd7a5ee075f",
   "metadata": {},
   "source": [
    "### 1. Supported Inference Parameters\n",
    "\n",
    "---\n",
    "This model supports the following inference payload parameters:\n",
    "\n",
    "* **max_new_tokens:** Model generates text until the output length (excluding the input context length) reaches max_new_tokens. If specified, it must be a positive integer.\n",
    "* **temperature:** Controls the randomness in the output. Higher temperature results in output sequence with low-probability words and lower temperature results in output sequence with high-probability words. If `temperature` -> 0, it results in greedy decoding. If specified, it must be a positive float.\n",
    "* **top_p:** In each step of text generation, sample from the smallest possible set of words with cumulative probability `top_p`. If specified, it must be a float between 0 and 1.\n",
    "* **return_full_text:** If True, input text will be part of the output generated text. If specified, it must be boolean. The default value for it is False.\n",
    "\n",
    "You may specify any subset of the parameters mentioned above while invoking an endpoint. \n",
    "\n",
    "\n",
    "### Notes\n",
    "- If `max_new_tokens` is not defined, the model may generate up to the maximum total tokens allowed, which is 4K for these models. This may result in endpoint query timeout errors, so it is recommended to set `max_new_tokens` when possible. For 7B, 13B, and 70B models, we recommend to set `max_new_tokens` no greater than 1500, 1000, and 500 respectively, while keeping the total number of tokens less than 4K.\n",
    "- In order to support a 4k context length, this model has restricted query payloads to only utilize a batch size of 1. Payloads with larger batch sizes will receive an endpoint error prior to inference.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d5df7e-95a5-47dc-b5d2-0178ebfc6b6f",
   "metadata": {},
   "source": [
    "### 2. Dataset formatting instruction for training\n",
    "\n",
    "---\n",
    "\n",
    "####  Fine-tune the Model on a New Dataset\n",
    "We currently offer two types of fine-tuning: instruction fine-tuning and domain adaption fine-tuning. You can easily switch to one of the training \n",
    "methods by specifying parameter `instruction_tuned` being 'True' or 'False'.\n",
    "\n",
    "\n",
    "#### 2.1. Domain adaptation fine-tuning\n",
    "The Text Generation model can also be fine-tuned on any domain specific dataset. After being fine-tuned on the domain specific dataset, the model\n",
    "is expected to generate domain specific text and solve various NLP tasks in that specific domain with **few shot prompting**.\n",
    "\n",
    "Below are the instructions for how the training data should be formatted for input to the model.\n",
    "\n",
    "- **Input:** A train and an optional validation directory. Each directory contains a CSV/JSON/TXT file. \n",
    "  - For CSV/JSON files, the train or validation data is used from the column called 'text' or the first column if no column called 'text' is found.\n",
    "  - The number of files under train and validation (if provided) should equal to one, respectively. \n",
    "- **Output:** A trained model that can be deployed for inference. \n",
    "\n",
    "Below is an example of a TXT file for fine-tuning the Text Generation model. The TXT file is SEC filings of Amazon from year 2021 to 2022.\n",
    "\n",
    "```Note About Forward-Looking Statements\n",
    "This report includes estimates, projections, statements relating to our\n",
    "business plans, objectives, and expected operating results that are “forward-\n",
    "looking statements” within the meaning of the Private Securities Litigation\n",
    "Reform Act of 1995, Section 27A of the Securities Act of 1933, and Section 21E\n",
    "of the Securities Exchange Act of 1934. Forward-looking statements may appear\n",
    "throughout this report, including the following sections: “Business” (Part I,\n",
    "Item 1 of this Form 10-K), “Risk Factors” (Part I, Item 1A of this Form 10-K),\n",
    "and “Management’s Discussion and Analysis of Financial Condition and Results\n",
    "of Operations” (Part II, Item 7 of this Form 10-K). These forward-looking\n",
    "statements generally are identified by the words “believe,” “project,”\n",
    "“expect,” “anticipate,” “estimate,” “intend,” “strategy,” “future,”\n",
    "“opportunity,” “plan,” “may,” “should,” “will,” “would,” “will be,” “will\n",
    "continue,” “will likely result,” and similar expressions. Forward-looking\n",
    "statements are based on current expectations and assumptions that are subject\n",
    "to risks and uncertainties that may cause actual results to differ materially.\n",
    "We describe risks and uncertainties that could cause actual results and events\n",
    "to differ materially in “Risk Factors,” “Management’s Discussion and Analysis\n",
    "of Financial Condition and Results of Operations,” and “Quantitative and\n",
    "Qualitative Disclosures about Market Risk” (Part II, Item 7A of this Form\n",
    "10-K). Readers are cautioned not to place undue reliance on forward-looking\n",
    "statements, which speak only as of the date they are made. We undertake no\n",
    "obligation to update or revise publicly any forward-looking statements,\n",
    "whether because of new information, future events, or otherwise.\n",
    "GENERAL\n",
    "Embracing Our Future ...\n",
    "```\n",
    "\n",
    "\n",
    "#### 2.2. Instruction fine-tuning\n",
    "The Text generation model can be instruction-tuned on any text data provided that the data \n",
    "is in the expected format. The instruction-tuned model can be further deployed for inference. \n",
    "Below are the instructions for how the training data should be formatted for input to the \n",
    "model.\n",
    "\n",
    "Below are the instructions for how the training data should be formatted for input to the model.\n",
    "\n",
    "- **Input:** A train and an optional validation directory. Train and validation directories should contain one or multiple JSON lines (`.jsonl`) formatted files. In particular, train directory can also contain an optional `*.json` file describing the input and output formats. \n",
    "  - The best model is selected according to the validation loss, calculated at the end of each epoch.\n",
    "  If a validation set is not given, an (adjustable) percentage of the training data is\n",
    "  automatically split and used for validation.\n",
    "  - The training data must be formatted in a JSON lines (`.jsonl`) format, where each line is a dictionary\n",
    "representing a single data sample. All training data must be in a single folder, however\n",
    "it can be saved in multiple jsonl files. The `.jsonl` file extension is mandatory. The training\n",
    "folder can also contain a `template.json` file describing the input and output formats. If no\n",
    "template file is given, the following template will be used:\n",
    "  ```json\n",
    "  {\n",
    "    \"prompt\": \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{context}\",\n",
    "    \"completion\": \"{response}\"\n",
    "  }\n",
    "  ```\n",
    "  - In this case, the data in the JSON lines entries must include `instruction`, `context` and `response` fields. If a custom template is provided it must also use `prompt` and `completion` keys to define\n",
    "  the input and output templates.\n",
    "  Below is a sample custom template:\n",
    "\n",
    "  ```json\n",
    "  {\n",
    "    \"prompt\": \"question: {question} context: {context}\",\n",
    "    \"completion\": \"{answer}\"\n",
    "  }\n",
    "  ```\n",
    "Here, the data in the JSON lines entries must include `question`, `context` and `answer` fields. \n",
    "- **Output:** A trained model that can be deployed for inference. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4edf0c9-3c95-4e4b-932e-5d8a839d4070",
   "metadata": {},
   "source": [
    "#### 2.3. Example fine-tuning with Domain-Adaptation dataset format\n",
    "---\n",
    "We provide a subset of SEC filings data of Amazon in domain adaptation dataset format. It is downloaded from publicly available [EDGAR](https://www.sec.gov/edgar/searchedgar/companysearch). Instruction of accessing the data is shown [here](https://www.sec.gov/os/accessing-edgar-data).\n",
    "\n",
    "License: [Creative Commons Attribution-ShareAlike License (CC BY-SA 4.0)](https://creativecommons.org/licenses/by-sa/4.0/legalcode).\n",
    "\n",
    "Please uncomment the following code to fine-tune the model on dataset in domain adaptation format.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85c93a9-ebe2-4966-a5d6-af4c053f69f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import boto3\n",
    "# model_id = \"meta-textgeneration-llama-2-7b\"\n",
    "\n",
    "# estimator = JumpStartEstimator(model_id=model_id,  environment={\"accept_eula\": \"true\"},instance_type = \"ml.g5.24xlarge\")\n",
    "# estimator.set_hyperparameters(instruction_tuned=\"False\", epoch=\"5\")\n",
    "# estimator.fit({\"training\": f\"s3://jumpstart-cache-prod-{boto3.Session().region_name}/training-datasets/sec_amazon\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f7e4f8-970f-4a1d-b6ee-86bc77b8b9a9",
   "metadata": {},
   "source": [
    "### 3. Supported Hyper-parameters for fine-tuning\n",
    "---\n",
    "- epoch: The number of passes that the fine-tuning algorithm takes through the training dataset. Must be an integer greater than 1. Default: 5\n",
    "- learning_rate: The rate at which the model weights are updated after working through each batch of training examples. Must be a positive float greater than 0. Default: 1e-4.\n",
    "- instruction_tuned: Whether to instruction-train the model or not. Must be 'True' or 'False'. Default: 'False'\n",
    "- per_device_train_batch_size: The batch size per GPU core/CPU for training. Must be a positive integer. Default: 4.\n",
    "- per_device_eval_batch_size: The batch size per GPU core/CPU for evaluation. Must be a positive integer. Default: 1\n",
    "- max_train_samples: For debugging purposes or quicker training, truncate the number of training examples to this value. Value -1 means using all of training samples. Must be a positive integer or -1. Default: -1. \n",
    "- max_val_samples: For debugging purposes or quicker training, truncate the number of validation examples to this value. Value -1 means using all of validation samples. Must be a positive integer or -1. Default: -1. \n",
    "- max_input_length: Maximum total input sequence length after tokenization. Sequences longer than this will be truncated. If -1, max_input_length is set to the minimum of 1024 and the maximum model length defined by the tokenizer. If set to a positive value, max_input_length is set to the minimum of the provided value and the model_max_length defined by the tokenizer. Must be a positive integer or -1. Default: -1. \n",
    "- validation_split_ratio: If validation channel is none, ratio of train-validation split from the train data. Must be between 0 and 1. Default: 0.2. \n",
    "- train_data_split_seed: If validation data is not present, this fixes the random splitting of the input training data to training and validation data used by the algorithm. Must be an integer. Default: 0.\n",
    "- preprocessing_num_workers: The number of processes to use for the preprocessing. If None, main process is used for preprocessing. Default: \"None\"\n",
    "- lora_r: Lora R. Must be a positive integer. Default: 8.\n",
    "- lora_alpha: Lora Alpha. Must be a positive integer. Default: 32\n",
    "- lora_dropout: Lora Dropout. must be a positive float between 0 and 1. Default: 0.05. \n",
    "- int8_quantization: If True, model is loaded with 8 bit precision for training. Default for 7B/13B: False. Default for 70B: True.\n",
    "- enable_fsdp: If True, training uses Fully Sharded Data Parallelism. Default for 7B/13B: True. Default for 70B: False.\n",
    "\n",
    "Note 1: int8_quantization is not supported with FSDP. Also, int8_quantization = 'False' and enable_fsdp = 'False' is not supported due to CUDA memory issues for any of the g5 family instances. Thus, we recommend setting exactly one of int8_quantization or enable_fsdp to be 'True'\n",
    "Note 2: Due to the size of the model, 70B model can not be fine-tuned with enable_fsdp = 'True' for any of the supported instance types.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b6d023-3487-4571-8b52-f332790c1ad7",
   "metadata": {},
   "source": [
    "### 4. Supported Instance types\n",
    "\n",
    "---\n",
    "We have tested our scripts on the following instances types:\n",
    "\n",
    "- 7B: ml.g5.12xlarge, nl.g5.24xlarge, ml.g5.48xlarge, ml.p3dn.24xlarge\n",
    "- 13B: ml.g5.24xlarge, ml.g5.48xlarge, ml.p3dn.24xlarge\n",
    "- 70B: ml.g5.48xlarge\n",
    "\n",
    "Other instance types may also work to fine-tune. Note: When using p3 instances, training will be done with 32 bit precision as bfloat16 is not supported on these instances. Thus, training job would consume double the amount of CUDA memory when training on p3 instances compared to g5 instances.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770d4350-cf4d-40a0-be1c-eba44efd33ab",
   "metadata": {},
   "source": [
    "### 5. Few notes about the fine-tuning method\n",
    "\n",
    "---\n",
    "- Fine-tuning scripts are based on [this repo](https://github.com/facebookresearch/llama-recipes/tree/main). \n",
    "- Instruction tuning dataset is first converted into domain adaptation dataset format before fine-tuning. \n",
    "- Fine-tuning scripts utilize Fully Sharded Data Parallel (FSDP) as well as Low Rank Adaptation (LoRA) method fine-tuning the models\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909ce841-3a2c-4c08-a102-b94148036a5a",
   "metadata": {},
   "source": [
    "### 6. Studio Kernel Dead/Creating JumpStart Model from the training Job\n",
    "---\n",
    "Due to the size of the Llama 70B model, training job may take several hours and the studio kernel may die during the training phase. However, during this time, training is still running in SageMaker. If this happens, you can still deploy the endpoint using the training job name with the following code:\n",
    "\n",
    "How to find the training job name? Go to Console -> SageMaker -> Training -> Training Jobs -> Identify the training job name and substitute in the following cell. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa60a66-1c2f-42df-8079-191319e28a65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "# training_job_name = <<training_job_name>>\n",
    "\n",
    "# attached_estimator = JumpStartEstimator.attach(training_job_name, model_id)\n",
    "# attached_estimator.logs()\n",
    "# attached_estimator.deploy()"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
